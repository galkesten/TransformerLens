{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8aa05580-5cab-4b40-90bf-3234c3c34b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Attention:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "176ac421-cfdf-47b5-a88f-39b22dcc337e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "import torch\n",
    "from jaxtyping import Float\n",
    "from torch.testing import assert_close\n",
    "import torch.nn as nn\n",
    "from transformer_lens.components import Attention\n",
    "from transformer_lens.components import LayerNorm\n",
    "from esm.layers.attention import MultiHeadAttention\n",
    "from transformer_lens.HookedTransformerConfig import HookedTransformerConfig\n",
    "from transformer_lens.components import ESM3_Hooked_MLP, swiglu_correction_fn\n",
    "from esm.layers.blocks import swiglu_ln_ffn\n",
    "import functools\n",
    "import einops\n",
    "import torch.nn.functional as F\n",
    "from transformer_lens.components import ESM3_Hooked_MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "384605f0-1c01-4a45-8388-d66210b619dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ATOL = 1e-4\n",
    "def create_multi_head_attention_params(d_model, n_heads, bias=False, qk_layernorm=False):\n",
    "    params = {\n",
    "        \"layernorm_qkv_weight\": torch.rand(d_model),  # Weight of LayerNorm\n",
    "        \"layernorm_qkv_bias\": torch.rand(d_model),    # Bias of LayerNorm\n",
    "        \"W_qkv_weight\": torch.rand(d_model * 3, d_model),  # Weight of Linear layer\n",
    "        \"W_qkv_bias\": torch.rand(d_model * 3) if bias else None,  # Bias of Linear layer\n",
    "        \"out_proj_weight\": torch.rand(d_model, d_model),  # Output projection weight\n",
    "        \"out_proj_bias\": torch.rand(d_model) if bias else None,  # Output projection bias\n",
    "    }\n",
    "    \n",
    "    if qk_layernorm:\n",
    "        params.update({\n",
    "            \"q_ln_weight\": torch.rand(d_model),\n",
    "            \"q_ln_bias\": torch.rand(d_model) if bias else None,\n",
    "            \"k_ln_weight\": torch.rand(d_model),\n",
    "            \"k_ln_bias\": torch.rand(d_model) if bias else None,\n",
    "        })\n",
    "    return params\n",
    "def assign_params_to__esm_attention_layer(layer, params, bias=True):\n",
    "    with torch.no_grad():\n",
    "        # Assign LayerNorm for QKV\n",
    "        layer.layernorm_qkv[0].weight.copy_(params[\"layernorm_qkv_weight\"])\n",
    "        layer.layernorm_qkv[0].bias.copy_(params[\"layernorm_qkv_bias\"])\n",
    "        \n",
    "        # Assign Weights and Bias for QKV Projection\n",
    "        layer.layernorm_qkv[1].weight.copy_(params[\"W_qkv_weight\"])\n",
    "        if bias:\n",
    "            layer.layernorm_qkv[1].bias.copy_(params[\"W_qkv_bias\"])\n",
    "        \n",
    "        # Assign Output Projection\n",
    "        layer.out_proj.weight.copy_(params[\"out_proj_weight\"])\n",
    "        if bias:\n",
    "            layer.out_proj.bias.copy_(params[\"out_proj_bias\"])\n",
    "        \n",
    "        # Assign LayerNorm for Q\n",
    "        if isinstance(layer.q_ln, nn.LayerNorm):\n",
    "            layer.q_ln.weight.copy_(params[\"q_ln_weight\"])\n",
    "            if bias:\n",
    "                layer.q_ln.bias.copy_(params[\"q_ln_bias\"])\n",
    "        \n",
    "        # Assign LayerNorm for K\n",
    "        if isinstance(layer.k_ln, nn.LayerNorm):\n",
    "            layer.k_ln.weight.copy_(params[\"k_ln_weight\"])\n",
    "            if bias:\n",
    "                layer.k_ln.bias.copy_(params[\"k_ln_bias\"])\n",
    "\n",
    "def assign_params_to_transformer_lens_attention_layer(attention_layer, pre_layer_norm, params, cfg, bias=True):\n",
    "    with torch.no_grad():\n",
    "        # Assign LayerNorm QKV\n",
    "        pre_layer_norm.w.copy_(params[\"layernorm_qkv_weight\"])\n",
    "        pre_layer_norm.b.copy_(params[\"layernorm_qkv_bias\"])\n",
    "\n",
    "        # Extract and split QKV weights\n",
    "        qkv_matrix = params[\"W_qkv_weight\"].clone()  # Shape: (d_model * 3, d_model)\n",
    "        assert qkv_matrix.shape == (cfg.d_model * 3, cfg.d_model), \"QKV weight shape mismatch.\"\n",
    "\n",
    "        qkv_reshaped = qkv_matrix.T  # Shape: (d_model, d_model * 3)\n",
    "        q, k, v = torch.chunk(qkv_reshaped, 3, dim=-1)  # Split into Q, K, V\n",
    "        \n",
    "        reshaper = functools.partial(\n",
    "            einops.rearrange, pattern=\"d_model (n_head d_head) -> n_head d_model d_head\", n_head=cfg.n_heads\n",
    "        )\n",
    "        q, k, v = map(reshaper, (q, k, v))\n",
    "        \n",
    "        # Copy Q, K, V weights\n",
    "        attention_layer.W_Q.copy_(q)\n",
    "        attention_layer.W_K.copy_(k)\n",
    "        attention_layer.W_V.copy_(v)\n",
    "\n",
    "        # Handle QKV bias\n",
    "        if bias and \"W_qkv_bias\" in params:\n",
    "            qkv_bias = params[\"W_qkv_bias\"].clone()  # Shape: (d_model * 3)\n",
    "            b_q, b_k, b_v = torch.chunk(qkv_bias, 3, dim=-1)\n",
    "            reshaper_bias = functools.partial(\n",
    "                einops.rearrange, pattern=\"(n_head d_head) -> n_head d_head\", n_head=cfg.n_heads\n",
    "            )\n",
    "            attention_layer.b_Q.copy_(reshaper_bias(b_q))\n",
    "            attention_layer.b_K.copy_(reshaper_bias(b_k))\n",
    "            attention_layer.b_V.copy_(reshaper_bias(b_v))\n",
    "\n",
    "        # Assign Output Projection\n",
    "        out_proj = params[\"out_proj_weight\"].clone()  # Shape: (d_model, d_model)\n",
    "        assert out_proj.shape == (cfg.d_model, cfg.d_model), \"Output projection weight shape mismatch.\"\n",
    "        out_proj_reshaped = einops.rearrange(out_proj.T, \"(n_head d_head) d_model -> n_head d_head d_model\", n_head=cfg.n_heads)\n",
    "        attention_layer.W_O.copy_(out_proj_reshaped)\n",
    "\n",
    "        # Assign Output Bias\n",
    "        if bias and \"out_proj_bias\" in params:\n",
    "            attention_layer.b_O.copy_(params[\"out_proj_bias\"])\n",
    "\n",
    "        # Assign LayerNorms for Q and K if qk_layernorm is enabled\n",
    "        if cfg.qk_layernorm:\n",
    "            attention_layer.q_ln.w.copy_(params[\"q_ln_weight\"])\n",
    "            attention_layer.k_ln.w.copy_(params[\"k_ln_weight\"])\n",
    "            if bias:\n",
    "                attention_layer.q_ln.b.copy_(params.get(\"q_ln_bias\", torch.zeros(cfg.d_model)))\n",
    "                attention_layer.k_ln.b.copy_(params.get(\"k_ln_bias\", torch.zeros(cfg.d_model)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0aaa8cd-cb92-4f22-bb77-6cac322472fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 512\n",
    "n_heads = 8\n",
    "d_head = d_model // n_heads\n",
    "bias = False\n",
    "batch_size = 1\n",
    "seq_len = 10\n",
    "qk_layernorm= True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1961a8e-1eaa-4b60-8326-dc55781f4e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_params = create_multi_head_attention_params(d_model, n_heads, bias=bias, qk_layernorm=qk_layernorm)\n",
    "\n",
    "# Create ESM original attention component\n",
    "esm_original_component = MultiHeadAttention(d_model, n_heads, bias, qk_layernorm).to(torch.float32)\n",
    "\n",
    "# Assign the explicit parameters to the model\n",
    "assign_params_to__esm_attention_layer(esm_original_component, fake_params, bias)\n",
    "\n",
    "#Now we want to create attention of transformer lens for comparing...\n",
    "\n",
    "cfg = HookedTransformerConfig(\n",
    "n_layers=1,           \n",
    "d_model=d_model,           \n",
    "n_ctx=20,            \n",
    "d_head=d_head,                     \n",
    "n_heads=n_heads,\n",
    "attention_dir=\"bidirectional\",\n",
    "init_weights=False,\n",
    "positional_embedding_type=\"rotary\",\n",
    "rotary_dim=d_head,\n",
    "default_prepend_bos=False,\n",
    "qk_layernorm=qk_layernorm,\n",
    "dtype=torch.float32,\n",
    "attn_only=True,\n",
    "use_attn_result=False\n",
    ")\n",
    "\n",
    "#create transformer lens attention and initialize: \n",
    "tested_attention_layer = Attention(cfg)\n",
    "pre_layer_norm = LayerNorm(cfg, d_model)\n",
    "assign_params_to_transformer_lens_attention_layer(tested_attention_layer, pre_layer_norm,fake_params, cfg, bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c0af246-3398-436b-b80a-2784a2d20ffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter\n",
      "torch.Size([1, 8, 10, 64])\n",
      "tensor([[[31908.0078, 33322.4922, 34319.7070,  ..., 31437.3828,\n",
      "          32462.5820, 33189.7734],\n",
      "         [31918.7949, 33333.3008, 34328.0820,  ..., 31449.6289,\n",
      "          32476.2598, 33202.3984],\n",
      "         [31947.8242, 33366.5820, 34361.9141,  ..., 31477.5039,\n",
      "          32507.8770, 33234.5000],\n",
      "         ...,\n",
      "         [31920.2891, 33335.3477, 34329.9609,  ..., 31447.3828,\n",
      "          32478.0410, 33205.3125],\n",
      "         [31963.3281, 33383.1719, 34379.3242,  ..., 31495.5664,\n",
      "          32523.2949, 33248.3828],\n",
      "         [31946.0703, 33367.8633, 34360.9961,  ..., 31476.6504,\n",
      "          32503.8379, 33232.1367]]])\n"
     ]
    }
   ],
   "source": [
    "x= torch.rand((batch_size, seq_len, d_model))\n",
    "with torch.no_grad():\n",
    "    layer_norm1= pre_layer_norm(x.clone())\n",
    "    q1, k1, v1 = tested_attention_layer.calculate_qkv_matrices(layer_norm1, layer_norm1, layer_norm1)\n",
    "    q1_flattened = einops.rearrange(q1, \"batch pos head_index d_head -> batch pos (head_index d_head)\")\n",
    "    k1_flattened = einops.rearrange(k1, \"batch pos head_index d_head -> batch pos (head_index d_head)\")\n",
    "    v1_flattened = einops.rearrange(v1, \"batch pos head_index d_head -> batch pos (head_index d_head)\")\n",
    "    if cfg.qk_layernorm:\n",
    "        q1 = tested_attention_layer.q_ln(q1_flattened)\n",
    "        k1 = tested_attention_layer.k_ln(k1_flattened)\n",
    "        q1 = einops.rearrange(q1, \"batch pos (head_index d_head) -> batch pos head_index d_head\", \n",
    "                                    head_index=cfg.n_heads, d_head=cfg.d_head)\n",
    "        k1 = einops.rearrange(k1, \"batch kv_pos (head_index d_head) -> batch kv_pos head_index d_head\", \n",
    "                        head_index=cfg.n_heads, d_head=cfg.d_head)\n",
    "    if cfg.positional_embedding_type == \"rotary\":\n",
    "        print(\"enter\")\n",
    "        kv_cache_pos_offset = 0\n",
    "        q1 = tested_attention_layer.apply_rotary(q1, kv_cache_pos_offset, None)\n",
    "        k1 = tested_attention_layer.apply_rotary(k1, 0, None)\n",
    "    q1 = einops.rearrange(q1, \"batch pos head_index d_head -> batch head_index pos d_head\")\n",
    "    k1 = einops.rearrange(k1, \"batch pos head_index d_head -> batch head_index pos d_head\")\n",
    "    v1 = einops.rearrange(v1, \"batch pos head_index d_head -> batch head_index pos d_head\")\n",
    "    # attn_scores = tested_attention_layer.calculate_attention_scores(q1, k1) \n",
    "    # pattern = F.softmax(attn_scores, dim=-1)\n",
    "    # pattern = torch.where(torch.isnan(pattern), torch.zeros_like(pattern), pattern)\n",
    "    # pattern = pattern.to(tested_attention_layer.cfg.dtype)\n",
    "    # pattern = pattern.to(v1.device)\n",
    "    # z = tested_attention_layer.calculate_z_scores(v1, pattern)  # [batch, pos, head_index, d_head]\n",
    "    z= F.scaled_dot_product_attention(\n",
    "                    q1, k1, v1\n",
    "                )\n",
    "    if not tested_attention_layer.cfg.use_attn_result:\n",
    "        w = einops.rearrange(\n",
    "            tested_attention_layer.W_O, \"head_index d_head d_model -> d_model (head_index d_head)\"\n",
    "        )\n",
    "        print(z.shape)\n",
    "        # ctx1=z.reshape(z.shape[0], z.shape[1], tested_attention_layer.cfg.d_head * tested_attention_layer.cfg.n_heads)\n",
    "        ctx1 = einops.rearrange(z, \"b h s d -> b s (h d)\")\n",
    "        out1 = F.linear(\n",
    "           ctx1,\n",
    "            w,\n",
    "            tested_attention_layer.b_O,\n",
    "        )\n",
    "        print(out1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b10788b0-c397-4d6b-8e43-d2df72e6deb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[31908.0176, 33322.4883, 34319.7148,  ..., 31437.3789,\n",
      "          32462.5781, 33189.7734],\n",
      "         [31918.7969, 33333.2969, 34328.0859,  ..., 31449.6289,\n",
      "          32476.2578, 33202.3984],\n",
      "         [31947.8281, 33366.5820, 34361.9141,  ..., 31477.5098,\n",
      "          32507.8750, 33234.4922],\n",
      "         ...,\n",
      "         [31920.2910, 33335.3438, 34329.9453,  ..., 31447.3867,\n",
      "          32478.0410, 33205.3125],\n",
      "         [31963.3281, 33383.1680, 34379.3359,  ..., 31495.5684,\n",
      "          32523.2969, 33248.3828],\n",
      "         [31946.0684, 33367.8789, 34361.0000,  ..., 31476.6484,\n",
      "          32503.8320, 33232.1406]]])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    qkv=esm_original_component.layernorm_qkv(x.clone())\n",
    "    q2, k2, v2 = torch.chunk(qkv, 3, dim=-1)\n",
    "    q2,k2 = (\n",
    "                esm_original_component.q_ln(q2).to(q2.dtype),\n",
    "                esm_original_component.k_ln(k2).to(q2.dtype),\n",
    "            )\n",
    "    q2, k2 = esm_original_component._apply_rotary(q2, k2)\n",
    "    n_heads = esm_original_component.n_heads\n",
    "    reshaper = functools.partial(\n",
    "        einops.rearrange, pattern=\"b s (h d) -> b h s d\", h=n_heads\n",
    "    )\n",
    "    q2, k2, v2 = map(\n",
    "        reshaper, (q2, k2, v2)\n",
    "    )\n",
    "    \n",
    "    ctx2 = F.scaled_dot_product_attention(\n",
    "        q2, k2, v2\n",
    "    )\n",
    "    ctx2 = einops.rearrange(ctx2, \"b h s d -> b s (h d)\")\n",
    "    out2=esm_original_component.out_proj(ctx2)\n",
    "    print(out2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "2a8014d2-6413-4d48-b578-62b040201648",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "#print(torch.allclose(q1, q2, atol=ATOL))\n",
    "#print(torch.allclose(k1, k2, atol=ATOL))\n",
    "#print(torch.allclose(v1_flattened, v2, atol=ATOL))\n",
    "print(torch.allclose(ctx1,ctx2, atol=ATOL))\n",
    "print(torch.allclose(w, esm_original_component.out_proj.weight, atol=ATOL))\n",
    "print(torch.allclose(out1, out2, atol=1e-4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "f02e6448-c619-4920-a4c6-7346c8980855",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.allclose(torch.zeros(10), torch.zeros(10), atol=ATOL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "49e1f225-23da-41e3-8759-4c6c10b5650e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x= torch.rand((batch_size, seq_len, d_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "4457a9b1-fa64-4eff-97d4-83ef40b02c09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "#layer_norm1= pre_layer_norm(x.clone())\n",
    "layer_norm1= esm_original_component.layernorm_qkv[0](x.clone())\n",
    "res1=tested_attention_layer.forward(layer_norm1, layer_norm1, layer_norm1)\n",
    "seq_id = None\n",
    "res2=esm_original_component.forward(x,seq_id )\n",
    "print(torch.allclose(res1, res2, atol=1e-10, rtol=1e-6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "52ef7528-876b-4719-8a8e-a926cb1008ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mlp_params(d_model, expansion_ratio, bias):\n",
    "    hidden_dim = swiglu_correction_fn(expansion_ratio, d_model)\n",
    "    params = {\n",
    "        \"layernorm_weight\": torch.rand(d_model),\n",
    "        \"layernorm_bias\": torch.rand(d_model),\n",
    "        \"l1_weight\": torch.rand(hidden_dim * 2, d_model),\n",
    "        \"l1_bias\": torch.rand(hidden_dim * 2) if bias else None,\n",
    "        \"l2_weight\": torch.rand(d_model, hidden_dim),\n",
    "        \"l2_bias\": torch.rand(d_model) if bias else None,\n",
    "    }\n",
    "    return params\n",
    "\n",
    "\n",
    "def assign_params_to_swiglu_mlp(mdl, params, bias):\n",
    "    with torch.no_grad():\n",
    "        # Assign LayerNorm parameters\n",
    "        mdl[0].weight.copy_(params[\"layernorm_weight\"])\n",
    "        mdl[0].bias.copy_(params[\"layernorm_bias\"])\n",
    "        # Assign first Linear layer parameters\n",
    "        mdl[1].weight.copy_(params[\"l1_weight\"])\n",
    "        if bias:\n",
    "            mdl[1].bias.copy_(params[\"l1_bias\"])\n",
    "        # Assign second Linear layer parameters\n",
    "        mdl[3].weight.copy_(params[\"l2_weight\"])\n",
    "        if bias:\n",
    "            mdl[3].bias.copy_(params[\"l2_bias\"])\n",
    "\n",
    "def assign_params_to_esm_mlp(mdl, params, bias, pre_layer_norm):\n",
    "    with torch.no_grad():\n",
    "        # Assign LayerNorm \n",
    "        pre_layer_norm.w.copy_(params[\"layernorm_weight\"])\n",
    "        pre_layer_norm.b.copy_(params[\"layernorm_bias\"])\n",
    "        # Assign first Linear layer parameters\n",
    "        mdl.l1.weight.copy_(params[\"l1_weight\"])\n",
    "        if bias:\n",
    "            mdl.l1.bias.copy_(params[\"l1_bias\"])\n",
    "        # Assign second Linear layer parameters\n",
    "        mdl.l2.weight.copy_(params[\"l2_weight\"])\n",
    "        if bias:\n",
    "            mdl.l2.bias.copy_(params[\"l2_bias\"])\n",
    "\n",
    "def test_compare_esm_and_swiglu_mlp(bias=False, expansion_ratio=4.0):\n",
    "    d_model = 512\n",
    "    batch_size = 1\n",
    "    seq_len = 10\n",
    "    # Create fake parameters for testing\n",
    "    fake_params = create_mlp_params(d_model, expansion_ratio, bias)\n",
    "\n",
    "    # Create the SwiGLU-based MLP\n",
    "    swiglu_mlp = swiglu_ln_ffn(d_model, expansion_ratio, bias)\n",
    "\n",
    "    # Assign parameters to SwiGLU MLP\n",
    "    assign_params_to_swiglu_mlp(swiglu_mlp, fake_params, bias)\n",
    "\n",
    "    # Create ESM3_Hooked_MLP configuration\n",
    "    cfg = HookedTransformerConfig(\n",
    "        n_layers=1,\n",
    "        d_head=64,\n",
    "        d_model=d_model,           \n",
    "        n_ctx=20,                               \n",
    "        init_weights=False,\n",
    "        dtype=torch.float32,\n",
    "        esm3_mlp_expansion_ratio=expansion_ratio,\n",
    "        act_fn = \"swiglu\"\n",
    "    )\n",
    "\n",
    "    # Create ESM3_Hooked_MLP\n",
    "    esm_mlp = ESM3_Hooked_MLP(cfg)\n",
    "    pre_layer_norm = LayerNorm(cfg, d_model)\n",
    "    # Assign parameters to ESM3_Hooked_MLP\n",
    "    assign_params_to_esm_mlp(esm_mlp, fake_params, bias, pre_layer_norm)\n",
    "\n",
    "    # Generate input tensor\n",
    "    x = torch.rand((batch_size, seq_len, d_model))\n",
    "\n",
    "    # Forward pass through both MLPs\n",
    "    with torch.no_grad():\n",
    "        original_output = swiglu_mlp(x.clone())\n",
    "        # gto do - add layer norm\n",
    "        #layer_norm1= pre_layer_norm(x.clone())\n",
    "        layer_norm1=swiglu_mlp[0](x.clone())\n",
    "        hooked_output = esm_mlp(layer_norm1)\n",
    "    # Compare outputs\n",
    "    assert torch.allclose(original_output, hooked_output, atol=1e-5, rtol=1e-3), \"Outputs do not match!\"\n",
    "    diff = torch.abs(original_output - hooked_output)\n",
    "    print(\"Maximum absolute difference:\", torch.max(diff))\n",
    "    print(\"Mean absolute difference:\", torch.mean(diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "92c8c784-a1c9-4f88-b816-99ff1db51901",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum absolute difference: tensor(0.)\n",
      "Mean absolute difference: tensor(0.)\n"
     ]
    }
   ],
   "source": [
    "test_compare_esm_and_swiglu_mlp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "6ec2f4aa-c00a-47e8-8e86-7a642d80cc21",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 512\n",
    "batch_size = 1\n",
    "seq_len = 10\n",
    "expansion_ratio = 4\n",
    "bias = False\n",
    "# Create fake parameters for testing\n",
    "fake_params = create_mlp_params(d_model, expansion_ratio, bias)\n",
    "\n",
    "# Create the SwiGLU-based MLP\n",
    "swiglu_mlp = swiglu_ln_ffn(d_model, expansion_ratio, bias)\n",
    "\n",
    "# Assign parameters to SwiGLU MLP\n",
    "assign_params_to_swiglu_mlp(swiglu_mlp, fake_params, bias)\n",
    "\n",
    "# Create ESM3_Hooked_MLP configuration\n",
    "cfg = HookedTransformerConfig(\n",
    "    n_layers=1,\n",
    "    d_head=64,\n",
    "    d_model=d_model,           \n",
    "    n_ctx=20,                               \n",
    "    init_weights=False,\n",
    "    dtype=torch.float32,\n",
    "    esm3_mlp_expansion_ratio=expansion_ratio,\n",
    "    act_fn = \"swiglu\"\n",
    ")\n",
    "esm_mlp = ESM3_Hooked_MLP(cfg)\n",
    "pre_layer_norm = LayerNorm(cfg, d_model)\n",
    "# Assign parameters to ESM3_Hooked_MLP\n",
    "assign_params_to_esm_mlp(esm_mlp, fake_params, bias, pre_layer_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "1a5adfe0-d236-45d4-9813-e0060a8ad5bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0)\n",
      "True\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.sum(esm_mlp.l1.weight != swiglu_mlp[1].weight))\n",
    "if esm_mlp.l1.bias is not None:\n",
    "    print(torch.sum(esm_mlp.l1.bias != swiglu_mlp[1].bias))\n",
    "else:\n",
    "    print(esm_mlp.l1.bias == swiglu_mlp[1].bias)\n",
    "print(torch.sum(pre_layer_norm.w != swiglu_mlp[0].weight))\n",
    "print(torch.sum(pre_layer_norm.b != swiglu_mlp[0].bias))\n",
    "print(torch.sum(esm_mlp.l2.weight != swiglu_mlp[3].weight))\n",
    "if esm_mlp.l1.bias is not None:\n",
    "    print(torch.sum(esm_mlp.l2.bias != swiglu_mlp[3].bias))\n",
    "else:\n",
    "    print(esm_mlp.l2.bias == swiglu_mlp[3].bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "97057ead-1cd5-4e3c-b11c-25fe9f3c0c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand((batch_size, seq_len, d_model))\n",
    "a= pre_layer_norm(x.clone())\n",
    "a= esm_mlp(a)\n",
    "b=swiglu_mlp(x.clone())\n",
    "assert torch.allclose(a, b, atol=1e-5, rtol=1e-3), \"Outputs do not match!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "4b567810-df5a-4e42-8ce1-49ace5b47a63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[16574672., 16639844., 16665702.,  ..., 16493135., 17035316.,\n",
       "          16245276.],\n",
       "         [17376684., 17435892., 17479734.,  ..., 17285440., 17864320.,\n",
       "          17032906.],\n",
       "         [18500416., 18574904., 18611504.,  ..., 18419540., 19019404.,\n",
       "          18134496.],\n",
       "         ...,\n",
       "         [18008828., 18080424., 18109598.,  ..., 17913314., 18493892.,\n",
       "          17644062.],\n",
       "         [16654212., 16736838., 16763836.,  ..., 16571718., 17121532.,\n",
       "          16326286.],\n",
       "         [18564806., 18627442., 18658598.,  ..., 18467940., 19047638.,\n",
       "          18182764.]]], grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "7ce6f108-b8d1-4450-b3df-d85ecdf5645b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[16574667., 16639840., 16665696.,  ..., 16493129., 17035308.,\n",
       "          16245270.],\n",
       "         [17376684., 17435892., 17479734.,  ..., 17285440., 17864320.,\n",
       "          17032906.],\n",
       "         [18500420., 18574904., 18611504.,  ..., 18419544., 19019404.,\n",
       "          18134498.],\n",
       "         ...,\n",
       "         [18008832., 18080430., 18109604.,  ..., 17913320., 18493896.,\n",
       "          17644068.],\n",
       "         [16654214., 16736840., 16763838.,  ..., 16571720., 17121534.,\n",
       "          16326287.],\n",
       "         [18564810., 18627446., 18658602.,  ..., 18467942., 19047642.,\n",
       "          18182768.]]], grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021c583c-d653-4497-9e08-ed8d2ea99809",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
