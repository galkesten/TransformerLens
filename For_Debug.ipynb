{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8aa05580-5cab-4b40-90bf-3234c3c34b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Attention:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44a8cf17-575a-44aa-9d75-38dd772183b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "176ac421-cfdf-47b5-a88f-39b22dcc337e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "import torch\n",
    "from jaxtyping import Float\n",
    "from torch.testing import assert_close\n",
    "import torch.nn as nn\n",
    "from transformer_lens.components import Attention\n",
    "from transformer_lens.components import LayerNorm\n",
    "from transformer_lens.components import HookedESM3MLP, swiglu_correction_fn\n",
    "from transformer_lens.components import HookedEsm3UnifiedTransformerBlock\n",
    "from esm.layers.attention import MultiHeadAttention\n",
    "from esm.layers.blocks import swiglu_ln_ffn, UnifiedTransformerBlock\n",
    "from transformer_lens.HookedTransformerConfig import HookedTransformerConfig\n",
    "import functools\n",
    "import einops\n",
    "from esm.utils.constants.esm3 import data_root\n",
    "import math\n",
    "from transformer_lens import HookedESM3,SupportedESM3Config\n",
    "from esm.pretrained import (\n",
    "    ESM3_sm_open_v0,\n",
    ")\n",
    "from esm.models.esm3 import ESM3\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "from esm.tokenization import get_esm3_model_tokenizers\n",
    "from esm.utils.structure.protein_chain import ProteinChain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7683190-3753-4ec9-8c29-7fd7a101e1b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Please notice the licsence - todo- add licenseSupport for ESM3 in TransformerLens is currently experimental, until such a time when it has feature parity with HookedTransformer and has been tested on real research tasks. Until then, backward compatibility is not guaranteed. Please see the docs for information on the limitations of the current implementation.\n",
      "If using ESM3 for interpretability research, keep in mind that ESM3 has some significant architectural differences to Language transformers like GPT.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4a11770814f410981af419cd665b55d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 22 files:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/galkesten/miniconda3/envs/transformer_lens_10/lib/python3.10/site-packages/esm/pretrained.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  cuda\n",
      "Loaded pretrained model esm3_sm_open_v1 into HookedESM3\n"
     ]
    }
   ],
   "source": [
    "config = SupportedESM3Config(\n",
    "    use_attn_result=False,\n",
    "    use_split_qkv_input=False,\n",
    "    use_hook_mlp_in=True,\n",
    "    use_attn_in=False,\n",
    "    esm3_output_type=\"all\",\n",
    "    esm3_use_torch_layer_norm=True,\n",
    "    esm3_use_torch_attention_calc=True\n",
    ")\n",
    "esm3_hooked1 = HookedESM3.from_pretrained(esm_cfg=config, device=device)\n",
    "esm3_original1 = ESM3_sm_open_v0(device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ac154ad-a6ae-4ea0-a21a-3b2ae16c56c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_identical_components(real,hooked):\n",
    "    # Compare parameters\n",
    "    for (name1, param1), (name2, param2) in zip(real.named_parameters(), hooked.named_parameters()):\n",
    "        if name1 != name2:\n",
    "            print(f\"Mismatch in parameter names: {name1} != {name2}\")\n",
    "            return False\n",
    "        assert torch.sum(param1 != param2) == 0\n",
    "\n",
    "    print(f\"verify_identical_components- All parameters match! {type(real)} {type(hooked)} \")\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a836519e-7e0b-4e54-9f63-02bb1861f991",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_transformer_blocks(real_block, hooked_block, cfg):\n",
    "    if cfg.esm3_use_torch_layer_norm:\n",
    "        assert torch.sum(real_block.attn.layernorm_qkv[0].weight !=  hooked_block.ln1.weight) == 0\n",
    "        assert torch.sum(real_block.attn.layernorm_qkv[0].bias !=  hooked_block.ln1.bias) == 0\n",
    "    else:\n",
    "        assert torch.sum(real_block.attn.layernorm_qkv[0].weight !=  hooked_block.ln1.w) == 0\n",
    "        assert torch.sum(real_block.attn.layernorm_qkv[0].bias !=  hooked_block.ln1.b) == 0\n",
    "        \n",
    "    qkv_matrix = real_block.attn.layernorm_qkv[1].weight\n",
    "    query_BLD, key_BLD, value_BLD = torch.chunk(qkv_matrix, 3, dim=-2)\n",
    "    q = einops.rearrange(hooked_block.attn.W_Q, \"n_head d_model d_head ->(n_head d_head) d_model\", n_head=hooked_block.attn.W_Q.shape[0])\n",
    "    v = einops.rearrange(hooked_block.attn.W_V, \"n_head d_model d_head ->(n_head d_head) d_model\", n_head=hooked_block.attn.W_V.shape[0])\n",
    "    k = einops.rearrange(hooked_block.attn.W_K, \"n_head d_model d_head ->(n_head d_head) d_model\", n_head=hooked_block.attn.W_K.shape[0])\n",
    "    assert torch.sum(query_BLD !=q) == 0\n",
    "    assert torch.sum(key_BLD !=k) == 0\n",
    "    assert torch.sum(value_BLD !=v) == 0\n",
    "    assert(real_block.attn.layernorm_qkv[1].bias is None)\n",
    "    assert torch.equal(hooked_block.attn.b_Q, torch.zeros_like(hooked_block.attn.b_Q)), \"The tensor is not all zeros.\"\n",
    "    assert torch.equal(hooked_block.attn.b_K, torch.zeros_like(hooked_block.attn.b_K)), \"The tensor is not all zeros.\"\n",
    "    assert torch.equal(hooked_block.attn.b_V, torch.zeros_like(hooked_block.attn.b_V)), \"The tensor is not all zeros.\"\n",
    "    \n",
    "    if cfg.esm3_use_torch_layer_norm:\n",
    "        assert torch.sum(real_block.attn.q_ln.weight !=  hooked_block.attn.q_ln.weight) == 0\n",
    "        assert torch.sum(real_block.attn.k_ln.weight != hooked_block.attn.k_ln.weight) == 0\n",
    "        assert real_block.attn.q_ln.bias is None\n",
    "        assert hooked_block.attn.q_ln.bias is None\n",
    "        assert real_block.attn.k_ln.bias is None\n",
    "        assert hooked_block.attn.k_ln.bias is None\n",
    "    else:\n",
    "        assert torch.sum(real_block.attn.q_ln.weight !=  hooked_block.attn.q_ln.w) == 0\n",
    "        assert torch.sum(real_block.attn.k_ln.weight != hooked_block.attn.k_ln.w) == 0\n",
    "        assert real_block.attn.q_ln.bias is None\n",
    "        assert torch.equal(hooked_block.attn.q_ln.b, torch.zeros_like(hooked_block.attn.q_ln.b)), \"The tensor is not all zeros.\"\n",
    "        assert real_block.attn.k_ln.bias is None\n",
    "        assert torch.equal(hooked_block.attn.k_ln.b, torch.zeros_like(hooked_block.attn.k_ln.b)), \"The tensor is not all zeros.\"\n",
    "\n",
    "    \n",
    "    out_proj = real_block.attn.out_proj.weight\n",
    "    W_O= einops.rearrange(hooked_block.attn.W_O, \"n_head d_head d_model -> d_model (n_head d_head)\", n_head=hooked_block.attn.W_O.shape[0])\n",
    "    assert torch.sum(W_O !=out_proj) == 0\n",
    "    assert real_block.attn.out_proj.bias is None\n",
    "    assert torch.equal(hooked_block.attn.b_O, torch.zeros_like(hooked_block.attn.b_O)), \"The tensor is not all zeros.\"\n",
    "\n",
    "    assert real_block.use_geom_attn == hooked_block.use_geom_attn\n",
    "    if real_block.use_geom_attn:\n",
    "        verify_identical_components(real_block.geom_attn, hooked_block.geom_attn)\n",
    "    if cfg.esm3_use_torch_layer_norm:\n",
    "        assert torch.sum(real_block.ffn[0].weight !=  hooked_block.ln2.weight) == 0\n",
    "        assert torch.sum(real_block.ffn[0].bias !=  hooked_block.ln2.bias) == 0\n",
    "    else:\n",
    "        assert torch.sum(real_block.ffn[0].weight !=  hooked_block.ln2.w) == 0\n",
    "        assert torch.sum(real_block.ffn[0].bias !=  hooked_block.ln2.b) == 0\n",
    "    assert torch.sum(real_block.ffn[1].weight !=  hooked_block.mlp.l1.weight) == 0\n",
    "    assert(real_block.ffn[1].bias is None)\n",
    "    assert(hooked_block.mlp.l1.bias is None)\n",
    "    assert torch.sum(real_block.ffn[3].weight !=  hooked_block.mlp.l2.weight) == 0\n",
    "    assert(real_block.ffn[3].bias is None)\n",
    "    assert(hooked_block.mlp.l2.bias is None)\n",
    "    print(\"compare_transformer_blocks- all params match\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c606b3f5-2d37-4e93-a605-55ab24d062f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_loading(esm3_original, esm3_hooked, cfg):\n",
    "    verify_identical_components(esm3_original.encoder ,esm3_hooked.embed.embed)\n",
    "    for l in range(len(esm3_original.transformer.blocks)):\n",
    "        real_block = esm3_original.transformer.blocks[l]\n",
    "        hooked_block = esm3_hooked.blocks[l]\n",
    "        compare_transformer_blocks(real_block, hooked_block, cfg)\n",
    "    if cfg.esm3_use_torch_layer_norm:\n",
    "        assert torch.sum(esm3_original.transformer.norm.weight !=  esm3_hooked.ln_final.weight) == 0\n",
    "        assert esm3_hooked.ln_final.bias is None\n",
    "        assert esm3_original.transformer.norm.bias is None\n",
    "    else:\n",
    "        assert torch.sum(esm3_original.transformer.norm.weight !=  esm3_hooked.ln_final.w) == 0\n",
    "        assert torch.equal(esm3_hooked.ln_final.b, torch.zeros_like(esm3_hooked.ln_final.b)), \"The tensor is not all zeros.\"\n",
    "    verify_identical_components(esm3_original.output_heads , esm3_hooked.unembed.output_heads)\n",
    "\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99406978-83d9-4118-864e-3db47a163f66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "verify_identical_components- All parameters match! <class 'esm.models.esm3.EncodeInputs'> <class 'esm.models.esm3.EncodeInputs'> \n",
      "verify_identical_components- All parameters match! <class 'esm.layers.geom_attention.GeometricReasoningOriginalImpl'> <class 'esm.layers.geom_attention.GeometricReasoningOriginalImpl'> \n",
      "compare_transformer_blocks- all params match\n",
      "compare_transformer_blocks- all params match\n",
      "compare_transformer_blocks- all params match\n",
      "compare_transformer_blocks- all params match\n",
      "compare_transformer_blocks- all params match\n",
      "compare_transformer_blocks- all params match\n",
      "compare_transformer_blocks- all params match\n",
      "compare_transformer_blocks- all params match\n",
      "compare_transformer_blocks- all params match\n",
      "compare_transformer_blocks- all params match\n",
      "compare_transformer_blocks- all params match\n",
      "compare_transformer_blocks- all params match\n",
      "compare_transformer_blocks- all params match\n",
      "compare_transformer_blocks- all params match\n",
      "compare_transformer_blocks- all params match\n",
      "compare_transformer_blocks- all params match\n",
      "compare_transformer_blocks- all params match\n",
      "compare_transformer_blocks- all params match\n",
      "compare_transformer_blocks- all params match\n",
      "compare_transformer_blocks- all params match\n",
      "compare_transformer_blocks- all params match\n",
      "compare_transformer_blocks- all params match\n",
      "compare_transformer_blocks- all params match\n",
      "compare_transformer_blocks- all params match\n",
      "compare_transformer_blocks- all params match\n",
      "compare_transformer_blocks- all params match\n",
      "compare_transformer_blocks- all params match\n",
      "compare_transformer_blocks- all params match\n",
      "compare_transformer_blocks- all params match\n",
      "compare_transformer_blocks- all params match\n",
      "compare_transformer_blocks- all params match\n",
      "compare_transformer_blocks- all params match\n",
      "compare_transformer_blocks- all params match\n",
      "compare_transformer_blocks- all params match\n",
      "compare_transformer_blocks- all params match\n",
      "compare_transformer_blocks- all params match\n",
      "compare_transformer_blocks- all params match\n",
      "compare_transformer_blocks- all params match\n",
      "compare_transformer_blocks- all params match\n",
      "compare_transformer_blocks- all params match\n",
      "compare_transformer_blocks- all params match\n",
      "compare_transformer_blocks- all params match\n",
      "compare_transformer_blocks- all params match\n",
      "compare_transformer_blocks- all params match\n",
      "compare_transformer_blocks- all params match\n",
      "compare_transformer_blocks- all params match\n",
      "compare_transformer_blocks- all params match\n",
      "compare_transformer_blocks- all params match\n",
      "verify_identical_components- All parameters match! <class 'esm.models.esm3.OutputHeads'> <class 'esm.models.esm3.OutputHeads'> \n"
     ]
    }
   ],
   "source": [
    "test_loading(esm3_original1, esm3_hooked1, esm3_hooked1.cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dcb58659-b880-4f98-9d31-9d2751f23582",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizers = get_esm3_model_tokenizers()\n",
    "esm3_original1.eval()  # Switch to evaluation mode to save memory\n",
    "esm3_hooked1.eval()\n",
    "sequence = \"MARPVSDRTPAPLLLGGPAGTPPGGGALLGLRSLLQGTSKPKEPASCLLKEKERKAALPAATTPGPGLETAGPADAPAGAVVGGGSPRGRPGPVPAPGLLAPLLWERTLPFGDVEYVDLDAFLLEHGLPPSPPPPGGPSPEPSPARTPAPSPGPGSCGSASPRSSPGHAPARAALGTASGHRAGLTSRDTPSPVDPDTVEVLMTFEPDPADLALSSIPGHETFDPRRHRFSEEELKPQPIMKKARKIQVPEEQKDEKYWSRRYKNNEAAKRSRDARRLKENQISVRAAFLEKENALLRQEVVAVRQELSHYRAVLSRYQAQHGAL\"\n",
    "tokens = tokenizers.sequence.encode(sequence)\n",
    "sequence_tokens = torch.tensor(tokens, dtype=torch.int64)\n",
    "sequence_tokens = sequence_tokens.to(device).unsqueeze(0)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output1 = esm3_original1.forward(\n",
    "        sequence_tokens=sequence_tokens\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d5f0636f-27e3-4064-aa47-06a2198b70f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bias: None\n",
      "bias: None\n",
      "bias: None\n",
      "bias: None\n",
      "bias: None\n",
      "bias: None\n",
      "bias: None\n",
      "bias: None\n",
      "bias: None\n",
      "bias: None\n",
      "bias: None\n",
      "bias: None\n",
      "bias: None\n",
      "bias: None\n",
      "bias: None\n",
      "bias: None\n",
      "bias: None\n",
      "bias: None\n",
      "bias: None\n",
      "bias: None\n",
      "bias: None\n",
      "bias: None\n",
      "bias: None\n",
      "bias: None\n",
      "bias: None\n",
      "bias: None\n",
      "bias: None\n",
      "bias: None\n",
      "bias: None\n",
      "bias: None\n",
      "bias: None\n",
      "bias: None\n",
      "bias: None\n",
      "bias: None\n",
      "bias: None\n",
      "bias: None\n",
      "bias: None\n",
      "bias: None\n",
      "bias: None\n",
      "bias: None\n",
      "bias: None\n",
      "bias: None\n",
      "bias: None\n",
      "bias: None\n",
      "bias: None\n",
      "bias: None\n",
      "bias: None\n",
      "bias: None\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    output2 = esm3_hooked1.forward(\n",
    "        sequence_tokens=sequence_tokens\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eba77a4a-6bb7-4215-a812-0f51798257a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.testing import assert_close\n",
    "assert_close(output1.sequence_logits, output2.sequence_logits, rtol=1e-6, atol=2e-5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eeb44910-acaf-412d-bbbe-3035b006d7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sequence2 = \"MLPGLALLLLAAWTARALEVPTDGNAGLLAEPQIAMFCGRLNMHMNVQNGKWDSDPSGTKTCIDTKEGILQYCQEVYPELQITNVVEANQPVTIQNWCKRGRKQCKTHPHFVIPYRCLVGEFVSDALLVPDKCKFLHQERMDVCETHLHWHTVAKETCSEKSTNLHDYGMLLPCGIDKFRGVEFVCCPLAEESDNVDSADAEEDDSDVWWGGADTDYADGSEDKVVEVAEEEEVAEVEEEEADDDEDDEDGDEVEEEAEEPYEEATERTTSIATTTTTTTESVEEVVREVCSEQAETGPCRAMISRWYFDVTEGKCAPFFYGGCGGNRNNFDTEEYCMAVCGSAMSQSLLKTTQEPLARDPVKLPTTAASTPDAVDKYLETPGDENEHAHFQKAKERLEAKHRERMSQVMREWEEAERQAKNLPKADKKAVIQHFQEKVESLEQEAANERQQLVETHMARVEAMLNDRRRLALENYITALQAVPPRPRHVFNMLKKYVRAEQKDRQHTLKHFEHVRMVDPKKAAQIRSQVMTHLRVIYERMNQSLSLLYNVPAVAEEIQDEVDELLQKEQNYSDDVLANMISEPRISYGNDALMPSLTETKTTVELLPVNGEFSLDDLQPWHSFGADSVPANTENEVEPVDARPAADRGLTTRPGSGLTNIKTEEISEVKMDAEFRHDSGYEVHHQKLVFFAEDVGSNKGAIIGLMVGGVVIATVIVITLVMLKKKQYTSIHHGVVEVDAAVTPEERHLSKMQQNGYENPTYKFFEQMQN\"\n",
    "sequence2 = sequence\n",
    "tokens2 = tokenizers.sequence.encode(sequence2)\n",
    "sequence_tokens2 = torch.tensor(tokens2, dtype=torch.int64)\n",
    "sequence_tokens2 = sequence_tokens2.to(device).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1c81f531-1db9-4992-a63e-df2347102776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hook triggered for encoder\n",
      "Hook triggered for layer_norm_output\n",
      "Hook triggered for layer_norm_output\n",
      "after q_ln\n",
      "after k_ln\n",
      "Hook triggered for rotary\n",
      "Hook triggered for org_attn_out_proj_input\n",
      "Hook triggered for org_attn_out\n",
      "Hook triggered for org_geo_attn_out\n",
      "org_mlp_in\n",
      "org_mlp_out\n",
      "org_residual_post\n",
      "Hook triggered for embedding\n",
      "Hook triggered for hook_post_layer_norm\n",
      "Hook triggered for q\n",
      "Hook triggered for q_ln\n",
      "Hook triggered for k_ln\n",
      "Hook triggered hook_rot_q\n",
      "Hook triggered for hook_rot_k\n",
      "Hook triggered for hook_z \n",
      "Hook triggered for attn_out\n",
      "Hook triggered for geo attn_out\n",
      "Hook triggered for hook_mlp_in\n",
      "Hook triggered for hook_mlp_out\n",
      "Hook triggered for hook_residual_post \n"
     ]
    }
   ],
   "source": [
    "encoder_output = None\n",
    "layer_norm_input=None\n",
    "layer_norm_output=None\n",
    "org_attn_out_proj_input=None\n",
    "org_attn_out = None\n",
    "org_geo_attn_out = None\n",
    "org_mlp_in = None\n",
    "org_mlp_out = None\n",
    "org_residual_post = None\n",
    "org_rot = None\n",
    "org_rot_input=None\n",
    "org_q_input = None\n",
    "embed_output = None\n",
    "hook_attn_in=None\n",
    "hook_post_layer_norm=None\n",
    "hook_attn_out=None\n",
    "hook_geo_attn_out=None\n",
    "hook_mlp_in = None\n",
    "hook_mlp_out = None\n",
    "hook_residual_post = None\n",
    "q_ln_org=None\n",
    "k_ln_org=None\n",
    "hook_q = None\n",
    "hook_q_ln=None\n",
    "hook_k_ln=None\n",
    "hook_z = None\n",
    "hook_rot_q= None\n",
    "hook_rot_k=None\n",
    "# Define the hook function\n",
    "def hook_fn1(activation, hook):\n",
    "    global embed_output  # To store the output globally\n",
    "    print(\"Hook triggered for embedding\")\n",
    "    embed_output = activation\n",
    "    \n",
    "# Define the hook function\n",
    "def hook_fn2(activation, hook):\n",
    "    global hook_attn_in  # To store the output globally\n",
    "    print(\"Hook triggered for attn in\")\n",
    "    hook_attn_in = activation\n",
    "    \n",
    "def hook_fn3(activation, hook):\n",
    "    global hook_attn_out  # To store the output globally\n",
    "    print(\"Hook triggered for attn_out\")\n",
    "    hook_attn_out = activation\n",
    "\n",
    "def hook_fn4(activation, hook):\n",
    "    global hook_geo_attn_out  # To store the output globally\n",
    "    print(\"Hook triggered for geo attn_out\")\n",
    "    hook_geo_attn_out = activation\n",
    "\n",
    "def hook_fn5(activation, hook):\n",
    "    global hook_mlp_in   # To store the output globally\n",
    "    print(\"Hook triggered for hook_mlp_in\")\n",
    "    hook_mlp_in  = activation\n",
    "\n",
    "def hook_fn6(activation, hook):\n",
    "    global hook_mlp_out  # To store the output globally\n",
    "    print(\"Hook triggered for hook_mlp_out\")\n",
    "    hook_mlp_out  = activation\n",
    "\n",
    "def hook_fn7(activation, hook):\n",
    "    global hook_residual_post   # To store the output globally\n",
    "    print(\"Hook triggered for hook_residual_post \")\n",
    "    hook_residual_post  = activation\n",
    "    \n",
    "def hook_fn8(module, input, output):\n",
    "    global encoder_output  # Declare as global to modify the global variable\n",
    "    print(\"Hook triggered for encoder\")\n",
    "    encoder_output = output \n",
    "\n",
    "def hook_fn9(module, input, output):\n",
    "    global layer_norm_output  # Declare as global to modify the global variable\n",
    "    print(\"Hook triggered for layer_norm_output\")\n",
    "    layer_norm_output = output \n",
    "def hook_fn10(module, input, output):\n",
    "    global org_attn_out  # Declare as global to modify the global variable\n",
    "    print(\"Hook triggered for org_attn_out\")\n",
    "    org_attn_out = output\n",
    "def hook_fn11(module, input, output):\n",
    "    global org_geo_attn_out # Declare as global to modify the global variable\n",
    "    print(\"Hook triggered for org_geo_attn_out\")\n",
    "    org_geo_attn_out = output\n",
    "def hook_fn12(module, input, output):\n",
    "    global org_mlp_in # Declare as global to modify the global variable\n",
    "    print(\"org_mlp_in\")\n",
    "    org_mlp_in = input\n",
    "def hook_fn13(module, input, output):\n",
    "    global org_mlp_out # Declare as global to modify the global variable\n",
    "    print(\"org_mlp_out\")\n",
    "    org_mlp_out = output\n",
    "def hook_fn14(module, input, output):\n",
    "    global org_residual_post # Declare as global to modify the global variable\n",
    "    print(\"org_residual_post\")\n",
    "    org_residual_post = output\n",
    "\n",
    "def hook_fn15(activation, hook):\n",
    "    global hook_q_ln   # To store the output globally\n",
    "    print(\"Hook triggered for q_ln\")\n",
    "    hook_q_ln  = activation\n",
    "    \n",
    "def hook_fn16(activation, hook):\n",
    "    global hook_k_ln   # To store the output globally\n",
    "    print(\"Hook triggered for k_ln\")\n",
    "    hook_k_ln  = activation\n",
    "def hook_fn17(module, input, output):\n",
    "    global q_ln_org # Declare as global to modify the global variable\n",
    "    global org_q_input\n",
    "    print(\"after q_ln\")\n",
    "    q_ln_org = output\n",
    "    org_q_input = input\n",
    "\n",
    "def hook_fn18(module, input, output):\n",
    "    global k_ln_org # Declare as global to modify the global variable\n",
    "    print(\"after k_ln\")\n",
    "    k_ln_org = output\n",
    "\n",
    "def hook_fn19(activation, hook):\n",
    "    global hook_post_layer_norm  # To store the output globally\n",
    "    print(\"Hook triggered for hook_post_layer_norm\")\n",
    "    hook_post_layer_norm = activation\n",
    "\n",
    "def hook_fn20(module, input, output):\n",
    "    global layer_norm_input  # Declare as global to modify the global variable\n",
    "    print(\"Hook triggered for layer_norm_input\")\n",
    "    layer_norm_input = input\n",
    "def hook_fn21(module, input, output):\n",
    "    global org_attn_out_proj_input  # Declare as global to modify the global variable\n",
    "    print(\"Hook triggered for org_attn_out_proj_input\")\n",
    "    org_attn_out_proj_input = input\n",
    "def hook_fn22(activation, hook):\n",
    "    global hook_z   # To store the output globally\n",
    "    print(\"Hook triggered for hook_z \")\n",
    "    hook_z  = activation\n",
    "def rotary_hook(module, input, output):\n",
    "    global org_rot, org_rot_input\n",
    "    print(\"Hook triggered for rotary\")\n",
    "    org_rot = output\n",
    "    org_rot_input = input\n",
    "\n",
    "    org_rot = output\n",
    "def hook_fn24(activation, hook):\n",
    "    global hook_rot_q   # To store the output globally\n",
    "    print(\"Hook triggered hook_rot_q\")\n",
    "    hook_rot_q  = activation\n",
    "    \n",
    "def hook_fn25(activation, hook):\n",
    "    global hook_rot_k  # To store the output globally\n",
    "    print(\"Hook triggered for hook_rot_k\")\n",
    "    hook_rot_k  = activation\n",
    "\n",
    "def hook_q(activation, hook):\n",
    "    global hook_q\n",
    "    print(\"Hook triggered for q\")\n",
    "    hook_q = activation\n",
    "    \n",
    "from torch.testing import assert_close\n",
    "esm3_hooked1.eval()\n",
    "esm3_original1.eval()\n",
    "with torch.no_grad():\n",
    "    esm3_original1.encoder.register_forward_hook(hook_fn8)\n",
    "    esm3_original1.transformer.blocks[0].attn.layernorm_qkv[0].register_forward_hook(hook_fn9)\n",
    "    esm3_original1.transformer.blocks[0].attn.out_proj.register_forward_hook(hook_fn21)\n",
    "    esm3_original1.transformer.blocks[0].attn.rotary.register_forward_hook(rotary_hook)\n",
    "    esm3_original1.transformer.blocks[0].attn.layernorm_qkv[0].register_forward_hook(hook_fn9)\n",
    "    esm3_original1.transformer.blocks[0].attn.q_ln.register_forward_hook(hook_fn17)\n",
    "    esm3_original1.transformer.blocks[0].attn.k_ln.register_forward_hook(hook_fn18)\n",
    "    esm3_original1.transformer.blocks[0].attn.register_forward_hook(hook_fn10)\n",
    "    esm3_original1.transformer.blocks[0].geom_attn.register_forward_hook(hook_fn11)\n",
    "    esm3_original1.transformer.blocks[0].ffn.register_forward_hook(hook_fn12)\n",
    "    esm3_original1.transformer.blocks[0].ffn.register_forward_hook(hook_fn13)\n",
    "    esm3_original1.transformer.blocks[0].register_forward_hook(hook_fn14)\n",
    "    output3 = esm3_original1.forward(\n",
    "        sequence_tokens=sequence_tokens2\n",
    "    )\n",
    "\n",
    "with torch.no_grad():\n",
    "    esm3_hooked1.add_hook(\"hook_embed\", hook_fn1)\n",
    "    esm3_hooked1.add_hook(\"blocks.0.hook_attn_in\", hook_fn2)\n",
    "    esm3_hooked1.add_hook(\"blocks.0.hook_post_layer_norm\", hook_fn19)\n",
    "    esm3_hooked1.add_hook(\"blocks.0.hook_attn_out\", hook_fn3)\n",
    "    esm3_hooked1.add_hook(\"blocks.0.hook_geo_attn_out\", hook_fn4)\n",
    "    esm3_hooked1.add_hook(\"blocks.0.hook_mlp_in\", hook_fn5)\n",
    "    esm3_hooked1.add_hook(\"blocks.0.hook_mlp_out\", hook_fn6)\n",
    "    esm3_hooked1.add_hook(\"blocks.0.hook_resid_post\", hook_fn7)\n",
    "    esm3_hooked1.add_hook(\"blocks.0.attn.hook_ln_q\", hook_fn15)\n",
    "    esm3_hooked1.add_hook(\"blocks.0.attn.hook_ln_k\", hook_fn16)\n",
    "    esm3_hooked1.add_hook(\"blocks.0.attn.hook_z\", hook_fn22)\n",
    "    esm3_hooked1.add_hook(\"blocks.0.attn.hook_rot_k\", hook_fn25)\n",
    "    esm3_hooked1.add_hook(\"blocks.0.attn.hook_rot_q\", hook_fn24)\n",
    "    esm3_hooked1.add_hook(\"blocks.0.attn.hook_q\", hook_q)\n",
    "    output4 = esm3_hooked1.forward(\n",
    "        sequence_tokens=sequence_tokens2\n",
    "    )\n",
    "\n",
    "assert torch.allclose(output3.sequence_logits, output4.sequence_logits, atol=1e-4, rtol=1e-4)\n",
    "\n",
    "assert torch.allclose(output3.structure_logits, output4.structure_logits, atol=1e-4, rtol=1e-4)\n",
    "\n",
    "assert torch.allclose(output3.sasa_logits, output4.sasa_logits, atol=1e-4, rtol=1e-4)\n",
    "\n",
    "assert torch.allclose(output3.secondary_structure_logits, output4.secondary_structure_logits, atol=1e-4, rtol=1e-4)\n",
    "\n",
    "#assert torch.allclose(output3.function_logits, output4.function_logits, atol=1e-4, rtol=1e-4)\n",
    "\n",
    "assert torch.allclose(output3.residue_logits, output4.residue_logits, atol=1e-4, rtol=1e-4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7aaf2fd0-ea2f-4c8c-bbad-73a25dcab128",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert torch.equal(encoder_output,embed_output)\n",
    "#assert torch.equal(layer_norm_output, hook_post_layer_norm[...,1,:])\n",
    "q_flattened = einops.rearrange(hook_q_ln, \"batch pos head_index d_head -> batch pos (head_index d_head)\")\n",
    "assert torch.allclose(q_ln_org, q_flattened, rtol=1.3e-6, atol=4e-5)\n",
    "k_flattened = einops.rearrange(hook_k_ln, \"batch pos head_index d_head -> batch pos (head_index d_head)\")\n",
    "assert torch.allclose(k_ln_org, k_flattened, rtol=1.3e-6, atol=4e-5)\n",
    "assert torch.allclose(hook_rot_k, org_rot[1], rtol=1.3e-6, atol=4e-5)\n",
    "assert torch.allclose(hook_rot_q, org_rot[0], rtol=1.3e-6, atol=4e-5)\n",
    "assert torch.allclose(org_attn_out, hook_attn_out, rtol=1.3e-6, atol=4e-5)\n",
    "assert torch.allclose(org_geo_attn_out,hook_geo_attn_out, rtol=1.3e-6, atol=4e-5)\n",
    "assert torch.allclose(hook_mlp_in, org_mlp_in[0], rtol=1.3e-6, atol=4e-5)\n",
    "assert torch.allclose(hook_mlp_out, org_mlp_out, rtol=3e-6, atol=4e-5)\n",
    "assert torch.allclose(hook_residual_post, org_residual_post, rtol=1.3e-6, atol=4e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d31a82a2-45b9-46db-9cb5-9d6cacc73980",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "allclose(): argument 'other' (position 2) must be Tensor, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mallclose\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhook_mlp_in\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morg_mlp_in\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: allclose(): argument 'other' (position 2) must be Tensor, not tuple"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a0e8ed79-33d1-473e-9950-747a9d01418f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hook triggered for embedding\n",
      "Hook triggered for hook_post_layer_norm\n",
      "Hook triggered for q\n",
      "Hook triggered for q_ln\n",
      "Hook triggered for k_ln\n",
      "Hook triggered hook_rot_q\n",
      "Hook triggered for hook_rot_k\n",
      "Hook triggered for hook_z \n",
      "Hook triggered for attn_out\n",
      "Hook triggered for geo attn_out\n",
      "Hook triggered for hook_mlp_in\n",
      "Hook triggered for hook_mlp_out\n",
      "Hook triggered for hook_residual_post \n"
     ]
    }
   ],
   "source": [
    "res, cache = esm3_hooked1.run_with_cache(sequence_tokens=sequence_tokens2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b0acbe15-ba82-4d97-90c4-eab32a124062",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 327, 24, 64])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cache['blocks.8.attn.hook_q'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eb291f2e-ae10-499f-a4bb-9fe7906b796a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([48, 1536, 4096])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "esm3_hooked1.W_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d7276367-5861-4ff3-8efc-5f236526540a",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torch' has no attribute '__version'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__version\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/transformer_lens_10/lib/python3.10/site-packages/torch/__init__.py:2562\u001b[0m, in \u001b[0;36m__getattr__\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m   2559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m _lazy_modules:\n\u001b[1;32m   2560\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m-> 2562\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodule \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'torch' has no attribute '__version'"
     ]
    }
   ],
   "source": [
    "torch.__version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563be139-6eec-4c53-86bf-fac779e58042",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
