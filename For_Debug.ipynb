{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8aa05580-5cab-4b40-90bf-3234c3c34b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Attention:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "176ac421-cfdf-47b5-a88f-39b22dcc337e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "import torch\n",
    "from jaxtyping import Float\n",
    "from torch.testing import assert_close\n",
    "import torch.nn as nn\n",
    "from transformer_lens.components import Attention\n",
    "from transformer_lens.components import LayerNorm\n",
    "from transformer_lens.components import HookedESM3MLP, swiglu_correction_fn\n",
    "from transformer_lens.components import HookedEsm3UnifiedTransformerBlock\n",
    "from esm.layers.attention import MultiHeadAttention\n",
    "from esm.layers.blocks import swiglu_ln_ffn, UnifiedTransformerBlock\n",
    "from transformer_lens.HookedTransformerConfig import HookedTransformerConfig\n",
    "import functools\n",
    "import einops\n",
    "from esm.utils.constants.esm3 import data_root\n",
    "import math\n",
    "from transformer_lens import HookedESM3,SupportedESM3Config\n",
    "from esm.pretrained import (\n",
    "    ESM3_sm_open_v0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "384605f0-1c01-4a45-8388-d66210b619dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_multi_head_attention_params(d_model, n_heads, bias=False, qk_layernorm=False):\n",
    "    params = {\n",
    "        \"layernorm_qkv_weight\": torch.rand(d_model),  # Weight of LayerNorm\n",
    "        \"layernorm_qkv_bias\": torch.rand(d_model),    # Bias of LayerNorm\n",
    "        \"W_qkv_weight\": torch.rand(d_model * 3, d_model),  # Weight of Linear layer\n",
    "        \"W_qkv_bias\": torch.rand(d_model * 3) if bias else None,  # Bias of Linear layer\n",
    "        \"out_proj_weight\": torch.rand(d_model, d_model),  # Output projection weight\n",
    "        \"out_proj_bias\": torch.rand(d_model) if bias else None,  # Output projection bias\n",
    "    }\n",
    "    \n",
    "    if qk_layernorm:\n",
    "        params.update({\n",
    "            \"q_ln_weight\": torch.rand(d_model),\n",
    "            \"q_ln_bias\": torch.rand(d_model) if bias else None,\n",
    "            \"k_ln_weight\": torch.rand(d_model),\n",
    "            \"k_ln_bias\": torch.rand(d_model) if bias else None,\n",
    "        })\n",
    "    return params\n",
    "    \n",
    "\n",
    "def assign_params_to_esm_attention_layer(layer, params, bias=True):\n",
    "    with torch.no_grad():\n",
    "        # Assign LayerNorm for QKV\n",
    "        layer.layernorm_qkv[0].weight.copy_(params[\"layernorm_qkv_weight\"])\n",
    "        layer.layernorm_qkv[0].bias.copy_(params[\"layernorm_qkv_bias\"])\n",
    "        \n",
    "        # Assign Weights and Bias for QKV Projection\n",
    "        layer.layernorm_qkv[1].weight.copy_(params[\"W_qkv_weight\"])\n",
    "        if bias:\n",
    "            layer.layernorm_qkv[1].bias.copy_(params[\"W_qkv_bias\"])\n",
    "        \n",
    "        # Assign Output Projection\n",
    "        layer.out_proj.weight.copy_(params[\"out_proj_weight\"])\n",
    "        if bias:\n",
    "            layer.out_proj.bias.copy_(params[\"out_proj_bias\"])\n",
    "        \n",
    "        # Assign LayerNorm for Q\n",
    "        if isinstance(layer.q_ln, nn.LayerNorm):\n",
    "            layer.q_ln.weight.copy_(params[\"q_ln_weight\"])\n",
    "            if bias:\n",
    "                layer.q_ln.bias.copy_(params[\"q_ln_bias\"])\n",
    "        \n",
    "        # Assign LayerNorm for K\n",
    "        if isinstance(layer.k_ln, nn.LayerNorm):\n",
    "            layer.k_ln.weight.copy_(params[\"k_ln_weight\"])\n",
    "            if bias:\n",
    "                layer.k_ln.bias.copy_(params[\"k_ln_bias\"])\n",
    "\n",
    "def assign_params_to_transformer_lens_attention_layer(attention_layer, pre_layer_norm, params, cfg, bias=True):\n",
    "    with torch.no_grad():\n",
    "        # Assign LayerNorm QKV\n",
    "        if isinstance(pre_layer_norm, nn.LayerNorm):\n",
    "            pre_layer_norm.weight.copy_(params[\"layernorm_qkv_weight\"])\n",
    "            pre_layer_norm.bias.copy_(params[\"layernorm_qkv_bias\"])\n",
    "        else:\n",
    "            pre_layer_norm.w.copy_(params[\"layernorm_qkv_weight\"])\n",
    "            pre_layer_norm.b.copy_(params[\"layernorm_qkv_bias\"])\n",
    "\n",
    "        # Extract and split QKV weights\n",
    "        qkv_matrix = params[\"W_qkv_weight\"].clone()  # Shape: (d_model * 3, d_model)\n",
    "        assert qkv_matrix.shape == (cfg.d_model * 3, cfg.d_model), \"QKV weight shape mismatch.\"\n",
    "\n",
    "        qkv_reshaped = qkv_matrix.T  # Shape: (d_model, d_model * 3)\n",
    "        q, k, v = torch.chunk(qkv_reshaped, 3, dim=-1)  # Split into Q, K, V\n",
    "        \n",
    "        reshaper = functools.partial(\n",
    "            einops.rearrange, pattern=\"d_model (n_head d_head) -> n_head d_model d_head\", n_head=cfg.n_heads\n",
    "        )\n",
    "        q, k, v = map(reshaper, (q, k, v))\n",
    "        \n",
    "        # Copy Q, K, V weights\n",
    "        attention_layer.W_Q.copy_(q)\n",
    "        attention_layer.W_K.copy_(k)\n",
    "        attention_layer.W_V.copy_(v)\n",
    "\n",
    "        # Handle QKV bias\n",
    "        if bias and \"W_qkv_bias\" in params:\n",
    "            qkv_bias = params[\"W_qkv_bias\"].clone()  # Shape: (d_model * 3)\n",
    "            b_q, b_k, b_v = torch.chunk(qkv_bias, 3, dim=-1)\n",
    "            reshaper_bias = functools.partial(\n",
    "                einops.rearrange, pattern=\"(n_head d_head) -> n_head d_head\", n_head=cfg.n_heads\n",
    "            )\n",
    "            attention_layer.b_Q.copy_(reshaper_bias(b_q))\n",
    "            attention_layer.b_K.copy_(reshaper_bias(b_k))\n",
    "            attention_layer.b_V.copy_(reshaper_bias(b_v))\n",
    "\n",
    "        # Assign Output Projection\n",
    "        out_proj = params[\"out_proj_weight\"].clone()  # Shape: (d_model, d_model)\n",
    "        assert out_proj.shape == (cfg.d_model, cfg.d_model), \"Output projection weight shape mismatch.\"\n",
    "        out_proj_reshaped = einops.rearrange(out_proj.T, \"(n_head d_head) d_model -> n_head d_head d_model\", n_head=cfg.n_heads)\n",
    "        attention_layer.W_O.copy_(out_proj_reshaped)\n",
    "\n",
    "        # Assign Output Bias\n",
    "        if bias and \"out_proj_bias\" in params:\n",
    "            attention_layer.b_O.copy_(params[\"out_proj_bias\"])\n",
    "\n",
    "        # Assign LayerNorms for Q and K if qk_layernorm is enabled\n",
    "        if cfg.qk_layernorm:\n",
    "            attention_layer.q_ln.w.copy_(params[\"q_ln_weight\"])\n",
    "            attention_layer.k_ln.w.copy_(params[\"k_ln_weight\"])\n",
    "            if bias:\n",
    "                attention_layer.q_ln.b.copy_(params.get(\"q_ln_bias\", torch.zeros(cfg.d_model)))\n",
    "                attention_layer.k_ln.b.copy_(params.get(\"k_ln_bias\", torch.zeros(cfg.d_model)))\n",
    "\n",
    "\n",
    "\n",
    "def create_mlp_params(d_model, expansion_ratio, bias):\n",
    "    hidden_dim = swiglu_correction_fn(expansion_ratio, d_model)\n",
    "    params = {\n",
    "        \"layernorm_weight\": torch.rand(d_model),\n",
    "        \"layernorm_bias\": torch.rand(d_model),\n",
    "        \"l1_weight\": torch.rand(hidden_dim * 2, d_model),\n",
    "        \"l1_bias\": torch.rand(hidden_dim * 2) if bias else None,\n",
    "        \"l2_weight\": torch.rand(d_model, hidden_dim),\n",
    "        \"l2_bias\": torch.rand(d_model) if bias else None,\n",
    "    }\n",
    "    return params\n",
    "\n",
    "\n",
    "def assign_params_to_swiglu_mlp(mdl, params, bias):\n",
    "    with torch.no_grad():\n",
    "        # Assign LayerNorm parameters\n",
    "        mdl[0].weight.copy_(params[\"layernorm_weight\"])\n",
    "        mdl[0].bias.copy_(params[\"layernorm_bias\"])\n",
    "        # Assign first Linear layer parameters\n",
    "        mdl[1].weight.copy_(params[\"l1_weight\"])\n",
    "        if bias:\n",
    "            mdl[1].bias.copy_(params[\"l1_bias\"])\n",
    "        # Assign second Linear layer parameters\n",
    "        mdl[3].weight.copy_(params[\"l2_weight\"])\n",
    "        if bias:\n",
    "            mdl[3].bias.copy_(params[\"l2_bias\"])\n",
    "\n",
    "def assign_params_to_esm_mlp(mdl, params, bias, pre_layer_norm):\n",
    "    with torch.no_grad():\n",
    "        # Assign LayerNorm\n",
    "        if isinstance(pre_layer_norm, nn.LayerNorm): \n",
    "            pre_layer_norm.weight.copy_(params[\"layernorm_weight\"])\n",
    "            pre_layer_norm.bias.copy_(params[\"layernorm_bias\"])\n",
    "        else:\n",
    "            pre_layer_norm.w.copy_(params[\"layernorm_weight\"])\n",
    "            pre_layer_norm.b.copy_(params[\"layernorm_bias\"])\n",
    "        # Assign first Linear layer parameters\n",
    "        mdl.l1.weight.copy_(params[\"l1_weight\"])\n",
    "        if bias:\n",
    "            mdl.l1.bias.copy_(params[\"l1_bias\"])\n",
    "        # Assign second Linear layer parameters\n",
    "        mdl.l2.weight.copy_(params[\"l2_weight\"])\n",
    "        if bias:\n",
    "            mdl.l2.bias.copy_(params[\"l2_bias\"])\n",
    "            \n",
    "def assign_params_to_hooked_esm3_transformer_block(block:HookedEsm3UnifiedTransformerBlock, attention_params, mlp_params ,bias, cfg):\n",
    "    attn = block.attn\n",
    "    attn_layer_norm = block.ln1\n",
    "    assign_params_to_transformer_lens_attention_layer(attn, attn_layer_norm, attention_params, cfg, bias=bias)\n",
    "    mlp = block.mlp\n",
    "    mlp_layer_norm = block.ln2\n",
    "    assign_params_to_esm_mlp(mlp, mlp_params,bias,mlp_layer_norm)\n",
    "\n",
    "\n",
    "def assign_params_to_original_transformer_block(block:UnifiedTransformerBlock, attention_params, mlp_params ,bias):\n",
    "    attention_layer = block.attn\n",
    "    assign_params_to_esm_attention_layer(attention_layer, attention_params, bias)\n",
    "    mlp = block.ffn\n",
    "    assign_params_to_swiglu_mlp(mlp, mlp_params, bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30740ede-25e7-446a-bdc1-99c6d2c7b3ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum absolute difference: tensor(130.)\n",
      "Mean absolute difference: tensor(50.9646)\n"
     ]
    }
   ],
   "source": [
    "d_model = 512\n",
    "n_heads = 8\n",
    "d_head = d_model // n_heads\n",
    "expansion_ratio = 4.0\n",
    "batch_size = 1\n",
    "seq_len = 10\n",
    "qk_layernorm = True\n",
    "residue_scaling_factor = math.sqrt(48 / 36)\n",
    "use_attn_in=False\n",
    "use_hook_mlp_in=False\n",
    "use_split_qkv_input=False\n",
    "bias=False\n",
    "\n",
    "attention_fake_params = create_multi_head_attention_params(d_model=d_model,  n_heads=n_heads, \n",
    "qk_layernorm=qk_layernorm, bias=bias)\n",
    "\n",
    "mlp_fake_params = create_mlp_params(d_model=d_model, expansion_ratio=expansion_ratio, bias=bias)\n",
    "\n",
    "original_block:UnifiedTransformerBlock = UnifiedTransformerBlock(\n",
    "    d_model=d_model,\n",
    "    n_heads=n_heads,\n",
    "    use_geom_attn=False,\n",
    "    use_plain_attn=True,\n",
    "    v_heads=None,\n",
    "    bias=bias,\n",
    "    expansion_ratio=expansion_ratio,\n",
    "    residue_scaling_factor=residue_scaling_factor,\n",
    "    qk_layernorm= qk_layernorm,\n",
    "    ffn_type=\"swiglu\",\n",
    ")\n",
    "\n",
    "assign_params_to_original_transformer_block(original_block, attention_fake_params, mlp_fake_params, bias)\n",
    "# Initialize HookedEsm3UnifiedTransformerBlock\n",
    "cfg = HookedTransformerConfig(\n",
    "n_layers=1,           \n",
    "d_model=d_model,           \n",
    "n_ctx=20,            \n",
    "d_head=d_head,                     \n",
    "n_heads=n_heads,\n",
    "attention_dir=\"bidirectional\",\n",
    "init_weights=False,\n",
    "positional_embedding_type=\"rotary\",\n",
    "rotary_dim=d_head,\n",
    "default_prepend_bos=False,\n",
    "qk_layernorm=qk_layernorm,\n",
    "dtype=torch.float32,\n",
    "use_attn_result=False, \n",
    "esm3_mlp_expansion_ratio=expansion_ratio,\n",
    "act_fn = \"swiglu\",\n",
    "esm3_bias = bias,\n",
    "use_attn_in = use_attn_in,\n",
    "use_hook_mlp_in = use_attn_in,\n",
    "use_split_qkv_input= use_attn_in,\n",
    "esm3_scaling_factor=residue_scaling_factor\n",
    ")\n",
    "hooked_block:HookedEsm3UnifiedTransformerBlock = HookedEsm3UnifiedTransformerBlock(cfg, block_index=0)\n",
    "assign_params_to_hooked_esm3_transformer_block(hooked_block,attention_fake_params, mlp_fake_params, bias, cfg)\n",
    "\n",
    "# Input tensor\n",
    "x = torch.rand((batch_size, seq_len, d_model))\n",
    "\n",
    "# Forward pass\n",
    "with torch.no_grad():\n",
    "    original_output =  original_block.forward(x.clone(), None, None, None, None)\n",
    "    hooked_output = hooked_block.forward(x.clone())\n",
    "\n",
    "# Compare outputs\n",
    "assert torch.allclose(original_output, hooked_output, atol=1e-5, rtol=1e-4), \"Outputs do not match!\"\n",
    "print(\"Maximum absolute difference:\", torch.max(torch.abs(original_output - hooked_output)))\n",
    "print(\"Mean absolute difference:\", torch.mean(torch.abs(original_output - hooked_output)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7683190-3753-4ec9-8c29-7fd7a101e1b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Please notice the licsence - todo- add licenseSupport for ESM3 in TransformerLens is currently experimental, until such a time when it has feature parity with HookedTransformer and has been tested on real research tasks. Until then, backward compatibility is not guaranteed. Please see the docs for information on the limitations of the current implementation.\n",
      "If using ESM3 for interpretability research, keep in mind that ESM3 has some significant architectural differences to Language transformers like GPT.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  cuda\n",
      "Loaded pretrained model esm3_sm_open_v1 into HookedESM3\n"
     ]
    }
   ],
   "source": [
    "config = SupportedESM3Config(\n",
    "    use_attn_result=False,\n",
    "    use_split_qkv_input=False,\n",
    "    use_hook_mlp_in=False,\n",
    "    use_attn_in=False,\n",
    "    esm3_output_type=None)\n",
    "esm3_hooked = HookedESM3.from_pretrained(esm_cfg=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d03a2175-423d-4f17-9908-cde98db7a4b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['embed.embed.sequence_embed.weight', 'embed.embed.plddt_projection.weight', 'embed.embed.plddt_projection.bias', 'embed.embed.structure_per_res_plddt_projection.weight', 'embed.embed.structure_per_res_plddt_projection.bias', 'embed.embed.structure_tokens_embed.weight', 'embed.embed.ss8_embed.weight', 'embed.embed.sasa_embed.weight', 'embed.embed.function_embed.0.weight', 'embed.embed.function_embed.1.weight', 'embed.embed.function_embed.2.weight', 'embed.embed.function_embed.3.weight', 'embed.embed.function_embed.4.weight', 'embed.embed.function_embed.5.weight', 'embed.embed.function_embed.6.weight', 'embed.embed.function_embed.7.weight', 'embed.embed.residue_embed.weight', 'blocks.0.ln1.w', 'blocks.0.ln1.b', 'blocks.0.ln2.w', 'blocks.0.ln2.b', 'blocks.0.attn.W_Q', 'blocks.0.attn.W_O', 'blocks.0.attn.b_Q', 'blocks.0.attn.b_O', 'blocks.0.attn.W_K', 'blocks.0.attn.W_V', 'blocks.0.attn.b_K', 'blocks.0.attn.b_V', 'blocks.0.attn.mask', 'blocks.0.attn.IGNORE', 'blocks.0.attn.rotary_sin', 'blocks.0.attn.rotary_cos', 'blocks.0.attn.q_ln.w', 'blocks.0.attn.q_ln.b', 'blocks.0.attn.k_ln.w', 'blocks.0.attn.k_ln.b', 'blocks.0.mlp.l1.weight', 'blocks.0.mlp.l2.weight', 'blocks.0.geom_attn.distance_scale_per_head', 'blocks.0.geom_attn.rotation_scale_per_head', 'blocks.0.geom_attn.s_norm.weight', 'blocks.0.geom_attn.proj.weight', 'blocks.0.geom_attn.out_proj.weight', 'blocks.1.ln1.w', 'blocks.1.ln1.b', 'blocks.1.ln2.w', 'blocks.1.ln2.b', 'blocks.1.attn.W_Q', 'blocks.1.attn.W_O', 'blocks.1.attn.b_Q', 'blocks.1.attn.b_O', 'blocks.1.attn.W_K', 'blocks.1.attn.W_V', 'blocks.1.attn.b_K', 'blocks.1.attn.b_V', 'blocks.1.attn.mask', 'blocks.1.attn.IGNORE', 'blocks.1.attn.rotary_sin', 'blocks.1.attn.rotary_cos', 'blocks.1.attn.q_ln.w', 'blocks.1.attn.q_ln.b', 'blocks.1.attn.k_ln.w', 'blocks.1.attn.k_ln.b', 'blocks.1.mlp.l1.weight', 'blocks.1.mlp.l2.weight', 'blocks.2.ln1.w', 'blocks.2.ln1.b', 'blocks.2.ln2.w', 'blocks.2.ln2.b', 'blocks.2.attn.W_Q', 'blocks.2.attn.W_O', 'blocks.2.attn.b_Q', 'blocks.2.attn.b_O', 'blocks.2.attn.W_K', 'blocks.2.attn.W_V', 'blocks.2.attn.b_K', 'blocks.2.attn.b_V', 'blocks.2.attn.mask', 'blocks.2.attn.IGNORE', 'blocks.2.attn.rotary_sin', 'blocks.2.attn.rotary_cos', 'blocks.2.attn.q_ln.w', 'blocks.2.attn.q_ln.b', 'blocks.2.attn.k_ln.w', 'blocks.2.attn.k_ln.b', 'blocks.2.mlp.l1.weight', 'blocks.2.mlp.l2.weight', 'blocks.3.ln1.w', 'blocks.3.ln1.b', 'blocks.3.ln2.w', 'blocks.3.ln2.b', 'blocks.3.attn.W_Q', 'blocks.3.attn.W_O', 'blocks.3.attn.b_Q', 'blocks.3.attn.b_O', 'blocks.3.attn.W_K', 'blocks.3.attn.W_V', 'blocks.3.attn.b_K', 'blocks.3.attn.b_V', 'blocks.3.attn.mask', 'blocks.3.attn.IGNORE', 'blocks.3.attn.rotary_sin', 'blocks.3.attn.rotary_cos', 'blocks.3.attn.q_ln.w', 'blocks.3.attn.q_ln.b', 'blocks.3.attn.k_ln.w', 'blocks.3.attn.k_ln.b', 'blocks.3.mlp.l1.weight', 'blocks.3.mlp.l2.weight', 'blocks.4.ln1.w', 'blocks.4.ln1.b', 'blocks.4.ln2.w', 'blocks.4.ln2.b', 'blocks.4.attn.W_Q', 'blocks.4.attn.W_O', 'blocks.4.attn.b_Q', 'blocks.4.attn.b_O', 'blocks.4.attn.W_K', 'blocks.4.attn.W_V', 'blocks.4.attn.b_K', 'blocks.4.attn.b_V', 'blocks.4.attn.mask', 'blocks.4.attn.IGNORE', 'blocks.4.attn.rotary_sin', 'blocks.4.attn.rotary_cos', 'blocks.4.attn.q_ln.w', 'blocks.4.attn.q_ln.b', 'blocks.4.attn.k_ln.w', 'blocks.4.attn.k_ln.b', 'blocks.4.mlp.l1.weight', 'blocks.4.mlp.l2.weight', 'blocks.5.ln1.w', 'blocks.5.ln1.b', 'blocks.5.ln2.w', 'blocks.5.ln2.b', 'blocks.5.attn.W_Q', 'blocks.5.attn.W_O', 'blocks.5.attn.b_Q', 'blocks.5.attn.b_O', 'blocks.5.attn.W_K', 'blocks.5.attn.W_V', 'blocks.5.attn.b_K', 'blocks.5.attn.b_V', 'blocks.5.attn.mask', 'blocks.5.attn.IGNORE', 'blocks.5.attn.rotary_sin', 'blocks.5.attn.rotary_cos', 'blocks.5.attn.q_ln.w', 'blocks.5.attn.q_ln.b', 'blocks.5.attn.k_ln.w', 'blocks.5.attn.k_ln.b', 'blocks.5.mlp.l1.weight', 'blocks.5.mlp.l2.weight', 'blocks.6.ln1.w', 'blocks.6.ln1.b', 'blocks.6.ln2.w', 'blocks.6.ln2.b', 'blocks.6.attn.W_Q', 'blocks.6.attn.W_O', 'blocks.6.attn.b_Q', 'blocks.6.attn.b_O', 'blocks.6.attn.W_K', 'blocks.6.attn.W_V', 'blocks.6.attn.b_K', 'blocks.6.attn.b_V', 'blocks.6.attn.mask', 'blocks.6.attn.IGNORE', 'blocks.6.attn.rotary_sin', 'blocks.6.attn.rotary_cos', 'blocks.6.attn.q_ln.w', 'blocks.6.attn.q_ln.b', 'blocks.6.attn.k_ln.w', 'blocks.6.attn.k_ln.b', 'blocks.6.mlp.l1.weight', 'blocks.6.mlp.l2.weight', 'blocks.7.ln1.w', 'blocks.7.ln1.b', 'blocks.7.ln2.w', 'blocks.7.ln2.b', 'blocks.7.attn.W_Q', 'blocks.7.attn.W_O', 'blocks.7.attn.b_Q', 'blocks.7.attn.b_O', 'blocks.7.attn.W_K', 'blocks.7.attn.W_V', 'blocks.7.attn.b_K', 'blocks.7.attn.b_V', 'blocks.7.attn.mask', 'blocks.7.attn.IGNORE', 'blocks.7.attn.rotary_sin', 'blocks.7.attn.rotary_cos', 'blocks.7.attn.q_ln.w', 'blocks.7.attn.q_ln.b', 'blocks.7.attn.k_ln.w', 'blocks.7.attn.k_ln.b', 'blocks.7.mlp.l1.weight', 'blocks.7.mlp.l2.weight', 'blocks.8.ln1.w', 'blocks.8.ln1.b', 'blocks.8.ln2.w', 'blocks.8.ln2.b', 'blocks.8.attn.W_Q', 'blocks.8.attn.W_O', 'blocks.8.attn.b_Q', 'blocks.8.attn.b_O', 'blocks.8.attn.W_K', 'blocks.8.attn.W_V', 'blocks.8.attn.b_K', 'blocks.8.attn.b_V', 'blocks.8.attn.mask', 'blocks.8.attn.IGNORE', 'blocks.8.attn.rotary_sin', 'blocks.8.attn.rotary_cos', 'blocks.8.attn.q_ln.w', 'blocks.8.attn.q_ln.b', 'blocks.8.attn.k_ln.w', 'blocks.8.attn.k_ln.b', 'blocks.8.mlp.l1.weight', 'blocks.8.mlp.l2.weight', 'blocks.9.ln1.w', 'blocks.9.ln1.b', 'blocks.9.ln2.w', 'blocks.9.ln2.b', 'blocks.9.attn.W_Q', 'blocks.9.attn.W_O', 'blocks.9.attn.b_Q', 'blocks.9.attn.b_O', 'blocks.9.attn.W_K', 'blocks.9.attn.W_V', 'blocks.9.attn.b_K', 'blocks.9.attn.b_V', 'blocks.9.attn.mask', 'blocks.9.attn.IGNORE', 'blocks.9.attn.rotary_sin', 'blocks.9.attn.rotary_cos', 'blocks.9.attn.q_ln.w', 'blocks.9.attn.q_ln.b', 'blocks.9.attn.k_ln.w', 'blocks.9.attn.k_ln.b', 'blocks.9.mlp.l1.weight', 'blocks.9.mlp.l2.weight', 'blocks.10.ln1.w', 'blocks.10.ln1.b', 'blocks.10.ln2.w', 'blocks.10.ln2.b', 'blocks.10.attn.W_Q', 'blocks.10.attn.W_O', 'blocks.10.attn.b_Q', 'blocks.10.attn.b_O', 'blocks.10.attn.W_K', 'blocks.10.attn.W_V', 'blocks.10.attn.b_K', 'blocks.10.attn.b_V', 'blocks.10.attn.mask', 'blocks.10.attn.IGNORE', 'blocks.10.attn.rotary_sin', 'blocks.10.attn.rotary_cos', 'blocks.10.attn.q_ln.w', 'blocks.10.attn.q_ln.b', 'blocks.10.attn.k_ln.w', 'blocks.10.attn.k_ln.b', 'blocks.10.mlp.l1.weight', 'blocks.10.mlp.l2.weight', 'blocks.11.ln1.w', 'blocks.11.ln1.b', 'blocks.11.ln2.w', 'blocks.11.ln2.b', 'blocks.11.attn.W_Q', 'blocks.11.attn.W_O', 'blocks.11.attn.b_Q', 'blocks.11.attn.b_O', 'blocks.11.attn.W_K', 'blocks.11.attn.W_V', 'blocks.11.attn.b_K', 'blocks.11.attn.b_V', 'blocks.11.attn.mask', 'blocks.11.attn.IGNORE', 'blocks.11.attn.rotary_sin', 'blocks.11.attn.rotary_cos', 'blocks.11.attn.q_ln.w', 'blocks.11.attn.q_ln.b', 'blocks.11.attn.k_ln.w', 'blocks.11.attn.k_ln.b', 'blocks.11.mlp.l1.weight', 'blocks.11.mlp.l2.weight', 'blocks.12.ln1.w', 'blocks.12.ln1.b', 'blocks.12.ln2.w', 'blocks.12.ln2.b', 'blocks.12.attn.W_Q', 'blocks.12.attn.W_O', 'blocks.12.attn.b_Q', 'blocks.12.attn.b_O', 'blocks.12.attn.W_K', 'blocks.12.attn.W_V', 'blocks.12.attn.b_K', 'blocks.12.attn.b_V', 'blocks.12.attn.mask', 'blocks.12.attn.IGNORE', 'blocks.12.attn.rotary_sin', 'blocks.12.attn.rotary_cos', 'blocks.12.attn.q_ln.w', 'blocks.12.attn.q_ln.b', 'blocks.12.attn.k_ln.w', 'blocks.12.attn.k_ln.b', 'blocks.12.mlp.l1.weight', 'blocks.12.mlp.l2.weight', 'blocks.13.ln1.w', 'blocks.13.ln1.b', 'blocks.13.ln2.w', 'blocks.13.ln2.b', 'blocks.13.attn.W_Q', 'blocks.13.attn.W_O', 'blocks.13.attn.b_Q', 'blocks.13.attn.b_O', 'blocks.13.attn.W_K', 'blocks.13.attn.W_V', 'blocks.13.attn.b_K', 'blocks.13.attn.b_V', 'blocks.13.attn.mask', 'blocks.13.attn.IGNORE', 'blocks.13.attn.rotary_sin', 'blocks.13.attn.rotary_cos', 'blocks.13.attn.q_ln.w', 'blocks.13.attn.q_ln.b', 'blocks.13.attn.k_ln.w', 'blocks.13.attn.k_ln.b', 'blocks.13.mlp.l1.weight', 'blocks.13.mlp.l2.weight', 'blocks.14.ln1.w', 'blocks.14.ln1.b', 'blocks.14.ln2.w', 'blocks.14.ln2.b', 'blocks.14.attn.W_Q', 'blocks.14.attn.W_O', 'blocks.14.attn.b_Q', 'blocks.14.attn.b_O', 'blocks.14.attn.W_K', 'blocks.14.attn.W_V', 'blocks.14.attn.b_K', 'blocks.14.attn.b_V', 'blocks.14.attn.mask', 'blocks.14.attn.IGNORE', 'blocks.14.attn.rotary_sin', 'blocks.14.attn.rotary_cos', 'blocks.14.attn.q_ln.w', 'blocks.14.attn.q_ln.b', 'blocks.14.attn.k_ln.w', 'blocks.14.attn.k_ln.b', 'blocks.14.mlp.l1.weight', 'blocks.14.mlp.l2.weight', 'blocks.15.ln1.w', 'blocks.15.ln1.b', 'blocks.15.ln2.w', 'blocks.15.ln2.b', 'blocks.15.attn.W_Q', 'blocks.15.attn.W_O', 'blocks.15.attn.b_Q', 'blocks.15.attn.b_O', 'blocks.15.attn.W_K', 'blocks.15.attn.W_V', 'blocks.15.attn.b_K', 'blocks.15.attn.b_V', 'blocks.15.attn.mask', 'blocks.15.attn.IGNORE', 'blocks.15.attn.rotary_sin', 'blocks.15.attn.rotary_cos', 'blocks.15.attn.q_ln.w', 'blocks.15.attn.q_ln.b', 'blocks.15.attn.k_ln.w', 'blocks.15.attn.k_ln.b', 'blocks.15.mlp.l1.weight', 'blocks.15.mlp.l2.weight', 'blocks.16.ln1.w', 'blocks.16.ln1.b', 'blocks.16.ln2.w', 'blocks.16.ln2.b', 'blocks.16.attn.W_Q', 'blocks.16.attn.W_O', 'blocks.16.attn.b_Q', 'blocks.16.attn.b_O', 'blocks.16.attn.W_K', 'blocks.16.attn.W_V', 'blocks.16.attn.b_K', 'blocks.16.attn.b_V', 'blocks.16.attn.mask', 'blocks.16.attn.IGNORE', 'blocks.16.attn.rotary_sin', 'blocks.16.attn.rotary_cos', 'blocks.16.attn.q_ln.w', 'blocks.16.attn.q_ln.b', 'blocks.16.attn.k_ln.w', 'blocks.16.attn.k_ln.b', 'blocks.16.mlp.l1.weight', 'blocks.16.mlp.l2.weight', 'blocks.17.ln1.w', 'blocks.17.ln1.b', 'blocks.17.ln2.w', 'blocks.17.ln2.b', 'blocks.17.attn.W_Q', 'blocks.17.attn.W_O', 'blocks.17.attn.b_Q', 'blocks.17.attn.b_O', 'blocks.17.attn.W_K', 'blocks.17.attn.W_V', 'blocks.17.attn.b_K', 'blocks.17.attn.b_V', 'blocks.17.attn.mask', 'blocks.17.attn.IGNORE', 'blocks.17.attn.rotary_sin', 'blocks.17.attn.rotary_cos', 'blocks.17.attn.q_ln.w', 'blocks.17.attn.q_ln.b', 'blocks.17.attn.k_ln.w', 'blocks.17.attn.k_ln.b', 'blocks.17.mlp.l1.weight', 'blocks.17.mlp.l2.weight', 'blocks.18.ln1.w', 'blocks.18.ln1.b', 'blocks.18.ln2.w', 'blocks.18.ln2.b', 'blocks.18.attn.W_Q', 'blocks.18.attn.W_O', 'blocks.18.attn.b_Q', 'blocks.18.attn.b_O', 'blocks.18.attn.W_K', 'blocks.18.attn.W_V', 'blocks.18.attn.b_K', 'blocks.18.attn.b_V', 'blocks.18.attn.mask', 'blocks.18.attn.IGNORE', 'blocks.18.attn.rotary_sin', 'blocks.18.attn.rotary_cos', 'blocks.18.attn.q_ln.w', 'blocks.18.attn.q_ln.b', 'blocks.18.attn.k_ln.w', 'blocks.18.attn.k_ln.b', 'blocks.18.mlp.l1.weight', 'blocks.18.mlp.l2.weight', 'blocks.19.ln1.w', 'blocks.19.ln1.b', 'blocks.19.ln2.w', 'blocks.19.ln2.b', 'blocks.19.attn.W_Q', 'blocks.19.attn.W_O', 'blocks.19.attn.b_Q', 'blocks.19.attn.b_O', 'blocks.19.attn.W_K', 'blocks.19.attn.W_V', 'blocks.19.attn.b_K', 'blocks.19.attn.b_V', 'blocks.19.attn.mask', 'blocks.19.attn.IGNORE', 'blocks.19.attn.rotary_sin', 'blocks.19.attn.rotary_cos', 'blocks.19.attn.q_ln.w', 'blocks.19.attn.q_ln.b', 'blocks.19.attn.k_ln.w', 'blocks.19.attn.k_ln.b', 'blocks.19.mlp.l1.weight', 'blocks.19.mlp.l2.weight', 'blocks.20.ln1.w', 'blocks.20.ln1.b', 'blocks.20.ln2.w', 'blocks.20.ln2.b', 'blocks.20.attn.W_Q', 'blocks.20.attn.W_O', 'blocks.20.attn.b_Q', 'blocks.20.attn.b_O', 'blocks.20.attn.W_K', 'blocks.20.attn.W_V', 'blocks.20.attn.b_K', 'blocks.20.attn.b_V', 'blocks.20.attn.mask', 'blocks.20.attn.IGNORE', 'blocks.20.attn.rotary_sin', 'blocks.20.attn.rotary_cos', 'blocks.20.attn.q_ln.w', 'blocks.20.attn.q_ln.b', 'blocks.20.attn.k_ln.w', 'blocks.20.attn.k_ln.b', 'blocks.20.mlp.l1.weight', 'blocks.20.mlp.l2.weight', 'blocks.21.ln1.w', 'blocks.21.ln1.b', 'blocks.21.ln2.w', 'blocks.21.ln2.b', 'blocks.21.attn.W_Q', 'blocks.21.attn.W_O', 'blocks.21.attn.b_Q', 'blocks.21.attn.b_O', 'blocks.21.attn.W_K', 'blocks.21.attn.W_V', 'blocks.21.attn.b_K', 'blocks.21.attn.b_V', 'blocks.21.attn.mask', 'blocks.21.attn.IGNORE', 'blocks.21.attn.rotary_sin', 'blocks.21.attn.rotary_cos', 'blocks.21.attn.q_ln.w', 'blocks.21.attn.q_ln.b', 'blocks.21.attn.k_ln.w', 'blocks.21.attn.k_ln.b', 'blocks.21.mlp.l1.weight', 'blocks.21.mlp.l2.weight', 'blocks.22.ln1.w', 'blocks.22.ln1.b', 'blocks.22.ln2.w', 'blocks.22.ln2.b', 'blocks.22.attn.W_Q', 'blocks.22.attn.W_O', 'blocks.22.attn.b_Q', 'blocks.22.attn.b_O', 'blocks.22.attn.W_K', 'blocks.22.attn.W_V', 'blocks.22.attn.b_K', 'blocks.22.attn.b_V', 'blocks.22.attn.mask', 'blocks.22.attn.IGNORE', 'blocks.22.attn.rotary_sin', 'blocks.22.attn.rotary_cos', 'blocks.22.attn.q_ln.w', 'blocks.22.attn.q_ln.b', 'blocks.22.attn.k_ln.w', 'blocks.22.attn.k_ln.b', 'blocks.22.mlp.l1.weight', 'blocks.22.mlp.l2.weight', 'blocks.23.ln1.w', 'blocks.23.ln1.b', 'blocks.23.ln2.w', 'blocks.23.ln2.b', 'blocks.23.attn.W_Q', 'blocks.23.attn.W_O', 'blocks.23.attn.b_Q', 'blocks.23.attn.b_O', 'blocks.23.attn.W_K', 'blocks.23.attn.W_V', 'blocks.23.attn.b_K', 'blocks.23.attn.b_V', 'blocks.23.attn.mask', 'blocks.23.attn.IGNORE', 'blocks.23.attn.rotary_sin', 'blocks.23.attn.rotary_cos', 'blocks.23.attn.q_ln.w', 'blocks.23.attn.q_ln.b', 'blocks.23.attn.k_ln.w', 'blocks.23.attn.k_ln.b', 'blocks.23.mlp.l1.weight', 'blocks.23.mlp.l2.weight', 'blocks.24.ln1.w', 'blocks.24.ln1.b', 'blocks.24.ln2.w', 'blocks.24.ln2.b', 'blocks.24.attn.W_Q', 'blocks.24.attn.W_O', 'blocks.24.attn.b_Q', 'blocks.24.attn.b_O', 'blocks.24.attn.W_K', 'blocks.24.attn.W_V', 'blocks.24.attn.b_K', 'blocks.24.attn.b_V', 'blocks.24.attn.mask', 'blocks.24.attn.IGNORE', 'blocks.24.attn.rotary_sin', 'blocks.24.attn.rotary_cos', 'blocks.24.attn.q_ln.w', 'blocks.24.attn.q_ln.b', 'blocks.24.attn.k_ln.w', 'blocks.24.attn.k_ln.b', 'blocks.24.mlp.l1.weight', 'blocks.24.mlp.l2.weight', 'blocks.25.ln1.w', 'blocks.25.ln1.b', 'blocks.25.ln2.w', 'blocks.25.ln2.b', 'blocks.25.attn.W_Q', 'blocks.25.attn.W_O', 'blocks.25.attn.b_Q', 'blocks.25.attn.b_O', 'blocks.25.attn.W_K', 'blocks.25.attn.W_V', 'blocks.25.attn.b_K', 'blocks.25.attn.b_V', 'blocks.25.attn.mask', 'blocks.25.attn.IGNORE', 'blocks.25.attn.rotary_sin', 'blocks.25.attn.rotary_cos', 'blocks.25.attn.q_ln.w', 'blocks.25.attn.q_ln.b', 'blocks.25.attn.k_ln.w', 'blocks.25.attn.k_ln.b', 'blocks.25.mlp.l1.weight', 'blocks.25.mlp.l2.weight', 'blocks.26.ln1.w', 'blocks.26.ln1.b', 'blocks.26.ln2.w', 'blocks.26.ln2.b', 'blocks.26.attn.W_Q', 'blocks.26.attn.W_O', 'blocks.26.attn.b_Q', 'blocks.26.attn.b_O', 'blocks.26.attn.W_K', 'blocks.26.attn.W_V', 'blocks.26.attn.b_K', 'blocks.26.attn.b_V', 'blocks.26.attn.mask', 'blocks.26.attn.IGNORE', 'blocks.26.attn.rotary_sin', 'blocks.26.attn.rotary_cos', 'blocks.26.attn.q_ln.w', 'blocks.26.attn.q_ln.b', 'blocks.26.attn.k_ln.w', 'blocks.26.attn.k_ln.b', 'blocks.26.mlp.l1.weight', 'blocks.26.mlp.l2.weight', 'blocks.27.ln1.w', 'blocks.27.ln1.b', 'blocks.27.ln2.w', 'blocks.27.ln2.b', 'blocks.27.attn.W_Q', 'blocks.27.attn.W_O', 'blocks.27.attn.b_Q', 'blocks.27.attn.b_O', 'blocks.27.attn.W_K', 'blocks.27.attn.W_V', 'blocks.27.attn.b_K', 'blocks.27.attn.b_V', 'blocks.27.attn.mask', 'blocks.27.attn.IGNORE', 'blocks.27.attn.rotary_sin', 'blocks.27.attn.rotary_cos', 'blocks.27.attn.q_ln.w', 'blocks.27.attn.q_ln.b', 'blocks.27.attn.k_ln.w', 'blocks.27.attn.k_ln.b', 'blocks.27.mlp.l1.weight', 'blocks.27.mlp.l2.weight', 'blocks.28.ln1.w', 'blocks.28.ln1.b', 'blocks.28.ln2.w', 'blocks.28.ln2.b', 'blocks.28.attn.W_Q', 'blocks.28.attn.W_O', 'blocks.28.attn.b_Q', 'blocks.28.attn.b_O', 'blocks.28.attn.W_K', 'blocks.28.attn.W_V', 'blocks.28.attn.b_K', 'blocks.28.attn.b_V', 'blocks.28.attn.mask', 'blocks.28.attn.IGNORE', 'blocks.28.attn.rotary_sin', 'blocks.28.attn.rotary_cos', 'blocks.28.attn.q_ln.w', 'blocks.28.attn.q_ln.b', 'blocks.28.attn.k_ln.w', 'blocks.28.attn.k_ln.b', 'blocks.28.mlp.l1.weight', 'blocks.28.mlp.l2.weight', 'blocks.29.ln1.w', 'blocks.29.ln1.b', 'blocks.29.ln2.w', 'blocks.29.ln2.b', 'blocks.29.attn.W_Q', 'blocks.29.attn.W_O', 'blocks.29.attn.b_Q', 'blocks.29.attn.b_O', 'blocks.29.attn.W_K', 'blocks.29.attn.W_V', 'blocks.29.attn.b_K', 'blocks.29.attn.b_V', 'blocks.29.attn.mask', 'blocks.29.attn.IGNORE', 'blocks.29.attn.rotary_sin', 'blocks.29.attn.rotary_cos', 'blocks.29.attn.q_ln.w', 'blocks.29.attn.q_ln.b', 'blocks.29.attn.k_ln.w', 'blocks.29.attn.k_ln.b', 'blocks.29.mlp.l1.weight', 'blocks.29.mlp.l2.weight', 'blocks.30.ln1.w', 'blocks.30.ln1.b', 'blocks.30.ln2.w', 'blocks.30.ln2.b', 'blocks.30.attn.W_Q', 'blocks.30.attn.W_O', 'blocks.30.attn.b_Q', 'blocks.30.attn.b_O', 'blocks.30.attn.W_K', 'blocks.30.attn.W_V', 'blocks.30.attn.b_K', 'blocks.30.attn.b_V', 'blocks.30.attn.mask', 'blocks.30.attn.IGNORE', 'blocks.30.attn.rotary_sin', 'blocks.30.attn.rotary_cos', 'blocks.30.attn.q_ln.w', 'blocks.30.attn.q_ln.b', 'blocks.30.attn.k_ln.w', 'blocks.30.attn.k_ln.b', 'blocks.30.mlp.l1.weight', 'blocks.30.mlp.l2.weight', 'blocks.31.ln1.w', 'blocks.31.ln1.b', 'blocks.31.ln2.w', 'blocks.31.ln2.b', 'blocks.31.attn.W_Q', 'blocks.31.attn.W_O', 'blocks.31.attn.b_Q', 'blocks.31.attn.b_O', 'blocks.31.attn.W_K', 'blocks.31.attn.W_V', 'blocks.31.attn.b_K', 'blocks.31.attn.b_V', 'blocks.31.attn.mask', 'blocks.31.attn.IGNORE', 'blocks.31.attn.rotary_sin', 'blocks.31.attn.rotary_cos', 'blocks.31.attn.q_ln.w', 'blocks.31.attn.q_ln.b', 'blocks.31.attn.k_ln.w', 'blocks.31.attn.k_ln.b', 'blocks.31.mlp.l1.weight', 'blocks.31.mlp.l2.weight', 'blocks.32.ln1.w', 'blocks.32.ln1.b', 'blocks.32.ln2.w', 'blocks.32.ln2.b', 'blocks.32.attn.W_Q', 'blocks.32.attn.W_O', 'blocks.32.attn.b_Q', 'blocks.32.attn.b_O', 'blocks.32.attn.W_K', 'blocks.32.attn.W_V', 'blocks.32.attn.b_K', 'blocks.32.attn.b_V', 'blocks.32.attn.mask', 'blocks.32.attn.IGNORE', 'blocks.32.attn.rotary_sin', 'blocks.32.attn.rotary_cos', 'blocks.32.attn.q_ln.w', 'blocks.32.attn.q_ln.b', 'blocks.32.attn.k_ln.w', 'blocks.32.attn.k_ln.b', 'blocks.32.mlp.l1.weight', 'blocks.32.mlp.l2.weight', 'blocks.33.ln1.w', 'blocks.33.ln1.b', 'blocks.33.ln2.w', 'blocks.33.ln2.b', 'blocks.33.attn.W_Q', 'blocks.33.attn.W_O', 'blocks.33.attn.b_Q', 'blocks.33.attn.b_O', 'blocks.33.attn.W_K', 'blocks.33.attn.W_V', 'blocks.33.attn.b_K', 'blocks.33.attn.b_V', 'blocks.33.attn.mask', 'blocks.33.attn.IGNORE', 'blocks.33.attn.rotary_sin', 'blocks.33.attn.rotary_cos', 'blocks.33.attn.q_ln.w', 'blocks.33.attn.q_ln.b', 'blocks.33.attn.k_ln.w', 'blocks.33.attn.k_ln.b', 'blocks.33.mlp.l1.weight', 'blocks.33.mlp.l2.weight', 'blocks.34.ln1.w', 'blocks.34.ln1.b', 'blocks.34.ln2.w', 'blocks.34.ln2.b', 'blocks.34.attn.W_Q', 'blocks.34.attn.W_O', 'blocks.34.attn.b_Q', 'blocks.34.attn.b_O', 'blocks.34.attn.W_K', 'blocks.34.attn.W_V', 'blocks.34.attn.b_K', 'blocks.34.attn.b_V', 'blocks.34.attn.mask', 'blocks.34.attn.IGNORE', 'blocks.34.attn.rotary_sin', 'blocks.34.attn.rotary_cos', 'blocks.34.attn.q_ln.w', 'blocks.34.attn.q_ln.b', 'blocks.34.attn.k_ln.w', 'blocks.34.attn.k_ln.b', 'blocks.34.mlp.l1.weight', 'blocks.34.mlp.l2.weight', 'blocks.35.ln1.w', 'blocks.35.ln1.b', 'blocks.35.ln2.w', 'blocks.35.ln2.b', 'blocks.35.attn.W_Q', 'blocks.35.attn.W_O', 'blocks.35.attn.b_Q', 'blocks.35.attn.b_O', 'blocks.35.attn.W_K', 'blocks.35.attn.W_V', 'blocks.35.attn.b_K', 'blocks.35.attn.b_V', 'blocks.35.attn.mask', 'blocks.35.attn.IGNORE', 'blocks.35.attn.rotary_sin', 'blocks.35.attn.rotary_cos', 'blocks.35.attn.q_ln.w', 'blocks.35.attn.q_ln.b', 'blocks.35.attn.k_ln.w', 'blocks.35.attn.k_ln.b', 'blocks.35.mlp.l1.weight', 'blocks.35.mlp.l2.weight', 'blocks.36.ln1.w', 'blocks.36.ln1.b', 'blocks.36.ln2.w', 'blocks.36.ln2.b', 'blocks.36.attn.W_Q', 'blocks.36.attn.W_O', 'blocks.36.attn.b_Q', 'blocks.36.attn.b_O', 'blocks.36.attn.W_K', 'blocks.36.attn.W_V', 'blocks.36.attn.b_K', 'blocks.36.attn.b_V', 'blocks.36.attn.mask', 'blocks.36.attn.IGNORE', 'blocks.36.attn.rotary_sin', 'blocks.36.attn.rotary_cos', 'blocks.36.attn.q_ln.w', 'blocks.36.attn.q_ln.b', 'blocks.36.attn.k_ln.w', 'blocks.36.attn.k_ln.b', 'blocks.36.mlp.l1.weight', 'blocks.36.mlp.l2.weight', 'blocks.37.ln1.w', 'blocks.37.ln1.b', 'blocks.37.ln2.w', 'blocks.37.ln2.b', 'blocks.37.attn.W_Q', 'blocks.37.attn.W_O', 'blocks.37.attn.b_Q', 'blocks.37.attn.b_O', 'blocks.37.attn.W_K', 'blocks.37.attn.W_V', 'blocks.37.attn.b_K', 'blocks.37.attn.b_V', 'blocks.37.attn.mask', 'blocks.37.attn.IGNORE', 'blocks.37.attn.rotary_sin', 'blocks.37.attn.rotary_cos', 'blocks.37.attn.q_ln.w', 'blocks.37.attn.q_ln.b', 'blocks.37.attn.k_ln.w', 'blocks.37.attn.k_ln.b', 'blocks.37.mlp.l1.weight', 'blocks.37.mlp.l2.weight', 'blocks.38.ln1.w', 'blocks.38.ln1.b', 'blocks.38.ln2.w', 'blocks.38.ln2.b', 'blocks.38.attn.W_Q', 'blocks.38.attn.W_O', 'blocks.38.attn.b_Q', 'blocks.38.attn.b_O', 'blocks.38.attn.W_K', 'blocks.38.attn.W_V', 'blocks.38.attn.b_K', 'blocks.38.attn.b_V', 'blocks.38.attn.mask', 'blocks.38.attn.IGNORE', 'blocks.38.attn.rotary_sin', 'blocks.38.attn.rotary_cos', 'blocks.38.attn.q_ln.w', 'blocks.38.attn.q_ln.b', 'blocks.38.attn.k_ln.w', 'blocks.38.attn.k_ln.b', 'blocks.38.mlp.l1.weight', 'blocks.38.mlp.l2.weight', 'blocks.39.ln1.w', 'blocks.39.ln1.b', 'blocks.39.ln2.w', 'blocks.39.ln2.b', 'blocks.39.attn.W_Q', 'blocks.39.attn.W_O', 'blocks.39.attn.b_Q', 'blocks.39.attn.b_O', 'blocks.39.attn.W_K', 'blocks.39.attn.W_V', 'blocks.39.attn.b_K', 'blocks.39.attn.b_V', 'blocks.39.attn.mask', 'blocks.39.attn.IGNORE', 'blocks.39.attn.rotary_sin', 'blocks.39.attn.rotary_cos', 'blocks.39.attn.q_ln.w', 'blocks.39.attn.q_ln.b', 'blocks.39.attn.k_ln.w', 'blocks.39.attn.k_ln.b', 'blocks.39.mlp.l1.weight', 'blocks.39.mlp.l2.weight', 'blocks.40.ln1.w', 'blocks.40.ln1.b', 'blocks.40.ln2.w', 'blocks.40.ln2.b', 'blocks.40.attn.W_Q', 'blocks.40.attn.W_O', 'blocks.40.attn.b_Q', 'blocks.40.attn.b_O', 'blocks.40.attn.W_K', 'blocks.40.attn.W_V', 'blocks.40.attn.b_K', 'blocks.40.attn.b_V', 'blocks.40.attn.mask', 'blocks.40.attn.IGNORE', 'blocks.40.attn.rotary_sin', 'blocks.40.attn.rotary_cos', 'blocks.40.attn.q_ln.w', 'blocks.40.attn.q_ln.b', 'blocks.40.attn.k_ln.w', 'blocks.40.attn.k_ln.b', 'blocks.40.mlp.l1.weight', 'blocks.40.mlp.l2.weight', 'blocks.41.ln1.w', 'blocks.41.ln1.b', 'blocks.41.ln2.w', 'blocks.41.ln2.b', 'blocks.41.attn.W_Q', 'blocks.41.attn.W_O', 'blocks.41.attn.b_Q', 'blocks.41.attn.b_O', 'blocks.41.attn.W_K', 'blocks.41.attn.W_V', 'blocks.41.attn.b_K', 'blocks.41.attn.b_V', 'blocks.41.attn.mask', 'blocks.41.attn.IGNORE', 'blocks.41.attn.rotary_sin', 'blocks.41.attn.rotary_cos', 'blocks.41.attn.q_ln.w', 'blocks.41.attn.q_ln.b', 'blocks.41.attn.k_ln.w', 'blocks.41.attn.k_ln.b', 'blocks.41.mlp.l1.weight', 'blocks.41.mlp.l2.weight', 'blocks.42.ln1.w', 'blocks.42.ln1.b', 'blocks.42.ln2.w', 'blocks.42.ln2.b', 'blocks.42.attn.W_Q', 'blocks.42.attn.W_O', 'blocks.42.attn.b_Q', 'blocks.42.attn.b_O', 'blocks.42.attn.W_K', 'blocks.42.attn.W_V', 'blocks.42.attn.b_K', 'blocks.42.attn.b_V', 'blocks.42.attn.mask', 'blocks.42.attn.IGNORE', 'blocks.42.attn.rotary_sin', 'blocks.42.attn.rotary_cos', 'blocks.42.attn.q_ln.w', 'blocks.42.attn.q_ln.b', 'blocks.42.attn.k_ln.w', 'blocks.42.attn.k_ln.b', 'blocks.42.mlp.l1.weight', 'blocks.42.mlp.l2.weight', 'blocks.43.ln1.w', 'blocks.43.ln1.b', 'blocks.43.ln2.w', 'blocks.43.ln2.b', 'blocks.43.attn.W_Q', 'blocks.43.attn.W_O', 'blocks.43.attn.b_Q', 'blocks.43.attn.b_O', 'blocks.43.attn.W_K', 'blocks.43.attn.W_V', 'blocks.43.attn.b_K', 'blocks.43.attn.b_V', 'blocks.43.attn.mask', 'blocks.43.attn.IGNORE', 'blocks.43.attn.rotary_sin', 'blocks.43.attn.rotary_cos', 'blocks.43.attn.q_ln.w', 'blocks.43.attn.q_ln.b', 'blocks.43.attn.k_ln.w', 'blocks.43.attn.k_ln.b', 'blocks.43.mlp.l1.weight', 'blocks.43.mlp.l2.weight', 'blocks.44.ln1.w', 'blocks.44.ln1.b', 'blocks.44.ln2.w', 'blocks.44.ln2.b', 'blocks.44.attn.W_Q', 'blocks.44.attn.W_O', 'blocks.44.attn.b_Q', 'blocks.44.attn.b_O', 'blocks.44.attn.W_K', 'blocks.44.attn.W_V', 'blocks.44.attn.b_K', 'blocks.44.attn.b_V', 'blocks.44.attn.mask', 'blocks.44.attn.IGNORE', 'blocks.44.attn.rotary_sin', 'blocks.44.attn.rotary_cos', 'blocks.44.attn.q_ln.w', 'blocks.44.attn.q_ln.b', 'blocks.44.attn.k_ln.w', 'blocks.44.attn.k_ln.b', 'blocks.44.mlp.l1.weight', 'blocks.44.mlp.l2.weight', 'blocks.45.ln1.w', 'blocks.45.ln1.b', 'blocks.45.ln2.w', 'blocks.45.ln2.b', 'blocks.45.attn.W_Q', 'blocks.45.attn.W_O', 'blocks.45.attn.b_Q', 'blocks.45.attn.b_O', 'blocks.45.attn.W_K', 'blocks.45.attn.W_V', 'blocks.45.attn.b_K', 'blocks.45.attn.b_V', 'blocks.45.attn.mask', 'blocks.45.attn.IGNORE', 'blocks.45.attn.rotary_sin', 'blocks.45.attn.rotary_cos', 'blocks.45.attn.q_ln.w', 'blocks.45.attn.q_ln.b', 'blocks.45.attn.k_ln.w', 'blocks.45.attn.k_ln.b', 'blocks.45.mlp.l1.weight', 'blocks.45.mlp.l2.weight', 'blocks.46.ln1.w', 'blocks.46.ln1.b', 'blocks.46.ln2.w', 'blocks.46.ln2.b', 'blocks.46.attn.W_Q', 'blocks.46.attn.W_O', 'blocks.46.attn.b_Q', 'blocks.46.attn.b_O', 'blocks.46.attn.W_K', 'blocks.46.attn.W_V', 'blocks.46.attn.b_K', 'blocks.46.attn.b_V', 'blocks.46.attn.mask', 'blocks.46.attn.IGNORE', 'blocks.46.attn.rotary_sin', 'blocks.46.attn.rotary_cos', 'blocks.46.attn.q_ln.w', 'blocks.46.attn.q_ln.b', 'blocks.46.attn.k_ln.w', 'blocks.46.attn.k_ln.b', 'blocks.46.mlp.l1.weight', 'blocks.46.mlp.l2.weight', 'blocks.47.ln1.w', 'blocks.47.ln1.b', 'blocks.47.ln2.w', 'blocks.47.ln2.b', 'blocks.47.attn.W_Q', 'blocks.47.attn.W_O', 'blocks.47.attn.b_Q', 'blocks.47.attn.b_O', 'blocks.47.attn.W_K', 'blocks.47.attn.W_V', 'blocks.47.attn.b_K', 'blocks.47.attn.b_V', 'blocks.47.attn.mask', 'blocks.47.attn.IGNORE', 'blocks.47.attn.rotary_sin', 'blocks.47.attn.rotary_cos', 'blocks.47.attn.q_ln.w', 'blocks.47.attn.q_ln.b', 'blocks.47.attn.k_ln.w', 'blocks.47.attn.k_ln.b', 'blocks.47.mlp.l1.weight', 'blocks.47.mlp.l2.weight', 'ln_final.w', 'ln_final.b', 'unembed.output_heads.sequence_head.0.weight', 'unembed.output_heads.sequence_head.0.bias', 'unembed.output_heads.sequence_head.2.weight', 'unembed.output_heads.sequence_head.2.bias', 'unembed.output_heads.sequence_head.3.weight', 'unembed.output_heads.sequence_head.3.bias', 'unembed.output_heads.structure_head.0.weight', 'unembed.output_heads.structure_head.0.bias', 'unembed.output_heads.structure_head.2.weight', 'unembed.output_heads.structure_head.2.bias', 'unembed.output_heads.structure_head.3.weight', 'unembed.output_heads.structure_head.3.bias', 'unembed.output_heads.ss8_head.0.weight', 'unembed.output_heads.ss8_head.0.bias', 'unembed.output_heads.ss8_head.2.weight', 'unembed.output_heads.ss8_head.2.bias', 'unembed.output_heads.ss8_head.3.weight', 'unembed.output_heads.ss8_head.3.bias', 'unembed.output_heads.sasa_head.0.weight', 'unembed.output_heads.sasa_head.0.bias', 'unembed.output_heads.sasa_head.2.weight', 'unembed.output_heads.sasa_head.2.bias', 'unembed.output_heads.sasa_head.3.weight', 'unembed.output_heads.sasa_head.3.bias', 'unembed.output_heads.function_head.0.weight', 'unembed.output_heads.function_head.0.bias', 'unembed.output_heads.function_head.2.weight', 'unembed.output_heads.function_head.2.bias', 'unembed.output_heads.function_head.3.weight', 'unembed.output_heads.function_head.3.bias', 'unembed.output_heads.residue_head.0.weight', 'unembed.output_heads.residue_head.0.bias', 'unembed.output_heads.residue_head.2.weight', 'unembed.output_heads.residue_head.2.bias', 'unembed.output_heads.residue_head.3.weight', 'unembed.output_heads.residue_head.3.bias'])\n"
     ]
    }
   ],
   "source": [
    "print(esm3_hooked.state_dict().keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "972f3952-5616-4555-a41f-cdef814823e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "813e5b30b37044ab8bdd0fafd3ed51ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 22 files:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = ESM3_sm_open_v0(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "14b40abc-a405-4cde-b20a-af14da214928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['encoder.sequence_embed.weight', 'encoder.plddt_projection.weight', 'encoder.plddt_projection.bias', 'encoder.structure_per_res_plddt_projection.weight', 'encoder.structure_per_res_plddt_projection.bias', 'encoder.structure_tokens_embed.weight', 'encoder.ss8_embed.weight', 'encoder.sasa_embed.weight', 'encoder.function_embed.0.weight', 'encoder.function_embed.1.weight', 'encoder.function_embed.2.weight', 'encoder.function_embed.3.weight', 'encoder.function_embed.4.weight', 'encoder.function_embed.5.weight', 'encoder.function_embed.6.weight', 'encoder.function_embed.7.weight', 'encoder.residue_embed.weight', 'transformer.blocks.0.attn.layernorm_qkv.0.weight', 'transformer.blocks.0.attn.layernorm_qkv.0.bias', 'transformer.blocks.0.attn.layernorm_qkv.1.weight', 'transformer.blocks.0.attn.out_proj.weight', 'transformer.blocks.0.attn.q_ln.weight', 'transformer.blocks.0.attn.k_ln.weight', 'transformer.blocks.0.geom_attn.distance_scale_per_head', 'transformer.blocks.0.geom_attn.rotation_scale_per_head', 'transformer.blocks.0.geom_attn.s_norm.weight', 'transformer.blocks.0.geom_attn.proj.weight', 'transformer.blocks.0.geom_attn.out_proj.weight', 'transformer.blocks.0.ffn.0.weight', 'transformer.blocks.0.ffn.0.bias', 'transformer.blocks.0.ffn.1.weight', 'transformer.blocks.0.ffn.3.weight', 'transformer.blocks.1.attn.layernorm_qkv.0.weight', 'transformer.blocks.1.attn.layernorm_qkv.0.bias', 'transformer.blocks.1.attn.layernorm_qkv.1.weight', 'transformer.blocks.1.attn.out_proj.weight', 'transformer.blocks.1.attn.q_ln.weight', 'transformer.blocks.1.attn.k_ln.weight', 'transformer.blocks.1.ffn.0.weight', 'transformer.blocks.1.ffn.0.bias', 'transformer.blocks.1.ffn.1.weight', 'transformer.blocks.1.ffn.3.weight', 'transformer.blocks.2.attn.layernorm_qkv.0.weight', 'transformer.blocks.2.attn.layernorm_qkv.0.bias', 'transformer.blocks.2.attn.layernorm_qkv.1.weight', 'transformer.blocks.2.attn.out_proj.weight', 'transformer.blocks.2.attn.q_ln.weight', 'transformer.blocks.2.attn.k_ln.weight', 'transformer.blocks.2.ffn.0.weight', 'transformer.blocks.2.ffn.0.bias', 'transformer.blocks.2.ffn.1.weight', 'transformer.blocks.2.ffn.3.weight', 'transformer.blocks.3.attn.layernorm_qkv.0.weight', 'transformer.blocks.3.attn.layernorm_qkv.0.bias', 'transformer.blocks.3.attn.layernorm_qkv.1.weight', 'transformer.blocks.3.attn.out_proj.weight', 'transformer.blocks.3.attn.q_ln.weight', 'transformer.blocks.3.attn.k_ln.weight', 'transformer.blocks.3.ffn.0.weight', 'transformer.blocks.3.ffn.0.bias', 'transformer.blocks.3.ffn.1.weight', 'transformer.blocks.3.ffn.3.weight', 'transformer.blocks.4.attn.layernorm_qkv.0.weight', 'transformer.blocks.4.attn.layernorm_qkv.0.bias', 'transformer.blocks.4.attn.layernorm_qkv.1.weight', 'transformer.blocks.4.attn.out_proj.weight', 'transformer.blocks.4.attn.q_ln.weight', 'transformer.blocks.4.attn.k_ln.weight', 'transformer.blocks.4.ffn.0.weight', 'transformer.blocks.4.ffn.0.bias', 'transformer.blocks.4.ffn.1.weight', 'transformer.blocks.4.ffn.3.weight', 'transformer.blocks.5.attn.layernorm_qkv.0.weight', 'transformer.blocks.5.attn.layernorm_qkv.0.bias', 'transformer.blocks.5.attn.layernorm_qkv.1.weight', 'transformer.blocks.5.attn.out_proj.weight', 'transformer.blocks.5.attn.q_ln.weight', 'transformer.blocks.5.attn.k_ln.weight', 'transformer.blocks.5.ffn.0.weight', 'transformer.blocks.5.ffn.0.bias', 'transformer.blocks.5.ffn.1.weight', 'transformer.blocks.5.ffn.3.weight', 'transformer.blocks.6.attn.layernorm_qkv.0.weight', 'transformer.blocks.6.attn.layernorm_qkv.0.bias', 'transformer.blocks.6.attn.layernorm_qkv.1.weight', 'transformer.blocks.6.attn.out_proj.weight', 'transformer.blocks.6.attn.q_ln.weight', 'transformer.blocks.6.attn.k_ln.weight', 'transformer.blocks.6.ffn.0.weight', 'transformer.blocks.6.ffn.0.bias', 'transformer.blocks.6.ffn.1.weight', 'transformer.blocks.6.ffn.3.weight', 'transformer.blocks.7.attn.layernorm_qkv.0.weight', 'transformer.blocks.7.attn.layernorm_qkv.0.bias', 'transformer.blocks.7.attn.layernorm_qkv.1.weight', 'transformer.blocks.7.attn.out_proj.weight', 'transformer.blocks.7.attn.q_ln.weight', 'transformer.blocks.7.attn.k_ln.weight', 'transformer.blocks.7.ffn.0.weight', 'transformer.blocks.7.ffn.0.bias', 'transformer.blocks.7.ffn.1.weight', 'transformer.blocks.7.ffn.3.weight', 'transformer.blocks.8.attn.layernorm_qkv.0.weight', 'transformer.blocks.8.attn.layernorm_qkv.0.bias', 'transformer.blocks.8.attn.layernorm_qkv.1.weight', 'transformer.blocks.8.attn.out_proj.weight', 'transformer.blocks.8.attn.q_ln.weight', 'transformer.blocks.8.attn.k_ln.weight', 'transformer.blocks.8.ffn.0.weight', 'transformer.blocks.8.ffn.0.bias', 'transformer.blocks.8.ffn.1.weight', 'transformer.blocks.8.ffn.3.weight', 'transformer.blocks.9.attn.layernorm_qkv.0.weight', 'transformer.blocks.9.attn.layernorm_qkv.0.bias', 'transformer.blocks.9.attn.layernorm_qkv.1.weight', 'transformer.blocks.9.attn.out_proj.weight', 'transformer.blocks.9.attn.q_ln.weight', 'transformer.blocks.9.attn.k_ln.weight', 'transformer.blocks.9.ffn.0.weight', 'transformer.blocks.9.ffn.0.bias', 'transformer.blocks.9.ffn.1.weight', 'transformer.blocks.9.ffn.3.weight', 'transformer.blocks.10.attn.layernorm_qkv.0.weight', 'transformer.blocks.10.attn.layernorm_qkv.0.bias', 'transformer.blocks.10.attn.layernorm_qkv.1.weight', 'transformer.blocks.10.attn.out_proj.weight', 'transformer.blocks.10.attn.q_ln.weight', 'transformer.blocks.10.attn.k_ln.weight', 'transformer.blocks.10.ffn.0.weight', 'transformer.blocks.10.ffn.0.bias', 'transformer.blocks.10.ffn.1.weight', 'transformer.blocks.10.ffn.3.weight', 'transformer.blocks.11.attn.layernorm_qkv.0.weight', 'transformer.blocks.11.attn.layernorm_qkv.0.bias', 'transformer.blocks.11.attn.layernorm_qkv.1.weight', 'transformer.blocks.11.attn.out_proj.weight', 'transformer.blocks.11.attn.q_ln.weight', 'transformer.blocks.11.attn.k_ln.weight', 'transformer.blocks.11.ffn.0.weight', 'transformer.blocks.11.ffn.0.bias', 'transformer.blocks.11.ffn.1.weight', 'transformer.blocks.11.ffn.3.weight', 'transformer.blocks.12.attn.layernorm_qkv.0.weight', 'transformer.blocks.12.attn.layernorm_qkv.0.bias', 'transformer.blocks.12.attn.layernorm_qkv.1.weight', 'transformer.blocks.12.attn.out_proj.weight', 'transformer.blocks.12.attn.q_ln.weight', 'transformer.blocks.12.attn.k_ln.weight', 'transformer.blocks.12.ffn.0.weight', 'transformer.blocks.12.ffn.0.bias', 'transformer.blocks.12.ffn.1.weight', 'transformer.blocks.12.ffn.3.weight', 'transformer.blocks.13.attn.layernorm_qkv.0.weight', 'transformer.blocks.13.attn.layernorm_qkv.0.bias', 'transformer.blocks.13.attn.layernorm_qkv.1.weight', 'transformer.blocks.13.attn.out_proj.weight', 'transformer.blocks.13.attn.q_ln.weight', 'transformer.blocks.13.attn.k_ln.weight', 'transformer.blocks.13.ffn.0.weight', 'transformer.blocks.13.ffn.0.bias', 'transformer.blocks.13.ffn.1.weight', 'transformer.blocks.13.ffn.3.weight', 'transformer.blocks.14.attn.layernorm_qkv.0.weight', 'transformer.blocks.14.attn.layernorm_qkv.0.bias', 'transformer.blocks.14.attn.layernorm_qkv.1.weight', 'transformer.blocks.14.attn.out_proj.weight', 'transformer.blocks.14.attn.q_ln.weight', 'transformer.blocks.14.attn.k_ln.weight', 'transformer.blocks.14.ffn.0.weight', 'transformer.blocks.14.ffn.0.bias', 'transformer.blocks.14.ffn.1.weight', 'transformer.blocks.14.ffn.3.weight', 'transformer.blocks.15.attn.layernorm_qkv.0.weight', 'transformer.blocks.15.attn.layernorm_qkv.0.bias', 'transformer.blocks.15.attn.layernorm_qkv.1.weight', 'transformer.blocks.15.attn.out_proj.weight', 'transformer.blocks.15.attn.q_ln.weight', 'transformer.blocks.15.attn.k_ln.weight', 'transformer.blocks.15.ffn.0.weight', 'transformer.blocks.15.ffn.0.bias', 'transformer.blocks.15.ffn.1.weight', 'transformer.blocks.15.ffn.3.weight', 'transformer.blocks.16.attn.layernorm_qkv.0.weight', 'transformer.blocks.16.attn.layernorm_qkv.0.bias', 'transformer.blocks.16.attn.layernorm_qkv.1.weight', 'transformer.blocks.16.attn.out_proj.weight', 'transformer.blocks.16.attn.q_ln.weight', 'transformer.blocks.16.attn.k_ln.weight', 'transformer.blocks.16.ffn.0.weight', 'transformer.blocks.16.ffn.0.bias', 'transformer.blocks.16.ffn.1.weight', 'transformer.blocks.16.ffn.3.weight', 'transformer.blocks.17.attn.layernorm_qkv.0.weight', 'transformer.blocks.17.attn.layernorm_qkv.0.bias', 'transformer.blocks.17.attn.layernorm_qkv.1.weight', 'transformer.blocks.17.attn.out_proj.weight', 'transformer.blocks.17.attn.q_ln.weight', 'transformer.blocks.17.attn.k_ln.weight', 'transformer.blocks.17.ffn.0.weight', 'transformer.blocks.17.ffn.0.bias', 'transformer.blocks.17.ffn.1.weight', 'transformer.blocks.17.ffn.3.weight', 'transformer.blocks.18.attn.layernorm_qkv.0.weight', 'transformer.blocks.18.attn.layernorm_qkv.0.bias', 'transformer.blocks.18.attn.layernorm_qkv.1.weight', 'transformer.blocks.18.attn.out_proj.weight', 'transformer.blocks.18.attn.q_ln.weight', 'transformer.blocks.18.attn.k_ln.weight', 'transformer.blocks.18.ffn.0.weight', 'transformer.blocks.18.ffn.0.bias', 'transformer.blocks.18.ffn.1.weight', 'transformer.blocks.18.ffn.3.weight', 'transformer.blocks.19.attn.layernorm_qkv.0.weight', 'transformer.blocks.19.attn.layernorm_qkv.0.bias', 'transformer.blocks.19.attn.layernorm_qkv.1.weight', 'transformer.blocks.19.attn.out_proj.weight', 'transformer.blocks.19.attn.q_ln.weight', 'transformer.blocks.19.attn.k_ln.weight', 'transformer.blocks.19.ffn.0.weight', 'transformer.blocks.19.ffn.0.bias', 'transformer.blocks.19.ffn.1.weight', 'transformer.blocks.19.ffn.3.weight', 'transformer.blocks.20.attn.layernorm_qkv.0.weight', 'transformer.blocks.20.attn.layernorm_qkv.0.bias', 'transformer.blocks.20.attn.layernorm_qkv.1.weight', 'transformer.blocks.20.attn.out_proj.weight', 'transformer.blocks.20.attn.q_ln.weight', 'transformer.blocks.20.attn.k_ln.weight', 'transformer.blocks.20.ffn.0.weight', 'transformer.blocks.20.ffn.0.bias', 'transformer.blocks.20.ffn.1.weight', 'transformer.blocks.20.ffn.3.weight', 'transformer.blocks.21.attn.layernorm_qkv.0.weight', 'transformer.blocks.21.attn.layernorm_qkv.0.bias', 'transformer.blocks.21.attn.layernorm_qkv.1.weight', 'transformer.blocks.21.attn.out_proj.weight', 'transformer.blocks.21.attn.q_ln.weight', 'transformer.blocks.21.attn.k_ln.weight', 'transformer.blocks.21.ffn.0.weight', 'transformer.blocks.21.ffn.0.bias', 'transformer.blocks.21.ffn.1.weight', 'transformer.blocks.21.ffn.3.weight', 'transformer.blocks.22.attn.layernorm_qkv.0.weight', 'transformer.blocks.22.attn.layernorm_qkv.0.bias', 'transformer.blocks.22.attn.layernorm_qkv.1.weight', 'transformer.blocks.22.attn.out_proj.weight', 'transformer.blocks.22.attn.q_ln.weight', 'transformer.blocks.22.attn.k_ln.weight', 'transformer.blocks.22.ffn.0.weight', 'transformer.blocks.22.ffn.0.bias', 'transformer.blocks.22.ffn.1.weight', 'transformer.blocks.22.ffn.3.weight', 'transformer.blocks.23.attn.layernorm_qkv.0.weight', 'transformer.blocks.23.attn.layernorm_qkv.0.bias', 'transformer.blocks.23.attn.layernorm_qkv.1.weight', 'transformer.blocks.23.attn.out_proj.weight', 'transformer.blocks.23.attn.q_ln.weight', 'transformer.blocks.23.attn.k_ln.weight', 'transformer.blocks.23.ffn.0.weight', 'transformer.blocks.23.ffn.0.bias', 'transformer.blocks.23.ffn.1.weight', 'transformer.blocks.23.ffn.3.weight', 'transformer.blocks.24.attn.layernorm_qkv.0.weight', 'transformer.blocks.24.attn.layernorm_qkv.0.bias', 'transformer.blocks.24.attn.layernorm_qkv.1.weight', 'transformer.blocks.24.attn.out_proj.weight', 'transformer.blocks.24.attn.q_ln.weight', 'transformer.blocks.24.attn.k_ln.weight', 'transformer.blocks.24.ffn.0.weight', 'transformer.blocks.24.ffn.0.bias', 'transformer.blocks.24.ffn.1.weight', 'transformer.blocks.24.ffn.3.weight', 'transformer.blocks.25.attn.layernorm_qkv.0.weight', 'transformer.blocks.25.attn.layernorm_qkv.0.bias', 'transformer.blocks.25.attn.layernorm_qkv.1.weight', 'transformer.blocks.25.attn.out_proj.weight', 'transformer.blocks.25.attn.q_ln.weight', 'transformer.blocks.25.attn.k_ln.weight', 'transformer.blocks.25.ffn.0.weight', 'transformer.blocks.25.ffn.0.bias', 'transformer.blocks.25.ffn.1.weight', 'transformer.blocks.25.ffn.3.weight', 'transformer.blocks.26.attn.layernorm_qkv.0.weight', 'transformer.blocks.26.attn.layernorm_qkv.0.bias', 'transformer.blocks.26.attn.layernorm_qkv.1.weight', 'transformer.blocks.26.attn.out_proj.weight', 'transformer.blocks.26.attn.q_ln.weight', 'transformer.blocks.26.attn.k_ln.weight', 'transformer.blocks.26.ffn.0.weight', 'transformer.blocks.26.ffn.0.bias', 'transformer.blocks.26.ffn.1.weight', 'transformer.blocks.26.ffn.3.weight', 'transformer.blocks.27.attn.layernorm_qkv.0.weight', 'transformer.blocks.27.attn.layernorm_qkv.0.bias', 'transformer.blocks.27.attn.layernorm_qkv.1.weight', 'transformer.blocks.27.attn.out_proj.weight', 'transformer.blocks.27.attn.q_ln.weight', 'transformer.blocks.27.attn.k_ln.weight', 'transformer.blocks.27.ffn.0.weight', 'transformer.blocks.27.ffn.0.bias', 'transformer.blocks.27.ffn.1.weight', 'transformer.blocks.27.ffn.3.weight', 'transformer.blocks.28.attn.layernorm_qkv.0.weight', 'transformer.blocks.28.attn.layernorm_qkv.0.bias', 'transformer.blocks.28.attn.layernorm_qkv.1.weight', 'transformer.blocks.28.attn.out_proj.weight', 'transformer.blocks.28.attn.q_ln.weight', 'transformer.blocks.28.attn.k_ln.weight', 'transformer.blocks.28.ffn.0.weight', 'transformer.blocks.28.ffn.0.bias', 'transformer.blocks.28.ffn.1.weight', 'transformer.blocks.28.ffn.3.weight', 'transformer.blocks.29.attn.layernorm_qkv.0.weight', 'transformer.blocks.29.attn.layernorm_qkv.0.bias', 'transformer.blocks.29.attn.layernorm_qkv.1.weight', 'transformer.blocks.29.attn.out_proj.weight', 'transformer.blocks.29.attn.q_ln.weight', 'transformer.blocks.29.attn.k_ln.weight', 'transformer.blocks.29.ffn.0.weight', 'transformer.blocks.29.ffn.0.bias', 'transformer.blocks.29.ffn.1.weight', 'transformer.blocks.29.ffn.3.weight', 'transformer.blocks.30.attn.layernorm_qkv.0.weight', 'transformer.blocks.30.attn.layernorm_qkv.0.bias', 'transformer.blocks.30.attn.layernorm_qkv.1.weight', 'transformer.blocks.30.attn.out_proj.weight', 'transformer.blocks.30.attn.q_ln.weight', 'transformer.blocks.30.attn.k_ln.weight', 'transformer.blocks.30.ffn.0.weight', 'transformer.blocks.30.ffn.0.bias', 'transformer.blocks.30.ffn.1.weight', 'transformer.blocks.30.ffn.3.weight', 'transformer.blocks.31.attn.layernorm_qkv.0.weight', 'transformer.blocks.31.attn.layernorm_qkv.0.bias', 'transformer.blocks.31.attn.layernorm_qkv.1.weight', 'transformer.blocks.31.attn.out_proj.weight', 'transformer.blocks.31.attn.q_ln.weight', 'transformer.blocks.31.attn.k_ln.weight', 'transformer.blocks.31.ffn.0.weight', 'transformer.blocks.31.ffn.0.bias', 'transformer.blocks.31.ffn.1.weight', 'transformer.blocks.31.ffn.3.weight', 'transformer.blocks.32.attn.layernorm_qkv.0.weight', 'transformer.blocks.32.attn.layernorm_qkv.0.bias', 'transformer.blocks.32.attn.layernorm_qkv.1.weight', 'transformer.blocks.32.attn.out_proj.weight', 'transformer.blocks.32.attn.q_ln.weight', 'transformer.blocks.32.attn.k_ln.weight', 'transformer.blocks.32.ffn.0.weight', 'transformer.blocks.32.ffn.0.bias', 'transformer.blocks.32.ffn.1.weight', 'transformer.blocks.32.ffn.3.weight', 'transformer.blocks.33.attn.layernorm_qkv.0.weight', 'transformer.blocks.33.attn.layernorm_qkv.0.bias', 'transformer.blocks.33.attn.layernorm_qkv.1.weight', 'transformer.blocks.33.attn.out_proj.weight', 'transformer.blocks.33.attn.q_ln.weight', 'transformer.blocks.33.attn.k_ln.weight', 'transformer.blocks.33.ffn.0.weight', 'transformer.blocks.33.ffn.0.bias', 'transformer.blocks.33.ffn.1.weight', 'transformer.blocks.33.ffn.3.weight', 'transformer.blocks.34.attn.layernorm_qkv.0.weight', 'transformer.blocks.34.attn.layernorm_qkv.0.bias', 'transformer.blocks.34.attn.layernorm_qkv.1.weight', 'transformer.blocks.34.attn.out_proj.weight', 'transformer.blocks.34.attn.q_ln.weight', 'transformer.blocks.34.attn.k_ln.weight', 'transformer.blocks.34.ffn.0.weight', 'transformer.blocks.34.ffn.0.bias', 'transformer.blocks.34.ffn.1.weight', 'transformer.blocks.34.ffn.3.weight', 'transformer.blocks.35.attn.layernorm_qkv.0.weight', 'transformer.blocks.35.attn.layernorm_qkv.0.bias', 'transformer.blocks.35.attn.layernorm_qkv.1.weight', 'transformer.blocks.35.attn.out_proj.weight', 'transformer.blocks.35.attn.q_ln.weight', 'transformer.blocks.35.attn.k_ln.weight', 'transformer.blocks.35.ffn.0.weight', 'transformer.blocks.35.ffn.0.bias', 'transformer.blocks.35.ffn.1.weight', 'transformer.blocks.35.ffn.3.weight', 'transformer.blocks.36.attn.layernorm_qkv.0.weight', 'transformer.blocks.36.attn.layernorm_qkv.0.bias', 'transformer.blocks.36.attn.layernorm_qkv.1.weight', 'transformer.blocks.36.attn.out_proj.weight', 'transformer.blocks.36.attn.q_ln.weight', 'transformer.blocks.36.attn.k_ln.weight', 'transformer.blocks.36.ffn.0.weight', 'transformer.blocks.36.ffn.0.bias', 'transformer.blocks.36.ffn.1.weight', 'transformer.blocks.36.ffn.3.weight', 'transformer.blocks.37.attn.layernorm_qkv.0.weight', 'transformer.blocks.37.attn.layernorm_qkv.0.bias', 'transformer.blocks.37.attn.layernorm_qkv.1.weight', 'transformer.blocks.37.attn.out_proj.weight', 'transformer.blocks.37.attn.q_ln.weight', 'transformer.blocks.37.attn.k_ln.weight', 'transformer.blocks.37.ffn.0.weight', 'transformer.blocks.37.ffn.0.bias', 'transformer.blocks.37.ffn.1.weight', 'transformer.blocks.37.ffn.3.weight', 'transformer.blocks.38.attn.layernorm_qkv.0.weight', 'transformer.blocks.38.attn.layernorm_qkv.0.bias', 'transformer.blocks.38.attn.layernorm_qkv.1.weight', 'transformer.blocks.38.attn.out_proj.weight', 'transformer.blocks.38.attn.q_ln.weight', 'transformer.blocks.38.attn.k_ln.weight', 'transformer.blocks.38.ffn.0.weight', 'transformer.blocks.38.ffn.0.bias', 'transformer.blocks.38.ffn.1.weight', 'transformer.blocks.38.ffn.3.weight', 'transformer.blocks.39.attn.layernorm_qkv.0.weight', 'transformer.blocks.39.attn.layernorm_qkv.0.bias', 'transformer.blocks.39.attn.layernorm_qkv.1.weight', 'transformer.blocks.39.attn.out_proj.weight', 'transformer.blocks.39.attn.q_ln.weight', 'transformer.blocks.39.attn.k_ln.weight', 'transformer.blocks.39.ffn.0.weight', 'transformer.blocks.39.ffn.0.bias', 'transformer.blocks.39.ffn.1.weight', 'transformer.blocks.39.ffn.3.weight', 'transformer.blocks.40.attn.layernorm_qkv.0.weight', 'transformer.blocks.40.attn.layernorm_qkv.0.bias', 'transformer.blocks.40.attn.layernorm_qkv.1.weight', 'transformer.blocks.40.attn.out_proj.weight', 'transformer.blocks.40.attn.q_ln.weight', 'transformer.blocks.40.attn.k_ln.weight', 'transformer.blocks.40.ffn.0.weight', 'transformer.blocks.40.ffn.0.bias', 'transformer.blocks.40.ffn.1.weight', 'transformer.blocks.40.ffn.3.weight', 'transformer.blocks.41.attn.layernorm_qkv.0.weight', 'transformer.blocks.41.attn.layernorm_qkv.0.bias', 'transformer.blocks.41.attn.layernorm_qkv.1.weight', 'transformer.blocks.41.attn.out_proj.weight', 'transformer.blocks.41.attn.q_ln.weight', 'transformer.blocks.41.attn.k_ln.weight', 'transformer.blocks.41.ffn.0.weight', 'transformer.blocks.41.ffn.0.bias', 'transformer.blocks.41.ffn.1.weight', 'transformer.blocks.41.ffn.3.weight', 'transformer.blocks.42.attn.layernorm_qkv.0.weight', 'transformer.blocks.42.attn.layernorm_qkv.0.bias', 'transformer.blocks.42.attn.layernorm_qkv.1.weight', 'transformer.blocks.42.attn.out_proj.weight', 'transformer.blocks.42.attn.q_ln.weight', 'transformer.blocks.42.attn.k_ln.weight', 'transformer.blocks.42.ffn.0.weight', 'transformer.blocks.42.ffn.0.bias', 'transformer.blocks.42.ffn.1.weight', 'transformer.blocks.42.ffn.3.weight', 'transformer.blocks.43.attn.layernorm_qkv.0.weight', 'transformer.blocks.43.attn.layernorm_qkv.0.bias', 'transformer.blocks.43.attn.layernorm_qkv.1.weight', 'transformer.blocks.43.attn.out_proj.weight', 'transformer.blocks.43.attn.q_ln.weight', 'transformer.blocks.43.attn.k_ln.weight', 'transformer.blocks.43.ffn.0.weight', 'transformer.blocks.43.ffn.0.bias', 'transformer.blocks.43.ffn.1.weight', 'transformer.blocks.43.ffn.3.weight', 'transformer.blocks.44.attn.layernorm_qkv.0.weight', 'transformer.blocks.44.attn.layernorm_qkv.0.bias', 'transformer.blocks.44.attn.layernorm_qkv.1.weight', 'transformer.blocks.44.attn.out_proj.weight', 'transformer.blocks.44.attn.q_ln.weight', 'transformer.blocks.44.attn.k_ln.weight', 'transformer.blocks.44.ffn.0.weight', 'transformer.blocks.44.ffn.0.bias', 'transformer.blocks.44.ffn.1.weight', 'transformer.blocks.44.ffn.3.weight', 'transformer.blocks.45.attn.layernorm_qkv.0.weight', 'transformer.blocks.45.attn.layernorm_qkv.0.bias', 'transformer.blocks.45.attn.layernorm_qkv.1.weight', 'transformer.blocks.45.attn.out_proj.weight', 'transformer.blocks.45.attn.q_ln.weight', 'transformer.blocks.45.attn.k_ln.weight', 'transformer.blocks.45.ffn.0.weight', 'transformer.blocks.45.ffn.0.bias', 'transformer.blocks.45.ffn.1.weight', 'transformer.blocks.45.ffn.3.weight', 'transformer.blocks.46.attn.layernorm_qkv.0.weight', 'transformer.blocks.46.attn.layernorm_qkv.0.bias', 'transformer.blocks.46.attn.layernorm_qkv.1.weight', 'transformer.blocks.46.attn.out_proj.weight', 'transformer.blocks.46.attn.q_ln.weight', 'transformer.blocks.46.attn.k_ln.weight', 'transformer.blocks.46.ffn.0.weight', 'transformer.blocks.46.ffn.0.bias', 'transformer.blocks.46.ffn.1.weight', 'transformer.blocks.46.ffn.3.weight', 'transformer.blocks.47.attn.layernorm_qkv.0.weight', 'transformer.blocks.47.attn.layernorm_qkv.0.bias', 'transformer.blocks.47.attn.layernorm_qkv.1.weight', 'transformer.blocks.47.attn.out_proj.weight', 'transformer.blocks.47.attn.q_ln.weight', 'transformer.blocks.47.attn.k_ln.weight', 'transformer.blocks.47.ffn.0.weight', 'transformer.blocks.47.ffn.0.bias', 'transformer.blocks.47.ffn.1.weight', 'transformer.blocks.47.ffn.3.weight', 'transformer.norm.weight', 'output_heads.sequence_head.0.weight', 'output_heads.sequence_head.0.bias', 'output_heads.sequence_head.2.weight', 'output_heads.sequence_head.2.bias', 'output_heads.sequence_head.3.weight', 'output_heads.sequence_head.3.bias', 'output_heads.structure_head.0.weight', 'output_heads.structure_head.0.bias', 'output_heads.structure_head.2.weight', 'output_heads.structure_head.2.bias', 'output_heads.structure_head.3.weight', 'output_heads.structure_head.3.bias', 'output_heads.ss8_head.0.weight', 'output_heads.ss8_head.0.bias', 'output_heads.ss8_head.2.weight', 'output_heads.ss8_head.2.bias', 'output_heads.ss8_head.3.weight', 'output_heads.ss8_head.3.bias', 'output_heads.sasa_head.0.weight', 'output_heads.sasa_head.0.bias', 'output_heads.sasa_head.2.weight', 'output_heads.sasa_head.2.bias', 'output_heads.sasa_head.3.weight', 'output_heads.sasa_head.3.bias', 'output_heads.function_head.0.weight', 'output_heads.function_head.0.bias', 'output_heads.function_head.2.weight', 'output_heads.function_head.2.bias', 'output_heads.function_head.3.weight', 'output_heads.function_head.3.bias', 'output_heads.residue_head.0.weight', 'output_heads.residue_head.0.bias', 'output_heads.residue_head.2.weight', 'output_heads.residue_head.2.bias', 'output_heads.residue_head.3.weight', 'output_heads.residue_head.3.bias'])\n"
     ]
    }
   ],
   "source": [
    "print(model.state_dict().keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df113c7-6063-44dc-b93f-b03128c9d5f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
