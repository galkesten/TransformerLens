{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8aa05580-5cab-4b40-90bf-3234c3c34b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Attention:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "176ac421-cfdf-47b5-a88f-39b22dcc337e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "import torch\n",
    "from jaxtyping import Float\n",
    "from torch.testing import assert_close\n",
    "import torch.nn as nn\n",
    "from transformer_lens.components import Attention\n",
    "from transformer_lens.components import LayerNorm\n",
    "from transformer_lens.components import ESM3_Hooked_MLP, swiglu_correction_fn\n",
    "from transformer_lens.components import HookedEsm3UnifiedTransformerBlock\n",
    "from esm.layers.attention import MultiHeadAttention\n",
    "from esm.layers.blocks import swiglu_ln_ffn, UnifiedTransformerBlock\n",
    "from transformer_lens.HookedTransformerConfig import HookedTransformerConfig\n",
    "import functools\n",
    "import einops\n",
    "from esm.utils.constants.esm3 import data_root\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "384605f0-1c01-4a45-8388-d66210b619dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_multi_head_attention_params(d_model, n_heads, bias=False, qk_layernorm=False):\n",
    "    params = {\n",
    "        \"layernorm_qkv_weight\": torch.rand(d_model),  # Weight of LayerNorm\n",
    "        \"layernorm_qkv_bias\": torch.rand(d_model),    # Bias of LayerNorm\n",
    "        \"W_qkv_weight\": torch.rand(d_model * 3, d_model),  # Weight of Linear layer\n",
    "        \"W_qkv_bias\": torch.rand(d_model * 3) if bias else None,  # Bias of Linear layer\n",
    "        \"out_proj_weight\": torch.rand(d_model, d_model),  # Output projection weight\n",
    "        \"out_proj_bias\": torch.rand(d_model) if bias else None,  # Output projection bias\n",
    "    }\n",
    "    \n",
    "    if qk_layernorm:\n",
    "        params.update({\n",
    "            \"q_ln_weight\": torch.rand(d_model),\n",
    "            \"q_ln_bias\": torch.rand(d_model) if bias else None,\n",
    "            \"k_ln_weight\": torch.rand(d_model),\n",
    "            \"k_ln_bias\": torch.rand(d_model) if bias else None,\n",
    "        })\n",
    "    return params\n",
    "    \n",
    "\n",
    "def assign_params_to_esm_attention_layer(layer, params, bias=True):\n",
    "    with torch.no_grad():\n",
    "        # Assign LayerNorm for QKV\n",
    "        layer.layernorm_qkv[0].weight.copy_(params[\"layernorm_qkv_weight\"])\n",
    "        layer.layernorm_qkv[0].bias.copy_(params[\"layernorm_qkv_bias\"])\n",
    "        \n",
    "        # Assign Weights and Bias for QKV Projection\n",
    "        layer.layernorm_qkv[1].weight.copy_(params[\"W_qkv_weight\"])\n",
    "        if bias:\n",
    "            layer.layernorm_qkv[1].bias.copy_(params[\"W_qkv_bias\"])\n",
    "        \n",
    "        # Assign Output Projection\n",
    "        layer.out_proj.weight.copy_(params[\"out_proj_weight\"])\n",
    "        if bias:\n",
    "            layer.out_proj.bias.copy_(params[\"out_proj_bias\"])\n",
    "        \n",
    "        # Assign LayerNorm for Q\n",
    "        if isinstance(layer.q_ln, nn.LayerNorm):\n",
    "            layer.q_ln.weight.copy_(params[\"q_ln_weight\"])\n",
    "            if bias:\n",
    "                layer.q_ln.bias.copy_(params[\"q_ln_bias\"])\n",
    "        \n",
    "        # Assign LayerNorm for K\n",
    "        if isinstance(layer.k_ln, nn.LayerNorm):\n",
    "            layer.k_ln.weight.copy_(params[\"k_ln_weight\"])\n",
    "            if bias:\n",
    "                layer.k_ln.bias.copy_(params[\"k_ln_bias\"])\n",
    "\n",
    "def assign_params_to_transformer_lens_attention_layer(attention_layer, pre_layer_norm, params, cfg, bias=True):\n",
    "    with torch.no_grad():\n",
    "        # Assign LayerNorm QKV\n",
    "        if isinstance(pre_layer_norm, nn.LayerNorm):\n",
    "            pre_layer_norm.weight.copy_(params[\"layernorm_qkv_weight\"])\n",
    "            pre_layer_norm.bias.copy_(params[\"layernorm_qkv_bias\"])\n",
    "        else:\n",
    "            pre_layer_norm.w.copy_(params[\"layernorm_qkv_weight\"])\n",
    "            pre_layer_norm.b.copy_(params[\"layernorm_qkv_bias\"])\n",
    "\n",
    "        # Extract and split QKV weights\n",
    "        qkv_matrix = params[\"W_qkv_weight\"].clone()  # Shape: (d_model * 3, d_model)\n",
    "        assert qkv_matrix.shape == (cfg.d_model * 3, cfg.d_model), \"QKV weight shape mismatch.\"\n",
    "\n",
    "        qkv_reshaped = qkv_matrix.T  # Shape: (d_model, d_model * 3)\n",
    "        q, k, v = torch.chunk(qkv_reshaped, 3, dim=-1)  # Split into Q, K, V\n",
    "        \n",
    "        reshaper = functools.partial(\n",
    "            einops.rearrange, pattern=\"d_model (n_head d_head) -> n_head d_model d_head\", n_head=cfg.n_heads\n",
    "        )\n",
    "        q, k, v = map(reshaper, (q, k, v))\n",
    "        \n",
    "        # Copy Q, K, V weights\n",
    "        attention_layer.W_Q.copy_(q)\n",
    "        attention_layer.W_K.copy_(k)\n",
    "        attention_layer.W_V.copy_(v)\n",
    "\n",
    "        # Handle QKV bias\n",
    "        if bias and \"W_qkv_bias\" in params:\n",
    "            qkv_bias = params[\"W_qkv_bias\"].clone()  # Shape: (d_model * 3)\n",
    "            b_q, b_k, b_v = torch.chunk(qkv_bias, 3, dim=-1)\n",
    "            reshaper_bias = functools.partial(\n",
    "                einops.rearrange, pattern=\"(n_head d_head) -> n_head d_head\", n_head=cfg.n_heads\n",
    "            )\n",
    "            attention_layer.b_Q.copy_(reshaper_bias(b_q))\n",
    "            attention_layer.b_K.copy_(reshaper_bias(b_k))\n",
    "            attention_layer.b_V.copy_(reshaper_bias(b_v))\n",
    "\n",
    "        # Assign Output Projection\n",
    "        out_proj = params[\"out_proj_weight\"].clone()  # Shape: (d_model, d_model)\n",
    "        assert out_proj.shape == (cfg.d_model, cfg.d_model), \"Output projection weight shape mismatch.\"\n",
    "        out_proj_reshaped = einops.rearrange(out_proj.T, \"(n_head d_head) d_model -> n_head d_head d_model\", n_head=cfg.n_heads)\n",
    "        attention_layer.W_O.copy_(out_proj_reshaped)\n",
    "\n",
    "        # Assign Output Bias\n",
    "        if bias and \"out_proj_bias\" in params:\n",
    "            attention_layer.b_O.copy_(params[\"out_proj_bias\"])\n",
    "\n",
    "        # Assign LayerNorms for Q and K if qk_layernorm is enabled\n",
    "        if cfg.qk_layernorm:\n",
    "            attention_layer.q_ln.w.copy_(params[\"q_ln_weight\"])\n",
    "            attention_layer.k_ln.w.copy_(params[\"k_ln_weight\"])\n",
    "            if bias:\n",
    "                attention_layer.q_ln.b.copy_(params.get(\"q_ln_bias\", torch.zeros(cfg.d_model)))\n",
    "                attention_layer.k_ln.b.copy_(params.get(\"k_ln_bias\", torch.zeros(cfg.d_model)))\n",
    "\n",
    "\n",
    "\n",
    "def create_mlp_params(d_model, expansion_ratio, bias):\n",
    "    hidden_dim = swiglu_correction_fn(expansion_ratio, d_model)\n",
    "    params = {\n",
    "        \"layernorm_weight\": torch.rand(d_model),\n",
    "        \"layernorm_bias\": torch.rand(d_model),\n",
    "        \"l1_weight\": torch.rand(hidden_dim * 2, d_model),\n",
    "        \"l1_bias\": torch.rand(hidden_dim * 2) if bias else None,\n",
    "        \"l2_weight\": torch.rand(d_model, hidden_dim),\n",
    "        \"l2_bias\": torch.rand(d_model) if bias else None,\n",
    "    }\n",
    "    return params\n",
    "\n",
    "\n",
    "def assign_params_to_swiglu_mlp(mdl, params, bias):\n",
    "    with torch.no_grad():\n",
    "        # Assign LayerNorm parameters\n",
    "        mdl[0].weight.copy_(params[\"layernorm_weight\"])\n",
    "        mdl[0].bias.copy_(params[\"layernorm_bias\"])\n",
    "        # Assign first Linear layer parameters\n",
    "        mdl[1].weight.copy_(params[\"l1_weight\"])\n",
    "        if bias:\n",
    "            mdl[1].bias.copy_(params[\"l1_bias\"])\n",
    "        # Assign second Linear layer parameters\n",
    "        mdl[3].weight.copy_(params[\"l2_weight\"])\n",
    "        if bias:\n",
    "            mdl[3].bias.copy_(params[\"l2_bias\"])\n",
    "\n",
    "def assign_params_to_esm_mlp(mdl, params, bias, pre_layer_norm):\n",
    "    with torch.no_grad():\n",
    "        # Assign LayerNorm\n",
    "        if isinstance(pre_layer_norm, nn.LayerNorm): \n",
    "            pre_layer_norm.weight.copy_(params[\"layernorm_weight\"])\n",
    "            pre_layer_norm.bias.copy_(params[\"layernorm_bias\"])\n",
    "        else:\n",
    "            pre_layer_norm.w.copy_(params[\"layernorm_weight\"])\n",
    "            pre_layer_norm.b.copy_(params[\"layernorm_bias\"])\n",
    "        # Assign first Linear layer parameters\n",
    "        mdl.l1.weight.copy_(params[\"l1_weight\"])\n",
    "        if bias:\n",
    "            mdl.l1.bias.copy_(params[\"l1_bias\"])\n",
    "        # Assign second Linear layer parameters\n",
    "        mdl.l2.weight.copy_(params[\"l2_weight\"])\n",
    "        if bias:\n",
    "            mdl.l2.bias.copy_(params[\"l2_bias\"])\n",
    "            \n",
    "def assign_params_to_hooked_esm3_transformer_block(block:HookedEsm3UnifiedTransformerBlock, attention_params, mlp_params ,bias, cfg):\n",
    "    attn = block.attn\n",
    "    attn_layer_norm = block.ln1\n",
    "    assign_params_to_transformer_lens_attention_layer(attn, attn_layer_norm, attention_params, cfg, bias=bias)\n",
    "    mlp = block.mlp\n",
    "    mlp_layer_norm = block.ln2\n",
    "    assign_params_to_esm_mlp(mlp, mlp_params,bias,mlp_layer_norm)\n",
    "\n",
    "\n",
    "def assign_params_to_original_transformer_block(block:UnifiedTransformerBlock, attention_params, mlp_params ,bias):\n",
    "    attention_layer = block.attn\n",
    "    assign_params_to_esm_attention_layer(attention_layer, attention_params, bias)\n",
    "    mlp = block.ffn\n",
    "    assign_params_to_swiglu_mlp(mlp, mlp_params, bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30740ede-25e7-446a-bdc1-99c6d2c7b3ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum absolute difference: tensor(130.)\n",
      "Mean absolute difference: tensor(50.9646)\n"
     ]
    }
   ],
   "source": [
    "d_model = 512\n",
    "n_heads = 8\n",
    "d_head = d_model // n_heads\n",
    "expansion_ratio = 4.0\n",
    "batch_size = 1\n",
    "seq_len = 10\n",
    "qk_layernorm = True\n",
    "residue_scaling_factor = math.sqrt(48 / 36)\n",
    "use_attn_in=False\n",
    "use_hook_mlp_in=False\n",
    "use_split_qkv_input=False\n",
    "bias=False\n",
    "\n",
    "attention_fake_params = create_multi_head_attention_params(d_model=d_model,  n_heads=n_heads, \n",
    "qk_layernorm=qk_layernorm, bias=bias)\n",
    "\n",
    "mlp_fake_params = create_mlp_params(d_model=d_model, expansion_ratio=expansion_ratio, bias=bias)\n",
    "\n",
    "original_block:UnifiedTransformerBlock = UnifiedTransformerBlock(\n",
    "    d_model=d_model,\n",
    "    n_heads=n_heads,\n",
    "    use_geom_attn=False,\n",
    "    use_plain_attn=True,\n",
    "    v_heads=None,\n",
    "    bias=bias,\n",
    "    expansion_ratio=expansion_ratio,\n",
    "    residue_scaling_factor=residue_scaling_factor,\n",
    "    qk_layernorm= qk_layernorm,\n",
    "    ffn_type=\"swiglu\",\n",
    ")\n",
    "\n",
    "assign_params_to_original_transformer_block(original_block, attention_fake_params, mlp_fake_params, bias)\n",
    "# Initialize HookedEsm3UnifiedTransformerBlock\n",
    "cfg = HookedTransformerConfig(\n",
    "n_layers=1,           \n",
    "d_model=d_model,           \n",
    "n_ctx=20,            \n",
    "d_head=d_head,                     \n",
    "n_heads=n_heads,\n",
    "attention_dir=\"bidirectional\",\n",
    "init_weights=False,\n",
    "positional_embedding_type=\"rotary\",\n",
    "rotary_dim=d_head,\n",
    "default_prepend_bos=False,\n",
    "qk_layernorm=qk_layernorm,\n",
    "dtype=torch.float32,\n",
    "use_attn_result=False, \n",
    "esm3_mlp_expansion_ratio=expansion_ratio,\n",
    "act_fn = \"swiglu\",\n",
    "esm3_bias = bias,\n",
    "use_attn_in = use_attn_in,\n",
    "use_hook_mlp_in = use_attn_in,\n",
    "use_split_qkv_input= use_attn_in,\n",
    "esm3_scaling_factor=residue_scaling_factor\n",
    ")\n",
    "hooked_block:HookedEsm3UnifiedTransformerBlock = HookedEsm3UnifiedTransformerBlock(cfg, block_index=0)\n",
    "assign_params_to_hooked_esm3_transformer_block(hooked_block,attention_fake_params, mlp_fake_params, bias, cfg)\n",
    "\n",
    "# Input tensor\n",
    "x = torch.rand((batch_size, seq_len, d_model))\n",
    "\n",
    "# Forward pass\n",
    "with torch.no_grad():\n",
    "    original_output =  original_block.forward(x.clone(), None, None, None, None)\n",
    "    hooked_output = hooked_block.forward(x.clone())\n",
    "\n",
    "# Compare outputs\n",
    "assert torch.allclose(original_output, hooked_output, atol=1e-5, rtol=1e-4), \"Outputs do not match!\"\n",
    "print(\"Maximum absolute difference:\", torch.max(torch.abs(original_output - hooked_output)))\n",
    "print(\"Mean absolute difference:\", torch.mean(torch.abs(original_output - hooked_output)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3ca060c-9461-42c5-bc78-c8fd4ad64951",
   "metadata": {},
   "outputs": [],
   "source": [
    "a=hooked_block.ln2(x.clone())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19f2e3bd-508f-43df-9ada-21ef8fc09ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "b=original_block.ffn[0](x.clone())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "03f6b79c-b207-4237-bbdb-9031c1d1fb1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.7684e-07, grad_fn=<MaxBackward1>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(torch.abs(a - b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aaaf283f-55e0-4652-b804-b27012cebc68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LayerNorm(\n",
       "  (hook_scale): HookPoint()\n",
       "  (hook_normalized): HookPoint()\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hooked_block.ln2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7475a6b3-8ddd-4a14-be69-36d96cc87fd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LayerNorm((512,), eps=1e-05, elementwise_affine=True)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_block.ffn[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae5fc82-d65a-49a0-9c62-6b2a38ad5c1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
