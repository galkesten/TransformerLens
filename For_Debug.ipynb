{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8aa05580-5cab-4b40-90bf-3234c3c34b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Attention:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44a8cf17-575a-44aa-9d75-38dd772183b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TOKENIZERS_PARALLELISM=false\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "%set_env TOKENIZERS_PARALLELISM=false\n",
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "176ac421-cfdf-47b5-a88f-39b22dcc337e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "import torch\n",
    "from jaxtyping import Float\n",
    "from torch.testing import assert_close\n",
    "import torch.nn as nn\n",
    "from transformer_lens.components import Attention\n",
    "from transformer_lens.components import LayerNorm\n",
    "from transformer_lens.components import HookedESM3MLP, swiglu_correction_fn\n",
    "from transformer_lens.components import HookedEsm3UnifiedTransformerBlock\n",
    "from esm.layers.attention import MultiHeadAttention\n",
    "from esm.layers.blocks import swiglu_ln_ffn, UnifiedTransformerBlock\n",
    "from transformer_lens.HookedTransformerConfig import HookedTransformerConfig\n",
    "import functools\n",
    "import einops\n",
    "from esm.utils.constants.esm3 import data_root\n",
    "import math\n",
    "from transformer_lens import HookedESM3,SupportedESM3Config\n",
    "from esm.pretrained import (\n",
    "    ESM3_sm_open_v0,\n",
    ")\n",
    "from esm.models.esm3 import ESM3\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "from esm.tokenization import get_esm3_model_tokenizers\n",
    "from esm.utils.structure.protein_chain import ProteinChain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e7683190-3753-4ec9-8c29-7fd7a101e1b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Please notice the licsence - todo- add licenseSupport for ESM3 in TransformerLens is currently experimental, until such a time when it has feature parity with HookedTransformer and has been tested on real research tasks. Until then, backward compatibility is not guaranteed. Please see the docs for information on the limitations of the current implementation.\n",
      "If using ESM3 for interpretability research, keep in mind that ESM3 has some significant architectural differences to Language transformers like GPT.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  cuda\n",
      "Loaded pretrained model esm3_sm_open_v1 into HookedESM3\n"
     ]
    }
   ],
   "source": [
    "config = SupportedESM3Config(\n",
    "    use_attn_result=True,\n",
    "    use_split_qkv_input=True,\n",
    "    use_hook_mlp_in=True,\n",
    "    use_attn_in=True,\n",
    "    esm3_output_type=\"all\",\n",
    "    esm3_use_torch_layer_norm=True,\n",
    "    esm3_use_torch_attention_calc=True,\n",
    "    esm3_use_org_rotary=True\n",
    ")\n",
    "esm3_hooked1 = HookedESM3.from_pretrained(esm_cfg=config, device=device)\n",
    "esm3_original1 = ESM3_sm_open_v0(device).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3ac154ad-a6ae-4ea0-a21a-3b2ae16c56c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_identical_components(real,hooked):\n",
    "    # Compare parameters\n",
    "    for (name1, param1), (name2, param2) in zip(real.named_parameters(), hooked.named_parameters()):\n",
    "        if name1 != name2:\n",
    "            print(f\"Mismatch in parameter names: {name1} != {name2}\")\n",
    "            return False\n",
    "        assert torch.sum(param1 != param2) == 0\n",
    "\n",
    "    print(f\"verify_identical_components- All parameters match! {type(real)} {type(hooked)} \")\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a836519e-7e0b-4e54-9f63-02bb1861f991",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_transformer_blocks(real_block, hooked_block, cfg):\n",
    "    if cfg.esm3_use_torch_layer_norm:\n",
    "        assert torch.sum(real_block.attn.layernorm_qkv[0].weight !=  hooked_block.ln1.weight) == 0\n",
    "        assert torch.sum(real_block.attn.layernorm_qkv[0].bias !=  hooked_block.ln1.bias) == 0\n",
    "    else:\n",
    "        assert torch.sum(real_block.attn.layernorm_qkv[0].weight !=  hooked_block.ln1.w) == 0\n",
    "        assert torch.sum(real_block.attn.layernorm_qkv[0].bias !=  hooked_block.ln1.b) == 0\n",
    "        \n",
    "    qkv_matrix = real_block.attn.layernorm_qkv[1].weight\n",
    "    query_BLD, key_BLD, value_BLD = torch.chunk(qkv_matrix, 3, dim=-2)\n",
    "    q = einops.rearrange(hooked_block.attn.W_Q, \"n_head d_model d_head ->(n_head d_head) d_model\", n_head=hooked_block.attn.W_Q.shape[0])\n",
    "    v = einops.rearrange(hooked_block.attn.W_V, \"n_head d_model d_head ->(n_head d_head) d_model\", n_head=hooked_block.attn.W_V.shape[0])\n",
    "    k = einops.rearrange(hooked_block.attn.W_K, \"n_head d_model d_head ->(n_head d_head) d_model\", n_head=hooked_block.attn.W_K.shape[0])\n",
    "    assert torch.sum(query_BLD !=q) == 0\n",
    "    assert torch.sum(key_BLD !=k) == 0\n",
    "    assert torch.sum(value_BLD !=v) == 0\n",
    "    assert(real_block.attn.layernorm_qkv[1].bias is None)\n",
    "    assert torch.equal(hooked_block.attn.b_Q, torch.zeros_like(hooked_block.attn.b_Q)), \"The tensor is not all zeros.\"\n",
    "    assert torch.equal(hooked_block.attn.b_K, torch.zeros_like(hooked_block.attn.b_K)), \"The tensor is not all zeros.\"\n",
    "    assert torch.equal(hooked_block.attn.b_V, torch.zeros_like(hooked_block.attn.b_V)), \"The tensor is not all zeros.\"\n",
    "    \n",
    "    if cfg.esm3_use_torch_layer_norm:\n",
    "        assert torch.sum(real_block.attn.q_ln.weight !=  hooked_block.attn.q_ln.weight) == 0\n",
    "        assert torch.sum(real_block.attn.k_ln.weight != hooked_block.attn.k_ln.weight) == 0\n",
    "        assert real_block.attn.q_ln.bias is None\n",
    "        assert hooked_block.attn.q_ln.bias is None\n",
    "        assert real_block.attn.k_ln.bias is None\n",
    "        assert hooked_block.attn.k_ln.bias is None\n",
    "    else:\n",
    "        assert torch.sum(real_block.attn.q_ln.weight !=  hooked_block.attn.q_ln.w) == 0\n",
    "        assert torch.sum(real_block.attn.k_ln.weight != hooked_block.attn.k_ln.w) == 0\n",
    "        assert real_block.attn.q_ln.bias is None\n",
    "        assert torch.equal(hooked_block.attn.q_ln.b, torch.zeros_like(hooked_block.attn.q_ln.b)), \"The tensor is not all zeros.\"\n",
    "        assert real_block.attn.k_ln.bias is None\n",
    "        assert torch.equal(hooked_block.attn.k_ln.b, torch.zeros_like(hooked_block.attn.k_ln.b)), \"The tensor is not all zeros.\"\n",
    "\n",
    "    \n",
    "    out_proj = real_block.attn.out_proj.weight\n",
    "    W_O= einops.rearrange(hooked_block.attn.W_O, \"n_head d_head d_model -> d_model (n_head d_head)\", n_head=hooked_block.attn.W_O.shape[0])\n",
    "    assert torch.sum(W_O !=out_proj) == 0\n",
    "    assert real_block.attn.out_proj.bias is None\n",
    "    assert torch.equal(hooked_block.attn.b_O, torch.zeros_like(hooked_block.attn.b_O)), \"The tensor is not all zeros.\"\n",
    "\n",
    "    assert real_block.use_geom_attn == hooked_block.use_geom_attn\n",
    "    if real_block.use_geom_attn:\n",
    "        verify_identical_components(real_block.geom_attn, hooked_block.geom_attn)\n",
    "    if cfg.esm3_use_torch_layer_norm:\n",
    "        assert torch.sum(real_block.ffn[0].weight !=  hooked_block.ln2.weight) == 0\n",
    "        assert torch.sum(real_block.ffn[0].bias !=  hooked_block.ln2.bias) == 0\n",
    "    else:\n",
    "        assert torch.sum(real_block.ffn[0].weight !=  hooked_block.ln2.w) == 0\n",
    "        assert torch.sum(real_block.ffn[0].bias !=  hooked_block.ln2.b) == 0\n",
    "    assert torch.sum(real_block.ffn[1].weight !=  hooked_block.mlp.l1.weight) == 0\n",
    "    assert(real_block.ffn[1].bias is None)\n",
    "    assert(hooked_block.mlp.l1.bias is None)\n",
    "    assert torch.sum(real_block.ffn[3].weight !=  hooked_block.mlp.l2.weight) == 0\n",
    "    assert(real_block.ffn[3].bias is None)\n",
    "    assert(hooked_block.mlp.l2.bias is None)\n",
    "    print(\"compare_transformer_blocks- all params match\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c606b3f5-2d37-4e93-a605-55ab24d062f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_loading(esm3_original, esm3_hooked, cfg):\n",
    "    verify_identical_components(esm3_original.encoder ,esm3_hooked.embed.embed)\n",
    "    for l in range(len(esm3_original.transformer.blocks)):\n",
    "        real_block = esm3_original.transformer.blocks[l]\n",
    "        hooked_block = esm3_hooked.blocks[l]\n",
    "        compare_transformer_blocks(real_block, hooked_block, cfg)\n",
    "    if cfg.esm3_use_torch_layer_norm:\n",
    "        assert torch.sum(esm3_original.transformer.norm.weight !=  esm3_hooked.ln_final.weight) == 0\n",
    "        assert esm3_hooked.ln_final.bias is None\n",
    "        assert esm3_original.transformer.norm.bias is None\n",
    "    else:\n",
    "        assert torch.sum(esm3_original.transformer.norm.weight !=  esm3_hooked.ln_final.w) == 0\n",
    "        assert torch.equal(esm3_hooked.ln_final.b, torch.zeros_like(esm3_hooked.ln_final.b)), \"The tensor is not all zeros.\"\n",
    "    verify_identical_components(esm3_original.output_heads , esm3_hooked.unembed.output_heads)\n",
    "\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "99406978-83d9-4118-864e-3db47a163f66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "verify_identical_components- All parameters match! <class 'esm.models.esm3.EncodeInputs'> <class 'esm.models.esm3.EncodeInputs'> \n",
      "verify_identical_components- All parameters match! <class 'esm.layers.geom_attention.GeometricReasoningOriginalImpl'> <class 'esm.layers.geom_attention.GeometricReasoningOriginalImpl'> \n",
      "compare_transformer_blocks- all params match\n",
      "compare_transformer_blocks- all params match\n",
      "compare_transformer_blocks- all params match\n",
      "compare_transformer_blocks- all params match\n",
      "compare_transformer_blocks- all params match\n",
      "compare_transformer_blocks- all params match\n",
      "compare_transformer_blocks- all params match\n",
      "compare_transformer_blocks- all params match\n",
      "compare_transformer_blocks- all params match\n",
      "compare_transformer_blocks- all params match\n",
      "compare_transformer_blocks- all params match\n",
      "compare_transformer_blocks- all params match\n",
      "compare_transformer_blocks- all params match\n",
      "compare_transformer_blocks- all params match\n",
      "compare_transformer_blocks- all params match\n",
      "compare_transformer_blocks- all params match\n",
      "compare_transformer_blocks- all params match\n",
      "compare_transformer_blocks- all params match\n",
      "compare_transformer_blocks- all params match\n",
      "compare_transformer_blocks- all params match\n",
      "compare_transformer_blocks- all params match\n",
      "compare_transformer_blocks- all params match\n",
      "compare_transformer_blocks- all params match\n",
      "compare_transformer_blocks- all params match\n",
      "compare_transformer_blocks- all params match\n",
      "compare_transformer_blocks- all params match\n",
      "compare_transformer_blocks- all params match\n",
      "compare_transformer_blocks- all params match\n",
      "compare_transformer_blocks- all params match\n",
      "compare_transformer_blocks- all params match\n",
      "compare_transformer_blocks- all params match\n",
      "compare_transformer_blocks- all params match\n",
      "compare_transformer_blocks- all params match\n",
      "compare_transformer_blocks- all params match\n",
      "compare_transformer_blocks- all params match\n",
      "compare_transformer_blocks- all params match\n",
      "compare_transformer_blocks- all params match\n",
      "compare_transformer_blocks- all params match\n",
      "compare_transformer_blocks- all params match\n",
      "compare_transformer_blocks- all params match\n",
      "compare_transformer_blocks- all params match\n",
      "compare_transformer_blocks- all params match\n",
      "compare_transformer_blocks- all params match\n",
      "compare_transformer_blocks- all params match\n",
      "compare_transformer_blocks- all params match\n",
      "compare_transformer_blocks- all params match\n",
      "compare_transformer_blocks- all params match\n",
      "compare_transformer_blocks- all params match\n",
      "verify_identical_components- All parameters match! <class 'esm.models.esm3.OutputHeads'> <class 'esm.models.esm3.OutputHeads'> \n"
     ]
    }
   ],
   "source": [
    "test_loading(esm3_original1, esm3_hooked1, esm3_hooked1.cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "dcb58659-b880-4f98-9d31-9d2751f23582",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizers = get_esm3_model_tokenizers()\n",
    "esm3_original1.eval()  # Switch to evaluation mode to save memory\n",
    "esm3_hooked1.eval()\n",
    "sequence = \"MKSLLLLSILAALAVAALCYESHESLESYEINPFINRRNANSFISPQQRWRAKAQERIRELNKPQYELNREACDDFKLCERYAMVYGYNAAYDRYFRQRRGAK\"\n",
    "tokens = tokenizers.sequence.encode(sequence)\n",
    "sequence_tokens = torch.tensor(tokens, dtype=torch.int64)\n",
    "sequence_tokens = sequence_tokens.to(device).unsqueeze(0)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output1 = esm3_original1.forward(\n",
    "        sequence_tokens=sequence_tokens\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d5f0636f-27e3-4064-aa47-06a2198b70f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    output2 = esm3_hooked1.forward(\n",
    "        sequence_tokens=sequence_tokens\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4c2dc71d-1463-47a1-ad20-5df5d5d11b78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.0518e-05, device='cuda:0')"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(torch.abs(output1.residue_logits-output2.residue_logits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ec5ba26a-0633-41b5-8f75-b8115556a713",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert torch.allclose(output1.residue_logits, output2.residue_logits,  rtol=1.3e-6, atol=4e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "64f14e17-1b72-4246-95f6-4eb1fb28efa4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-20.0799, -20.0326, -19.9857,  ..., -20.0251, -20.0564, -19.9789],\n",
       "         [-17.5317, -17.4406, -17.4318,  ..., -17.4344, -17.5385, -17.4107],\n",
       "         [-20.0069, -19.9976, -20.0968,  ..., -20.1231, -20.1056, -20.0137],\n",
       "         ...,\n",
       "         [-19.6266, -19.5679, -19.5718,  ..., -19.6159, -19.5687, -19.6079],\n",
       "         [-18.6520, -18.6278, -18.6809,  ..., -18.6770, -18.6054, -18.6648],\n",
       "         [-18.6440, -18.5629, -18.6397,  ..., -18.6421, -18.5863, -18.6621]]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output1.function_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eba77a4a-6bb7-4215-a812-0f51798257a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.testing import assert_close\n",
    "assert_close(output1.sequence_logits, output2.sequence_logits, rtol=1e-6, atol=2e-5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "eeb44910-acaf-412d-bbbe-3035b006d7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence2 = \"MLPGLALLLLAAWTARALEVPTDGNAGLLAEPQIAMFCGRLNMHMNVQNGKWDSDPSGTKTCIDTKEGILQYCQEVYPELQITNVVEANQPVTIQNWCKRGRKQCKTHPHFVIPYRCLVGEFVSDALLVPDKCKFLHQERMDVCETHLHWHTVAKETCSEKSTNLHDYGMLLPCGIDKFRGVEFVCCPLAEESDNVDSADAEEDDSDVWWGGADTDYADGSEDKVVEVAEEEEVAEVEEEEADDDEDDEDGDEVEEEAEEPYEEATERTTSIATTTTTTTESVEEVVREVCSEQAETGPCRAMISRWYFDVTEGKCAPFFYGGCGGNRNNFDTEEYCMAVCGSAMSQSLLKTTQEPLARDPVKLPTTAASTPDAVDKYLETPGDENEHAHFQKAKERLEAKHRERMSQVMREWEEAERQAKNLPKADKKAVIQHFQEKVESLEQEAANERQQLVETHMARVEAMLNDRRRLALENYITALQAVPPRPRHVFNMLKKYVRAEQKDRQHTLKHFEHVRMVDPKKAAQIRSQVMTHLRVIYERMNQSLSLLYNVPAVAEEIQDEVDELLQKEQNYSDDVLANMISEPRISYGNDALMPSLTETKTTVELLPVNGEFSLDDLQPWHSFGADSVPANTENEVEPVDARPAADRGLTTRPGSGLTNIKTEEISEVKMDAEFRHDSGYEVHHQKLVFFAEDVGSNKGAIIGLMVGGVVIATVIVITLVMLKKKQYTSIHHGVVEVDAAVTPEERHLSKMQQNGYENPTYKFFEQMQN\"\n",
    "tokens2 = tokenizers.sequence.encode(sequence2)\n",
    "sequence_tokens2 = torch.tensor(tokens2, dtype=torch.int64)\n",
    "sequence_tokens2 = sequence_tokens2.to(device).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1c81f531-1db9-4992-a63e-df2347102776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hook triggered for encoder\n",
      "Hook triggered for layer_norm_output\n",
      "Hook triggered for layer_norm_output\n",
      "after q_ln\n",
      "after k_ln\n",
      "Hook triggered for rotary\n",
      "Hook triggered for org_attn_out_proj_input\n",
      "Hook triggered for org_attn_out\n",
      "Hook triggered for org_geo_attn_out\n",
      "org_mlp_in\n",
      "org_mlp_out\n",
      "org_residual_post\n",
      "Hook triggered for embedding\n",
      "Hook triggered for hook_post_layer_norm\n",
      "Hook triggered for q\n",
      "Hook triggered for q_ln\n",
      "Hook triggered for k_ln\n",
      "Hook triggered hook_rot_q\n",
      "Hook triggered for hook_rot_k\n",
      "Hook triggered for hook_z \n",
      "Hook triggered for attn_out\n",
      "Hook triggered for geo attn_out\n",
      "Hook triggered for hook_mlp_in\n",
      "Hook triggered for hook_mlp_out\n",
      "Hook triggered for hook_residual_post \n"
     ]
    }
   ],
   "source": [
    "encoder_output = None\n",
    "layer_norm_input=None\n",
    "layer_norm_output=None\n",
    "org_attn_out_proj_input=None\n",
    "org_attn_out = None\n",
    "org_geo_attn_out = None\n",
    "org_mlp_in = None\n",
    "org_mlp_out = None\n",
    "org_residual_post = None\n",
    "org_rot = None\n",
    "org_rot_input=None\n",
    "org_q_input = None\n",
    "embed_output = None\n",
    "hook_attn_in=None\n",
    "hook_post_layer_norm=None\n",
    "hook_attn_out=None\n",
    "hook_geo_attn_out=None\n",
    "hook_mlp_in = None\n",
    "hook_mlp_out = None\n",
    "hook_residual_post = None\n",
    "q_ln_org=None\n",
    "k_ln_org=None\n",
    "hook_q = None\n",
    "hook_q_ln=None\n",
    "hook_k_ln=None\n",
    "hook_z = None\n",
    "hook_rot_q= None\n",
    "hook_rot_k=None\n",
    "# Define the hook function\n",
    "def hook_fn1(activation, hook):\n",
    "    global embed_output  # To store the output globally\n",
    "    print(\"Hook triggered for embedding\")\n",
    "    embed_output = activation\n",
    "    \n",
    "# Define the hook function\n",
    "def hook_fn2(activation, hook):\n",
    "    global hook_attn_in  # To store the output globally\n",
    "    print(\"Hook triggered for attn in\")\n",
    "    hook_attn_in = activation\n",
    "    \n",
    "def hook_fn3(activation, hook):\n",
    "    global hook_attn_out  # To store the output globally\n",
    "    print(\"Hook triggered for attn_out\")\n",
    "    hook_attn_out = activation\n",
    "\n",
    "def hook_fn4(activation, hook):\n",
    "    global hook_geo_attn_out  # To store the output globally\n",
    "    print(\"Hook triggered for geo attn_out\")\n",
    "    hook_geo_attn_out = activation\n",
    "\n",
    "def hook_fn5(activation, hook):\n",
    "    global hook_mlp_in   # To store the output globally\n",
    "    print(\"Hook triggered for hook_mlp_in\")\n",
    "    hook_mlp_in  = activation\n",
    "\n",
    "def hook_fn6(activation, hook):\n",
    "    global hook_mlp_out  # To store the output globally\n",
    "    print(\"Hook triggered for hook_mlp_out\")\n",
    "    hook_mlp_out  = activation\n",
    "\n",
    "def hook_fn7(activation, hook):\n",
    "    global hook_residual_post   # To store the output globally\n",
    "    print(\"Hook triggered for hook_residual_post \")\n",
    "    hook_residual_post  = activation\n",
    "    \n",
    "def hook_fn8(module, input, output):\n",
    "    global encoder_output  # Declare as global to modify the global variable\n",
    "    print(\"Hook triggered for encoder\")\n",
    "    encoder_output = output \n",
    "\n",
    "def hook_fn9(module, input, output):\n",
    "    global layer_norm_output  # Declare as global to modify the global variable\n",
    "    print(\"Hook triggered for layer_norm_output\")\n",
    "    layer_norm_output = output \n",
    "def hook_fn10(module, input, output):\n",
    "    global org_attn_out  # Declare as global to modify the global variable\n",
    "    print(\"Hook triggered for org_attn_out\")\n",
    "    org_attn_out = output\n",
    "def hook_fn11(module, input, output):\n",
    "    global org_geo_attn_out # Declare as global to modify the global variable\n",
    "    print(\"Hook triggered for org_geo_attn_out\")\n",
    "    org_geo_attn_out = output\n",
    "def hook_fn12(module, input, output):\n",
    "    global org_mlp_in # Declare as global to modify the global variable\n",
    "    print(\"org_mlp_in\")\n",
    "    org_mlp_in = input\n",
    "def hook_fn13(module, input, output):\n",
    "    global org_mlp_out # Declare as global to modify the global variable\n",
    "    print(\"org_mlp_out\")\n",
    "    org_mlp_out = output\n",
    "def hook_fn14(module, input, output):\n",
    "    global org_residual_post # Declare as global to modify the global variable\n",
    "    print(\"org_residual_post\")\n",
    "    org_residual_post = output\n",
    "\n",
    "def hook_fn15(activation, hook):\n",
    "    global hook_q_ln   # To store the output globally\n",
    "    print(\"Hook triggered for q_ln\")\n",
    "    hook_q_ln  = activation\n",
    "    \n",
    "def hook_fn16(activation, hook):\n",
    "    global hook_k_ln   # To store the output globally\n",
    "    print(\"Hook triggered for k_ln\")\n",
    "    hook_k_ln  = activation\n",
    "def hook_fn17(module, input, output):\n",
    "    global q_ln_org # Declare as global to modify the global variable\n",
    "    global org_q_input\n",
    "    print(\"after q_ln\")\n",
    "    q_ln_org = output\n",
    "    org_q_input = input\n",
    "\n",
    "def hook_fn18(module, input, output):\n",
    "    global k_ln_org # Declare as global to modify the global variable\n",
    "    print(\"after k_ln\")\n",
    "    k_ln_org = output\n",
    "\n",
    "def hook_fn19(activation, hook):\n",
    "    global hook_post_layer_norm  # To store the output globally\n",
    "    print(\"Hook triggered for hook_post_layer_norm\")\n",
    "    hook_post_layer_norm = activation\n",
    "\n",
    "def hook_fn20(module, input, output):\n",
    "    global layer_norm_input  # Declare as global to modify the global variable\n",
    "    print(\"Hook triggered for layer_norm_input\")\n",
    "    layer_norm_input = input\n",
    "def hook_fn21(module, input, output):\n",
    "    global org_attn_out_proj_input  # Declare as global to modify the global variable\n",
    "    print(\"Hook triggered for org_attn_out_proj_input\")\n",
    "    org_attn_out_proj_input = input\n",
    "def hook_fn22(activation, hook):\n",
    "    global hook_z   # To store the output globally\n",
    "    print(\"Hook triggered for hook_z \")\n",
    "    hook_z  = activation\n",
    "def rotary_hook(module, input, output):\n",
    "    global org_rot, org_rot_input\n",
    "    print(\"Hook triggered for rotary\")\n",
    "    org_rot = output\n",
    "    org_rot_input = input\n",
    "\n",
    "    org_rot = output\n",
    "def hook_fn24(activation, hook):\n",
    "    global hook_rot_q   # To store the output globally\n",
    "    print(\"Hook triggered hook_rot_q\")\n",
    "    hook_rot_q  = activation\n",
    "    \n",
    "def hook_fn25(activation, hook):\n",
    "    global hook_rot_k  # To store the output globally\n",
    "    print(\"Hook triggered for hook_rot_k\")\n",
    "    hook_rot_k  = activation\n",
    "\n",
    "def hook_q(activation, hook):\n",
    "    global hook_q\n",
    "    print(\"Hook triggered for q\")\n",
    "    hook_q = activation\n",
    "    \n",
    "from torch.testing import assert_close\n",
    "esm3_hooked1.eval()\n",
    "esm3_original1.eval()\n",
    "with torch.no_grad():\n",
    "    esm3_original1.encoder.register_forward_hook(hook_fn8)\n",
    "    esm3_original1.transformer.blocks[0].attn.layernorm_qkv[0].register_forward_hook(hook_fn9)\n",
    "    esm3_original1.transformer.blocks[0].attn.out_proj.register_forward_hook(hook_fn21)\n",
    "    esm3_original1.transformer.blocks[0].attn.rotary.register_forward_hook(rotary_hook)\n",
    "    esm3_original1.transformer.blocks[0].attn.layernorm_qkv[0].register_forward_hook(hook_fn9)\n",
    "    esm3_original1.transformer.blocks[0].attn.q_ln.register_forward_hook(hook_fn17)\n",
    "    esm3_original1.transformer.blocks[0].attn.k_ln.register_forward_hook(hook_fn18)\n",
    "    esm3_original1.transformer.blocks[0].attn.register_forward_hook(hook_fn10)\n",
    "    esm3_original1.transformer.blocks[0].geom_attn.register_forward_hook(hook_fn11)\n",
    "    esm3_original1.transformer.blocks[0].ffn.register_forward_hook(hook_fn12)\n",
    "    esm3_original1.transformer.blocks[0].ffn.register_forward_hook(hook_fn13)\n",
    "    esm3_original1.transformer.blocks[0].register_forward_hook(hook_fn14)\n",
    "    output3 = esm3_original1.forward(\n",
    "        sequence_tokens=sequence_tokens2\n",
    "    )\n",
    "\n",
    "with torch.no_grad():\n",
    "    esm3_hooked1.add_hook(\"hook_embed\", hook_fn1)\n",
    "    esm3_hooked1.add_hook(\"blocks.0.hook_attn_in\", hook_fn2)\n",
    "    esm3_hooked1.add_hook(\"blocks.0.hook_post_layer_norm\", hook_fn19)\n",
    "    esm3_hooked1.add_hook(\"blocks.0.hook_attn_out\", hook_fn3)\n",
    "    esm3_hooked1.add_hook(\"blocks.0.hook_geo_attn_out\", hook_fn4)\n",
    "    esm3_hooked1.add_hook(\"blocks.0.hook_mlp_in\", hook_fn5)\n",
    "    esm3_hooked1.add_hook(\"blocks.0.hook_mlp_out\", hook_fn6)\n",
    "    esm3_hooked1.add_hook(\"blocks.0.hook_resid_post\", hook_fn7)\n",
    "    esm3_hooked1.add_hook(\"blocks.0.attn.hook_ln_q\", hook_fn15)\n",
    "    esm3_hooked1.add_hook(\"blocks.0.attn.hook_ln_k\", hook_fn16)\n",
    "    esm3_hooked1.add_hook(\"blocks.0.attn.hook_z\", hook_fn22)\n",
    "    esm3_hooked1.add_hook(\"blocks.0.attn.hook_rot_k\", hook_fn25)\n",
    "    esm3_hooked1.add_hook(\"blocks.0.attn.hook_rot_q\", hook_fn24)\n",
    "    esm3_hooked1.add_hook(\"blocks.0.attn.hook_q\", hook_q)\n",
    "    output4 = esm3_hooked1.forward(\n",
    "        sequence_tokens=sequence_tokens2\n",
    "    )\n",
    "\n",
    "assert torch.allclose(output3.sequence_logits, output4.sequence_logits, atol=1e-4, rtol=1e-4)\n",
    "\n",
    "assert torch.allclose(output3.structure_logits, output4.structure_logits, atol=1e-4, rtol=1e-4)\n",
    "\n",
    "assert torch.allclose(output3.sasa_logits, output4.sasa_logits, atol=1e-4, rtol=1e-4)\n",
    "\n",
    "assert torch.allclose(output3.secondary_structure_logits, output4.secondary_structure_logits, atol=1e-4, rtol=1e-4)\n",
    "\n",
    "assert torch.allclose(output3.function_logits, output4.function_logits, atol=1e-4, rtol=1e-4)\n",
    "\n",
    "assert torch.allclose(output3.residue_logits, output4.residue_logits, atol=1e-4, rtol=1e-4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7aaf2fd0-ea2f-4c8c-bbad-73a25dcab128",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert torch.equal(encoder_output,embed_output)\n",
    "#assert torch.equal(layer_norm_output, hook_post_layer_norm[...,1,:])\n",
    "q_flattened = einops.rearrange(hook_q_ln, \"batch pos head_index d_head -> batch pos (head_index d_head)\")\n",
    "assert torch.allclose(q_ln_org, q_flattened, rtol=1.3e-6, atol=4e-5)\n",
    "k_flattened = einops.rearrange(hook_k_ln, \"batch pos head_index d_head -> batch pos (head_index d_head)\")\n",
    "assert torch.allclose(k_ln_org, k_flattened, rtol=1.3e-6, atol=4e-5)\n",
    "assert torch.allclose(hook_rot_k, org_rot[1], rtol=1.3e-6, atol=4e-5)\n",
    "assert torch.allclose(hook_rot_q, org_rot[0], rtol=1.3e-6, atol=4e-5)\n",
    "assert torch.allclose(org_attn_out, hook_attn_out, rtol=1.3e-6, atol=4e-5)\n",
    "assert torch.allclose(org_geo_attn_out,hook_geo_attn_out, rtol=1.3e-6, atol=4e-5)\n",
    "assert torch.allclose(hook_mlp_in, org_mlp_in[0], rtol=1.3e-6, atol=4e-5)\n",
    "assert torch.allclose(hook_mlp_out, org_mlp_out, rtol=3e-6, atol=4e-5)\n",
    "assert torch.allclose(hook_residual_post, org_residual_post, rtol=1.3e-6, atol=4e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "48aa9e02-9ec7-4cf7-883c-3f7e755be4cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hook triggered for encoder\n",
      "Hook triggered for layer_norm_output\n",
      "Hook triggered for layer_norm_output\n",
      "after q_ln\n",
      "after k_ln\n",
      "Hook triggered for rotary\n",
      "Hook triggered for org_attn_out_proj_input\n",
      "Hook triggered for org_attn_out\n",
      "Hook triggered for org_geo_attn_out\n",
      "org_mlp_in\n",
      "org_mlp_out\n",
      "org_residual_post\n",
      "Hook triggered for embedding\n",
      "Hook triggered for hook_post_layer_norm\n",
      "Hook triggered for q\n",
      "Hook triggered for q_ln\n",
      "Hook triggered for k_ln\n",
      "Hook triggered hook_rot_q\n",
      "Hook triggered for hook_rot_k\n",
      "Hook triggered for hook_z \n",
      "Hook triggered for attn_out\n",
      "Hook triggered for geo attn_out\n",
      "Hook triggered for hook_mlp_in\n",
      "Hook triggered for hook_mlp_out\n",
      "Hook triggered for hook_residual_post \n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 47.44 GiB of which 21.38 MiB is free. Including non-PyTorch memory, this process has 47.41 GiB memory in use. Of the allocated memory 47.05 GiB is allocated by PyTorch, and 48.09 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m esm3_hooked1\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39muse_attn_result\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m      2\u001b[0m output3 \u001b[38;5;241m=\u001b[39m esm3_original1\u001b[38;5;241m.\u001b[39mforward(\n\u001b[1;32m      3\u001b[0m         sequence_tokens\u001b[38;5;241m=\u001b[39msequence_tokens2\n\u001b[1;32m      4\u001b[0m     )\n\u001b[0;32m----> 5\u001b[0m output4 \u001b[38;5;241m=\u001b[39m \u001b[43mesm3_hooked1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43msequence_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msequence_tokens2\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m torch\u001b[38;5;241m.\u001b[39mmax(torch\u001b[38;5;241m.\u001b[39mabs(output3\u001b[38;5;241m.\u001b[39msequence_logits\u001b[38;5;241m-\u001b[39moutput4\u001b[38;5;241m.\u001b[39msequence_logits))\n",
      "File \u001b[0;32m~/TransformerLens/transformer_lens/HookedESM3.py:297\u001b[0m, in \u001b[0;36mHookedESM3.forward\u001b[0;34m(self, sequence_tokens, structure_tokens, ss8_tokens, sasa_tokens, function_tokens, residue_annotation_tokens, average_plddt, per_res_plddt, structure_coords, chain_id, sequence_id, return_type)\u001b[0m\n\u001b[1;32m    286\u001b[0m resid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhook_embed(\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed(sequence_tokens,\n\u001b[1;32m    288\u001b[0m     structure_tokens,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    293\u001b[0m     function_tokens,\n\u001b[1;32m    294\u001b[0m     residue_annotation_tokens,))\n\u001b[1;32m    296\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks:\n\u001b[0;32m--> 297\u001b[0m     resid  \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msequence_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maffine\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maffine_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchain_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    298\u001b[0m normalised \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_final(resid)\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/tl_esm_env_old_torch_1/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/tl_esm_env_old_torch_1/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/TransformerLens/transformer_lens/components/esm3_hooked_block.py:117\u001b[0m, in \u001b[0;36mHookedEsm3UnifiedTransformerBlock.forward\u001b[0;34m(self, resid_pre, sequence_id, frames, frames_mask, chain_id)\u001b[0m\n\u001b[1;32m    110\u001b[0m     key_input \u001b[38;5;241m=\u001b[39m attn_in\n\u001b[1;32m    111\u001b[0m     value_input \u001b[38;5;241m=\u001b[39m attn_in\n\u001b[1;32m    113\u001b[0m attn_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhook_attn_out(\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;66;03m# hook the residual stream states that are used to calculate the\u001b[39;00m\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;66;03m# queries, keys and values, independently.\u001b[39;00m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m# Then take the layer norm of these inputs, and pass these to the attention module.\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhook_post_layer_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mln1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_input\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mln1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey_input\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalue_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mln1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue_input\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_kv_cache_entry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    124\u001b[0m )  \u001b[38;5;66;03m# [batch, pos, d_model]\u001b[39;00m\n\u001b[1;32m    125\u001b[0m scaled_attn_out \u001b[38;5;241m=\u001b[39m attn_out \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mesm3_scaling_factor\n\u001b[1;32m    126\u001b[0m resid_mid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhook_resid_mid(resid_pre \u001b[38;5;241m+\u001b[39mscaled_attn_out)  \u001b[38;5;66;03m# [batch, pos, d_model]\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/tl_esm_env_old_torch_1/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/tl_esm_env_old_torch_1/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/TransformerLens/transformer_lens/components/abstract_attention.py:208\u001b[0m, in \u001b[0;36mAbstractAttention.forward\u001b[0;34m(self, query_input, key_input, value_input, past_kv_cache_entry, additive_attention_mask, attention_mask, position_bias)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    181\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    182\u001b[0m     query_input: Union[\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    199\u001b[0m     position_bias: Optional[Float[torch\u001b[38;5;241m.\u001b[39mTensor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1 head_index pos kv_pos\u001b[39m\u001b[38;5;124m\"\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    200\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Float[torch\u001b[38;5;241m.\u001b[39mTensor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch pos d_model\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    201\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;124;03m    shortformer_pos_embed is only used if self.cfg.positional_embedding_type == \"shortformer\", else defaults to None and is irrelevant. See HookedTransformerConfig for more details\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;124;03m    past_kv_cache_entry is an optional entry of past keys and values for this layer, only relevant if generating text. Defaults to None\u001b[39;00m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;124;03m    additive_attention_mask is an optional mask to add to the attention weights. Defaults to None.\u001b[39;00m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;124;03m    attention_mask is the attention mask for padded tokens. Defaults to None.\u001b[39;00m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 208\u001b[0m     q, k, v \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcalculate_qkv_matrices\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mqk_layernorm:\n\u001b[1;32m    210\u001b[0m         \u001b[38;5;66;03m# Rearrange q and k to concatenate the last two dimensions\u001b[39;00m\n\u001b[1;32m    211\u001b[0m         q_flattened \u001b[38;5;241m=\u001b[39m einops\u001b[38;5;241m.\u001b[39mrearrange(q, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch pos head_index d_head -> batch pos (head_index d_head)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/TransformerLens/transformer_lens/components/abstract_attention.py:422\u001b[0m, in \u001b[0;36mAbstractAttention.calculate_qkv_matrices\u001b[0;34m(self, query_input, key_input, value_input)\u001b[0m\n\u001b[1;32m    409\u001b[0m     k \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhook_k(\n\u001b[1;32m    410\u001b[0m         \u001b[38;5;66;03m# call bitsandbytes method to dequantize and multiply\u001b[39;00m\n\u001b[1;32m    411\u001b[0m         bnb\u001b[38;5;241m.\u001b[39mmatmul_4bit(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    419\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mb_K\n\u001b[1;32m    420\u001b[0m     )\n\u001b[1;32m    421\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 422\u001b[0m     k \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhook_k(\u001b[43mattn_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mW_K\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mb_K\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    424\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mload_in_4bit:\n\u001b[1;32m    425\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW_V, Params4bit):\n",
      "File \u001b[0;32m~/TransformerLens/transformer_lens/utilities/attention.py:22\u001b[0m, in \u001b[0;36msimple_attn_linear\u001b[0;34m(input, w, b)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mall(b_\u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m     21\u001b[0m     b_\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb_\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], b\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], b\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 47.44 GiB of which 21.38 MiB is free. Including non-PyTorch memory, this process has 47.41 GiB memory in use. Of the allocated memory 47.05 GiB is allocated by PyTorch, and 48.09 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "esm3_hooked1.cfg.use_attn_result=False\n",
    "output3 = esm3_original1.forward(\n",
    "        sequence_tokens=sequence_tokens2\n",
    "    )\n",
    "output4 = esm3_hooked1.forward(\n",
    "        sequence_tokens=sequence_tokens2\n",
    "    )\n",
    "torch.max(torch.abs(output3.sequence_logits-output4.sequence_logits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a0e8ed79-33d1-473e-9950-747a9d01418f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hook triggered for embedding\n",
      "Hook triggered for attn in\n",
      "Hook triggered for hook_post_layer_norm\n",
      "Hook triggered for q\n",
      "Hook triggered for q_ln\n",
      "Hook triggered for k_ln\n",
      "Hook triggered hook_rot_q\n",
      "Hook triggered for hook_rot_k\n",
      "Hook triggered for hook_z \n",
      "Hook triggered for attn_out\n",
      "Hook triggered for geo attn_out\n",
      "Hook triggered for hook_mlp_in\n",
      "Hook triggered for hook_mlp_out\n",
      "Hook triggered for hook_residual_post \n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 6.79 GiB. GPU 0 has a total capacity of 47.44 GiB of which 6.72 GiB is free. Including non-PyTorch memory, this process has 40.71 GiB memory in use. Of the allocated memory 40.35 GiB is allocated by PyTorch, and 48.63 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m res, cache \u001b[38;5;241m=\u001b[39m \u001b[43mesm3_hooked1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_with_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43msequence_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msequence_tokens2\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/TransformerLens/transformer_lens/HookedESM3.py:334\u001b[0m, in \u001b[0;36mHookedESM3.run_with_cache\u001b[0;34m(self, return_cache_object, remove_batch_dim, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_with_cache\u001b[39m(\n\u001b[1;32m    323\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    324\u001b[0m     \u001b[38;5;241m*\u001b[39mmodel_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    329\u001b[0m         Union[ActivationCache, Dict[\u001b[38;5;28mstr\u001b[39m, torch\u001b[38;5;241m.\u001b[39mTensor]]\n\u001b[1;32m    330\u001b[0m     ]:\n\u001b[1;32m    331\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;124;03m    Wrapper around run_with_cache in HookedRootModule. If return_cache_object is True, this will return an ActivationCache object, with a bunch of useful HookedTransformer specific methods, otherwise it will return a dictionary of activations as in HookedRootModule. This function was copied directly from HookedTransformer.\u001b[39;00m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 334\u001b[0m     out, cache_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    335\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mremove_batch_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremove_batch_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    336\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    337\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m return_cache_object:\n\u001b[1;32m    338\u001b[0m         cache \u001b[38;5;241m=\u001b[39m ActivationCache(cache_dict, \u001b[38;5;28mself\u001b[39m, has_batch_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m remove_batch_dim)\n",
      "File \u001b[0;32m~/TransformerLens/transformer_lens/hook_points.py:568\u001b[0m, in \u001b[0;36mHookedRootModule.run_with_cache\u001b[0;34m(self, names_filter, device, remove_batch_dim, incl_bwd, reset_hooks_end, clear_contexts, pos_slice, *model_args, **model_kwargs)\u001b[0m\n\u001b[1;32m    554\u001b[0m cache_dict, fwd, bwd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_caching_hooks(\n\u001b[1;32m    555\u001b[0m     names_filter,\n\u001b[1;32m    556\u001b[0m     incl_bwd,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    559\u001b[0m     pos_slice\u001b[38;5;241m=\u001b[39mpos_slice,\n\u001b[1;32m    560\u001b[0m )\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhooks(\n\u001b[1;32m    563\u001b[0m     fwd_hooks\u001b[38;5;241m=\u001b[39mfwd,\n\u001b[1;32m    564\u001b[0m     bwd_hooks\u001b[38;5;241m=\u001b[39mbwd,\n\u001b[1;32m    565\u001b[0m     reset_hooks_end\u001b[38;5;241m=\u001b[39mreset_hooks_end,\n\u001b[1;32m    566\u001b[0m     clear_contexts\u001b[38;5;241m=\u001b[39mclear_contexts,\n\u001b[1;32m    567\u001b[0m ):\n\u001b[0;32m--> 568\u001b[0m     model_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m incl_bwd:\n\u001b[1;32m    570\u001b[0m         model_out\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/miniconda3/envs/tl_esm_env_old_torch_1/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/tl_esm_env_old_torch_1/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/TransformerLens/transformer_lens/HookedESM3.py:297\u001b[0m, in \u001b[0;36mHookedESM3.forward\u001b[0;34m(self, sequence_tokens, structure_tokens, ss8_tokens, sasa_tokens, function_tokens, residue_annotation_tokens, average_plddt, per_res_plddt, structure_coords, chain_id, sequence_id, return_type)\u001b[0m\n\u001b[1;32m    286\u001b[0m resid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhook_embed(\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed(sequence_tokens,\n\u001b[1;32m    288\u001b[0m     structure_tokens,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    293\u001b[0m     function_tokens,\n\u001b[1;32m    294\u001b[0m     residue_annotation_tokens,))\n\u001b[1;32m    296\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks:\n\u001b[0;32m--> 297\u001b[0m     resid  \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msequence_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maffine\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maffine_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchain_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    298\u001b[0m normalised \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_final(resid)\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/tl_esm_env_old_torch_1/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/tl_esm_env_old_torch_1/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/TransformerLens/transformer_lens/components/esm3_hooked_block.py:117\u001b[0m, in \u001b[0;36mHookedEsm3UnifiedTransformerBlock.forward\u001b[0;34m(self, resid_pre, sequence_id, frames, frames_mask, chain_id)\u001b[0m\n\u001b[1;32m    110\u001b[0m     key_input \u001b[38;5;241m=\u001b[39m attn_in\n\u001b[1;32m    111\u001b[0m     value_input \u001b[38;5;241m=\u001b[39m attn_in\n\u001b[1;32m    113\u001b[0m attn_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhook_attn_out(\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;66;03m# hook the residual stream states that are used to calculate the\u001b[39;00m\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;66;03m# queries, keys and values, independently.\u001b[39;00m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m# Then take the layer norm of these inputs, and pass these to the attention module.\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhook_post_layer_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mln1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_input\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mln1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey_input\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalue_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mln1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue_input\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_kv_cache_entry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    124\u001b[0m )  \u001b[38;5;66;03m# [batch, pos, d_model]\u001b[39;00m\n\u001b[1;32m    125\u001b[0m scaled_attn_out \u001b[38;5;241m=\u001b[39m attn_out \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mesm3_scaling_factor\n\u001b[1;32m    126\u001b[0m resid_mid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhook_resid_mid(resid_pre \u001b[38;5;241m+\u001b[39mscaled_attn_out)  \u001b[38;5;66;03m# [batch, pos, d_model]\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/tl_esm_env_old_torch_1/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/tl_esm_env_old_torch_1/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/TransformerLens/transformer_lens/components/abstract_attention.py:208\u001b[0m, in \u001b[0;36mAbstractAttention.forward\u001b[0;34m(self, query_input, key_input, value_input, past_kv_cache_entry, additive_attention_mask, attention_mask, position_bias)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    181\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    182\u001b[0m     query_input: Union[\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    199\u001b[0m     position_bias: Optional[Float[torch\u001b[38;5;241m.\u001b[39mTensor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1 head_index pos kv_pos\u001b[39m\u001b[38;5;124m\"\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    200\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Float[torch\u001b[38;5;241m.\u001b[39mTensor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch pos d_model\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    201\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;124;03m    shortformer_pos_embed is only used if self.cfg.positional_embedding_type == \"shortformer\", else defaults to None and is irrelevant. See HookedTransformerConfig for more details\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;124;03m    past_kv_cache_entry is an optional entry of past keys and values for this layer, only relevant if generating text. Defaults to None\u001b[39;00m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;124;03m    additive_attention_mask is an optional mask to add to the attention weights. Defaults to None.\u001b[39;00m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;124;03m    attention_mask is the attention mask for padded tokens. Defaults to None.\u001b[39;00m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 208\u001b[0m     q, k, v \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcalculate_qkv_matrices\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mqk_layernorm:\n\u001b[1;32m    210\u001b[0m         \u001b[38;5;66;03m# Rearrange q and k to concatenate the last two dimensions\u001b[39;00m\n\u001b[1;32m    211\u001b[0m         q_flattened \u001b[38;5;241m=\u001b[39m einops\u001b[38;5;241m.\u001b[39mrearrange(q, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch pos head_index d_head -> batch pos (head_index d_head)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/TransformerLens/transformer_lens/components/abstract_attention.py:405\u001b[0m, in \u001b[0;36mAbstractAttention.calculate_qkv_matrices\u001b[0;34m(self, query_input, key_input, value_input)\u001b[0m\n\u001b[1;32m    389\u001b[0m     q \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhook_q(\n\u001b[1;32m    390\u001b[0m         \u001b[38;5;66;03m# call bitsandbytes method to dequantize and multiply\u001b[39;00m\n\u001b[1;32m    391\u001b[0m         bnb\u001b[38;5;241m.\u001b[39mmatmul_4bit(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    402\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mb_Q\n\u001b[1;32m    403\u001b[0m     )\n\u001b[1;32m    404\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 405\u001b[0m     q \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhook_q(\u001b[43mattn_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mW_Q\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mb_Q\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    406\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mload_in_4bit:\n\u001b[1;32m    407\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW_K, Params4bit):\n",
      "File \u001b[0;32m~/TransformerLens/transformer_lens/utilities/attention.py:42\u001b[0m, in \u001b[0;36mcomplex_attn_linear\u001b[0;34m(input, w, b)\u001b[0m\n\u001b[1;32m     39\u001b[0m w \u001b[38;5;241m=\u001b[39m einops\u001b[38;5;241m.\u001b[39mrearrange(w, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhead_index d_model d_head -> 1 1 head_index d_model d_head\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# Element-wise multiplication and sum over the d_model dimension\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\n\u001b[1;32m     43\u001b[0m result \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39msum(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result \u001b[38;5;241m+\u001b[39m b\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 6.79 GiB. GPU 0 has a total capacity of 47.44 GiB of which 6.72 GiB is free. Including non-PyTorch memory, this process has 40.71 GiB memory in use. Of the allocated memory 40.35 GiB is allocated by PyTorch, and 48.63 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "res, cache = esm3_hooked1.run_with_cache(sequence_tokens=sequence_tokens2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2b2a6f71-df98-41ee-a044-c59d226e21e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "del output1\n",
    "del output2\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "40282a9c-c021-4cd7-b637-d1ed94f9cbec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 1            |        cudaMalloc retries: 1         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |  48172 MiB |  48183 MiB | 554607 MiB | 506435 MiB |\n",
      "|       from large pool |  47982 MiB |  47993 MiB | 553608 MiB | 505626 MiB |\n",
      "|       from small pool |    190 MiB |    190 MiB |    999 MiB |    808 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |  48172 MiB |  48183 MiB | 554607 MiB | 506435 MiB |\n",
      "|       from large pool |  47982 MiB |  47993 MiB | 553608 MiB | 505626 MiB |\n",
      "|       from small pool |    190 MiB |    190 MiB |    999 MiB |    808 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Requested memory      |  48171 MiB |  48182 MiB | 554601 MiB | 506430 MiB |\n",
      "|       from large pool |  47982 MiB |  47993 MiB | 553608 MiB | 505626 MiB |\n",
      "|       from small pool |    189 MiB |    189 MiB |    993 MiB |    803 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |  48232 MiB |  48232 MiB |  48232 MiB |      0 B   |\n",
      "|       from large pool |  48040 MiB |  48040 MiB |  48040 MiB |      0 B   |\n",
      "|       from small pool |    192 MiB |    192 MiB |    192 MiB |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |    7370    |    7378    |   47420    |   40050    |\n",
      "|       from large pool |    3305    |    3309    |   21111    |   17806    |\n",
      "|       from small pool |    4065    |    4069    |   26309    |   22244    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |    7370    |    7378    |   47420    |   40050    |\n",
      "|       from large pool |    3305    |    3309    |   21111    |   17806    |\n",
      "|       from small pool |    4065    |    4069    |   26309    |   22244    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.memory_summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d04b492b-7685-427d-84f3-a8772b663302",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model itself takes approximately 5540.33 MB of memory.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def model_memory_usage(model):\n",
    "    \"\"\"\n",
    "    Calculate the total memory used by a model's parameters and buffers in bytes.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The PyTorch model.\n",
    "\n",
    "    Returns:\n",
    "        float: Memory usage in MB.\n",
    "    \"\"\"\n",
    "    total_params = sum(p.numel() * p.element_size() for p in model.parameters())\n",
    "    total_buffers = sum(b.numel() * b.element_size() for b in model.buffers())\n",
    "    total_memory = total_params + total_buffers  # Total memory in bytes\n",
    "    return total_memory / (1024 ** 2)  # Convert to MB\n",
    "\n",
    "# Example usage\n",
    "memory_in_mb = model_memory_usage(esm3_hooked1)\n",
    "print(f\"The model itself takes approximately {memory_in_mb:.2f} MB of memory.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "10c261d5-dbc1-46e0-8f5e-6bcc0a841846",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizers = get_esm3_model_tokenizers()\n",
    "sequence1 = \"MPGWFKKAWYGLASLLSFSSFILIIVALVVPHWLSGKILCQTGVDLVNATDRELVKFIGDIYYGLFRGCKVRQCGLGGRQSQFTIFPHLVKELNAGLHVMILLLLFLALALALVSMGFAILNMIQVPYRAVSGPGGICLWNVLAGGVVALAIASFVAAVKFHDLTERIANFQEKLFQFVVVEEQYEESFWICVASASAHAANLVVVAISQIPLPEIKTKIEEATVTAEDILY\"\n",
    "sequence2= \"MAAA<mask>\"\n",
    "arr = [sequence1, sequence2]\n",
    "tokens = tokenizers.sequence(arr, return_tensors=\"pt\", padding=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8b9e246c-ddf0-4441-ba60-8a054f503d3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "print(tokens.attention_mask[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6a742a0-9a27-4906-8dba-2bf517d88824",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "import torch\n",
    "from jaxtyping import Float\n",
    "from torch.testing import assert_close\n",
    "import torch.nn as nn\n",
    "from transformer_lens.components import Attention\n",
    "from transformer_lens.components import LayerNorm\n",
    "from transformer_lens.components import HookedESM3MLP, swiglu_correction_fn\n",
    "from transformer_lens.components import HookedEsm3UnifiedTransformerBlock\n",
    "from esm.layers.attention import MultiHeadAttention\n",
    "from esm.layers.blocks import swiglu_ln_ffn, UnifiedTransformerBlock\n",
    "from transformer_lens.HookedTransformerConfig import HookedTransformerConfig\n",
    "import functools\n",
    "import einops\n",
    "import math\n",
    "from esm.pretrained import (\n",
    "    ESM3_sm_open_v0,\n",
    ")\n",
    "from transformer_lens import HookedESM3,SupportedESM3Config\n",
    "from esm.tokenization import get_esm3_model_tokenizers\n",
    "import gc\n",
    "def test_full_model(\n",
    "    device=\"cuda\",\n",
    "    esm3_use_torch_attention_calc=True,\n",
    "    use_attn_result=False,\n",
    "    use_split_qkv_input=True,\n",
    "    esm3_use_org_rotary=True,\n",
    "    esm3_use_torch_layer_norm=True,\n",
    "):\n",
    "    esm3_original = ESM3_sm_open_v0(device).to(device)\n",
    "    esm3_original.eval()\n",
    "    tokenizer = tokenizers = get_esm3_model_tokenizers()\n",
    "    sequence = \"MDADKEKDLQKFLKNVDEISNLIQEMNSDDPVVQQKAVLETEKRLLLMEEDQEEDECRTTLNKTMISPPQTAMKSAEEINSEAFLASVEKDAKERAKRRRENKVLADALKEKGNEAFAEGNYETAILRYSEGLEKLKDMKVLYTNRAQAYMKLEDYEKALVDCEWALKCDEKCTKAYFHMGKANLALKNYSVSRECYKKILEINPKLQTQVKGYLNQVDLQEKADLQEKEAHELLDSGKNTAVTTKNLLETLSKPDQIPLFYAGGIEILTEMINECTEQTLFRMHNGFSIISDNEVIRRCFSTAGNDAVEEMVCVSVLKLWQAVCSRNEENQRVLVIHHDRARLLAALLSSKVLAIRQQSFALLLHLAQTESGRSLIINHLDLTRLLEALVSFLDFSDKEANTAMGLFTDLALEERFQVWFQANLPGVLPALTGVLKTDPKVSSSSALCQCIAIMGNLSAEPTTRRHMAACEEFGDGCLSLLARCEEDVDLFREVIYTLLGLMMNLCLQAPFVSEVWAVEVSRRCLSLLNSQDGGILTRAAGVLSRTLSSSLKIVEEALRAGVVKKMMKFLKTGGETASRYAIKILAICTNSYHEAREEVIRLDKKLSVMMKLLSSEDEVLVGNAALCLGNCMEVPNVASSLLKTDLLQVLLKLAGSDTQKTAVQVNAGIALGKLCTAEPRFAAQLRKLHGLEILNSTMKYISDS\"\n",
    "    tokens = tokenizers.sequence.encode(sequence)\n",
    "    sequence_tokens = torch.tensor(tokens, dtype=torch.int64)\n",
    "    sequence_tokens = sequence_tokens.to(device).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        output1 = esm3_original.forward(\n",
    "            sequence_tokens=sequence_tokens\n",
    "        )\n",
    "    del esm3_original\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    config = SupportedESM3Config(\n",
    "        use_attn_result=use_attn_result,\n",
    "        use_split_qkv_input=use_split_qkv_input,\n",
    "        use_hook_mlp_in=False,\n",
    "        use_attn_in=False,\n",
    "        esm3_output_type=\"all\",\n",
    "        esm3_use_torch_layer_norm=esm3_use_torch_layer_norm,\n",
    "        esm3_use_torch_attention_calc=esm3_use_torch_attention_calc,\n",
    "        esm3_use_org_rotary = esm3_use_org_rotary\n",
    "    )\n",
    "    esm3_hooked = HookedESM3.from_pretrained(esm_cfg=config, device=device)\n",
    "    esm3_hooked.eval()\n",
    "    with torch.no_grad():\n",
    "        output2 = esm3_hooked.forward(\n",
    "            sequence_tokens=sequence_tokens\n",
    "        )\n",
    "    print(output1.function_logits.shape)\n",
    "    assert torch.allclose(output1.sequence_logits, output1.sequence_logits, rtol=1e-5, atol=4e-5)\n",
    "    print(torch.max(torch.abs(output1.sequence_logits-output2.function_logits)))\n",
    "    assert torch.allclose(output1.structure_logits, output2.structure_logits, rtol=1e-5, atol=4e-5)\n",
    "    print(torch.max(torch.abs(output1.structure_logits-output2.structure_logits)))\n",
    "    assert torch.allclose(output1.sasa_logits, output2.sasa_logits, rtol=1e-5, atol=4e-5)\n",
    "    print(torch.max(torch.abs(output1.sasa_logits-output2.sasa_logits)))\n",
    "    assert torch.allclose(output1.secondary_structure_logits, output2.secondary_structure_logits,  rtol=1e-5, atol=4e-5)\n",
    "    print(torch.max(torch.abs(output1.secondary_structure_logits-output2.secondary_structure_logits)))\n",
    "    \n",
    "    assert torch.allclose(output1.function_logits, output2.function_logits,  rtol=1e-4, atol=2e-4)\n",
    "    print(torch.max(torch.abs(output1.function_logits-output2.function_logits)))\n",
    "    assert torch.allclose(output1.residue_logits, output2.residue_logits,  rtol=1e-5, atol=4e-5)\n",
    "    print(torch.max(torch.abs(output1.residue_logits-output2.residue_logits)))\n",
    "    del esm3_hooked\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa7fb3d2-48ec-45bf-adf7-6724bb1570ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Please notice the licsence - todo- add licenseSupport for ESM3 in TransformerLens is currently experimental, until such a time when it has feature parity with HookedTransformer and has been tested on real research tasks. Until then, backward compatibility is not guaranteed. Please see the docs for information on the limitations of the current implementation.\n",
      "If using ESM3 for interpretability research, keep in mind that ESM3 has some significant architectural differences to Language transformers like GPT.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  cuda\n",
      "Loaded pretrained model esm3_sm_open_v1 into HookedESM3\n",
      "torch.Size([1, 707, 8, 260])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (64) must match the size of tensor b (260) at non-singleton dimension 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtest_full_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 63\u001b[0m, in \u001b[0;36mtest_full_model\u001b[0;34m(device, esm3_use_torch_attention_calc, use_attn_result, use_split_qkv_input, esm3_use_org_rotary, esm3_use_torch_layer_norm)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28mprint\u001b[39m(output1\u001b[38;5;241m.\u001b[39mfunction_logits\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mallclose(output1\u001b[38;5;241m.\u001b[39msequence_logits, output1\u001b[38;5;241m.\u001b[39msequence_logits, rtol\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-5\u001b[39m, atol\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4e-5\u001b[39m)\n\u001b[0;32m---> 63\u001b[0m \u001b[38;5;28mprint\u001b[39m(torch\u001b[38;5;241m.\u001b[39mmax(torch\u001b[38;5;241m.\u001b[39mabs(\u001b[43moutput1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msequence_logits\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43moutput2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_logits\u001b[49m)))\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mallclose(output1\u001b[38;5;241m.\u001b[39mstructure_logits, output2\u001b[38;5;241m.\u001b[39mstructure_logits, rtol\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-5\u001b[39m, atol\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4e-5\u001b[39m)\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28mprint\u001b[39m(torch\u001b[38;5;241m.\u001b[39mmax(torch\u001b[38;5;241m.\u001b[39mabs(output1\u001b[38;5;241m.\u001b[39mstructure_logits\u001b[38;5;241m-\u001b[39moutput2\u001b[38;5;241m.\u001b[39mstructure_logits)))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (64) must match the size of tensor b (260) at non-singleton dimension 3"
     ]
    }
   ],
   "source": [
    "test_full_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba3a0a2-4da4-4b67-9907-3f5b1f1a7af1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
